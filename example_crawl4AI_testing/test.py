import asyncio
import json
import hashlib
import re
from urllib.parse import urlparse, urlunparse
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

def normalize_url(url):
    """Chuáº©n hÃ³a URL Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p vÃ  loáº¡i bá» rÃ¡c"""
    try:
        parsed = urlparse(url)
        path = parsed.path.rstrip('/')
        return urlunparse((parsed.scheme.lower(), parsed.netloc.lower(), path, '', '', ''))
    except:
        return url

async def universal_crawler_engine(base_url: str):
    # --- 1. THIáº¾T Láº¬P Bá»˜ Lá»ŒC THÃ”NG MINH (DÃ¹ng cho má»i web) ---
    # Bá»™ lá»c nÃ y tá»± tÃ­nh toÃ¡n máº­t Ä‘á»™ vÄƒn báº£n Ä‘á»ƒ tÃ¬m ra bÃ i viáº¿t chÃ­nh
    prune_filter = PruningContentFilter(
        threshold=0.48,           # NgÆ°á»¡ng tá»‘i Æ°u cho háº§u háº¿t cÃ¡c trang tin/blog
        threshold_type="dynamic", # Tá»± thÃ­ch nghi vá»›i tá»«ng cáº¥u trÃºc web khÃ¡c nhau
        min_word_threshold=15     # Bá» qua cÃ¡c khá»‘i vÄƒn báº£n quÃ¡ ngáº¯n
    )
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)

    # --- 2. Cáº¤U HÃŒNH TRÃŒNH DUYá»†T (TÃ ng hÃ¬nh & Äa nÄƒng) ---
    browser_config = BrowserConfig(
        headless=True,
        enable_stealth=True,
        browser_type="chromium",
        user_agent_mode="random"  # Má»—i láº§n cháº¡y giáº£ danh má»™t trÃ¬nh duyá»‡t khÃ¡c nhau
    )

    # --- 3. Cáº¤U HÃŒNH CHáº Y (Cuá»™n trang & VÆ°á»£t rÃ o) ---
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=md_generator,
        magic=True,               # Tá»± Ä‘á»™ng vÆ°á»£t qua cÃ¡c báº«y cháº·n bot
        scan_full_page=True,      # Cuá»™n xuá»‘ng Ä‘á»ƒ load háº¿t link áº©n/ná»™i dung Ä‘á»™ng
        scroll_delay=0.5,
        word_count_threshold=30   # Chá»‰ láº¥y nhá»¯ng trang cÃ³ ná»™i dung thá»±c sá»±
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # BÆ¯á»šC A: KHÃM PHÃ Má»ŒI LINK
        print(f"ğŸ” Äang thÃ¡m hiá»ƒm há»‡ thá»‘ng link cá»§a: {base_url}")
        initial_res = await crawler.arun(url=base_url, config=run_config)
        
        if not initial_res.success:
            print(f"âŒ KhÃ´ng thá»ƒ truy cáº­p trang web nÃ y: {initial_res.error_message}")
            return

        # Láº¥y táº¥t cáº£ link ná»™i bá»™
        internal_links = initial_res.links.get("internal", [])
        
        # Lá»c URL: Loáº¡i bá» cÃ¡c link cháº¯c cháº¯n lÃ  rÃ¡c (áº£nh, css, js, login, logout)
        exclude_patterns = [r'\.(jpg|png|pdf|zip|css|js)$', r'/(login|logout|signin|signup)', r'#']
        
        urls_to_crawl = []
        for link in internal_links:
            href = link.get('href')
            if href and not any(re.search(p, href, re.IGNORECASE) for p in exclude_patterns):
                urls_to_crawl.append(normalize_url(href))
        
        urls_to_crawl = list(set(urls_to_crawl)) # Chá»‰ giá»¯ láº¡i cÃ¡c URL duy 
        urls_to_crawl = urls_to_crawl[:100]
        print(f"âœ… TÃ¬m tháº¥y {len(urls_to_crawl)} trang tiá»m nÄƒng Ä‘á»ƒ quÃ©t sáº¡ch.")

        # BÆ¯á»šC B: Tá»”NG Táº¤N CÃ”NG (QuÃ©t hÃ ng loáº¡t)
        # Sá»­ dá»¥ng tá»‘i Ä‘a 5 tab Ä‘á»ƒ khÃ´ng lÃ m website má»¥c tiÃªu "phÃ¡t hoáº£ng" mÃ  cháº·n báº¡n
        dispatcher = MemoryAdaptiveDispatcher(
            memory_threshold_percent=75.0,
            max_session_permit=5 
        )

        print(f"ğŸš€ Báº¯t Ä‘áº§u hÃºt dá»¯ liá»‡u hÃ ng loáº¡t...")
        results = await crawler.arun_many(
            urls=urls_to_crawl,
            config=run_config,
            dispatcher=dispatcher
        )

        # BÆ¯á»šC C: Tá»”NG Há»¢P & Lá»ŒC TRÃ™NG Ná»˜I DUNG TUYá»†T Äá»I
        final_data = []
        seen_hashes = set()

        for res in results:
            if res.success:
                # Láº¥y ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c bá»™ lá»c thÃ´ng minh lÃ m sáº¡ch
                content = res.markdown.fit_markdown or res.markdown.raw_markdown
                if not content or len(content) < 100: continue # Bá» qua trang quÃ¡ Ã­t chá»¯

                # Táº¡o vÃ¢n tay ná»™i dung Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng lÆ°u trÃ¹ng dÃ¹ URL khÃ¡c nhau
                c_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
                
                if c_hash not in seen_hashes:
                    seen_hashes.add(c_hash)
                    final_data.append({
                        "url": res.url,
                        "title": res.metadata.get('title', 'No Title'),
                        "content": content,
                        "hash": c_hash
                    })

        # XUáº¤T Káº¾T QUáº¢
        domain_name = urlparse(base_url).netloc.replace('.', '_')
        output_file = f"scraped_{domain_name}.json"
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ‰ HOÃ€N THÃ€NH! ÄÃ£ quÃ©t sáº¡ch {len(final_data)} trang cháº¥t lÆ°á»£ng.")
        print(f"ğŸ“‚ Dá»¯ liá»‡u náº±m táº¡i file: {output_file}")

if __name__ == "__main__":
    # Báº¡n chá»‰ cáº§n Ä‘á»•i link á»Ÿ Ä‘Ã¢y lÃ  nÃ³ sáº½ tá»± quÃ©t sáº¡ch web Ä‘Ã³
    target_url = "https://vnexpress.net/" 
    asyncio.run(universal_crawler_engine(target_url))