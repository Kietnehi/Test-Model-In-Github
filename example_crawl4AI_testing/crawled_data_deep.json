[
  {
    "url": "https://docs.crawl4ai.com",
    "depth": 0,
    "title": "Home - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "2104036f927c615b6df4775fc559918e",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * Home\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [ðŸš€ðŸ¤– Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper](https://docs.crawl4ai.com/#crawl4ai-open-source-llm-friendly-web-crawler-scraper)\n  * [ðŸš€ Crawl4AI Cloud API â€” Closed Beta (Launching Soon)](https://docs.crawl4ai.com/#crawl4ai-cloud-api-closed-beta-launching-soon)\n  * [ðŸ†• AI Assistant Skill Now Available!](https://docs.crawl4ai.com/#ai-assistant-skill-now-available)\n  * [ðŸŽ¯ New: Adaptive Web Crawling](https://docs.crawl4ai.com/#new-adaptive-web-crawling)\n  * [Quick Start](https://docs.crawl4ai.com/#quick-start)\n  * [Video Tutorial](https://docs.crawl4ai.com/#video-tutorial)\n  * [What Does Crawl4AI Do?](https://docs.crawl4ai.com/#what-does-crawl4ai-do)\n  * [Documentation Structure](https://docs.crawl4ai.com/#documentation-structure)\n  * [How You Can Support](https://docs.crawl4ai.com/#how-you-can-support)\n  * [Quick Links](https://docs.crawl4ai.com/#quick-links)\n\n\n# ðŸš€ðŸ¤– Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper\n[ ![unclecode%2Fcrawl4ai | Trendshift](https://trendshift.io/api/badge/repositories/11716) ](https://trendshift.io/repositories/11716)\n[ ![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social) ](https://github.com/unclecode/crawl4ai/stargazers) [ ![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social) ](https://github.com/unclecode/crawl4ai/network/members) [ ![PyPI version](https://badge.fury.io/py/crawl4ai.svg) ](https://badge.fury.io/py/crawl4ai)\n[ ![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai) ](https://pypi.org/project/crawl4ai/) [ ![Downloads](https://static.pepy.tech/badge/crawl4ai/month) ](https://pepy.tech/project/crawl4ai) [ ![License](https://img.shields.io/github/license/unclecode/crawl4ai) ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)\n[ ![Follow on X](https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white) ](https://x.com/crawl4ai) [ ![Follow on LinkedIn](https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white) ](https://www.linkedin.com/company/crawl4ai) [ ![Join our Discord](https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white) ](https://discord.gg/jP8KfhDhyN)\n* * *\n#### ðŸš€ Crawl4AI Cloud API â€” Closed Beta (Launching Soon)\nReliable, large-scale web extraction, now built to be _**drastically more cost-effective**_ than any of the existing solutions.\nðŸ‘‰ **Apply[here](https://forms.gle/E9MyPaNXACnAMaqG7) for early access**  \n_Weâ€™ll be onboarding in phases and working closely with early users. Limited slots._\n* * *\nCrawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, **Crawl4AI** empowers developers with unmatched speed, precision, and deployment ease.\n> Enjoy using Crawl4AI? Consider **[becoming a sponsor](https://github.com/sponsors/unclecode)** to support ongoing development and community growth!\n## ðŸ†• AI Assistant Skill Now Available!\n### ðŸ¤– Crawl4AI Skill for Claude & AI Assistants\nSupercharge your AI coding assistant with complete Crawl4AI knowledge! Download our comprehensive skill package that includes:\n  * ðŸ“š Complete SDK reference (23K+ words)\n  * ðŸš€ Ready-to-use extraction scripts\n  * âš¡ Schema generation for efficient scraping\n  * ðŸ”§ Version 0.7.4 compatible\n\n\n[ ðŸ“¦ Download Skill Package ](https://docs.crawl4ai.com/assets/crawl4ai-skill.zip)\nWorks with Claude, Cursor, Windsurf, and other AI coding assistants. Import the .zip file into your AI assistant's skill/knowledge system. \n## ðŸŽ¯ New: Adaptive Web Crawling\nCrawl4AI now features intelligent adaptive crawling that knows when to stop! Using advanced information foraging algorithms, it determines when sufficient information has been gathered to answer your query.\n[Learn more about Adaptive Crawling â†’](https://docs.crawl4ai.com/core/adaptive-crawling/)\n## Quick Start\nHere's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler() as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://crawl4ai.com\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())\nCopy\n```\n\n* * *\n## Video Tutorial\n[](https://www.youtube.com/channel/UCpnV0jtSvh4uifO7C6fSnVg?embeds_referring_euri=http%3A%2F%2Fwww.srpko.com%2F)\nUnclecode\n2,33 tys. subskrybentÃ³w\n[Crawl4AI Official Tutorial, Full 1hr with Quickstart Examples](https://www.youtube.com/watch?v=xo3qK6Hg9AA)\n[](https://docs.crawl4ai.com/)\nUnclecode\nSzukaj\nDo obejrzenia\nUdostÄ™pnij\nKopiuj link\nInformacje\nZakupy\nWyÅ‚Ä…cz wyciszenie\nJeÅ›li odtwarzanie zaraz siÄ™ nie zacznie, sprÃ³buj ponownie uruchomiÄ‡ urzÄ…dzenie.\nWiÄ™cej filmÃ³w\n## WiÄ™cej filmÃ³w\nNie korzystasz teraz z konta\nFilmy, ktÃ³re oglÄ…dasz, mogÄ… zostaÄ‡ dodane do historii oglÄ…dania YouTube TV i wpÅ‚ywaÄ‡ na pojawiajÄ…ce siÄ™ tam rekomendacje. Aby tego uniknÄ…Ä‡, anuluj proces i zaloguj siÄ™ w YouTube na komputerze.\nAnulujPotwierdÅº\nUdostÄ™pnij\nDoÅ‚Ä…cz playlistÄ™\nPodczas pobierania informacji o udostÄ™pnianiu wystÄ…piÅ‚ bÅ‚Ä…d. SprÃ³buj ponownie pÃ³Åºniej.\n[Obejrzyj w](https://www.youtube.com/watch?t=15&v=xo3qK6Hg9AA&embeds_referring_euri=http%3A%2F%2Fwww.srpko.com%2F)\n0:15\n0:15 / 1:02:39\nâ€¢Na Å¼ywo\nâ€¢\n[](https://www.youtube.com/watch?v=xo3qK6Hg9AA)\n* * *\n## What Does Crawl4AI Do?\nCrawl4AI is a feature-rich crawler and scraper that aims to:\n1. **Generate Clean Markdown** : Perfect for RAG pipelines or direct ingestion into LLMs.  \n2. **Structured Extraction** : Parse repeated patterns with CSS, XPath, or LLM-based extraction.  \n3. **Advanced Browser Control** : Hooks, proxies, stealth modes, session re-useâ€”fine-grained control.  \n4. **High Performance** : Parallel crawling, chunk-based extraction, real-time use cases.  \n5. **Open Source** : No forced API keys, no paywallsâ€”everyone can access their data. \n**Core Philosophies** : - **Democratize Data** : Free to use, transparent, and highly configurable.  \n- **LLM Friendly** : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it.\n* * *\n## Documentation Structure\nTo help you get started, weâ€™ve organized our docs into clear sections:\n  * **Setup & Installation**  \nBasic instructions to install Crawl4AI via pip or Docker. \n  * **Quick Start**  \nA hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. \n  * **Core**  \nDeeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. \n  * **Advanced**  \nExplore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. \n  * **Extraction**  \nDetailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. \n  * **API Reference**  \nFind the technical specifics of each class and method, including `AsyncWebCrawler`, `arun()`, and `CrawlResult`.\n\n\nThroughout these sections, youâ€™ll find code samples you can **copy-paste** into your environment. If something is missing or unclear, raise an issue or PR.\n* * *\n## How You Can Support\n  * **Star & Fork**: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. \n  * **File Issues** : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. \n  * **Pull Requests** : Whether itâ€™s a small fix, a big feature, or better docsâ€”contributions are always welcome. \n  * **Join Discord** : Come chat about web scraping, crawling tips, or AI workflows with the community. \n  * **Spread the Word** : Mention Crawl4AI in your blog posts, talks, or on social media. \n\n\n**Our mission** : to empower everyoneâ€”students, researchers, entrepreneurs, data scientistsâ€”to access, parse, and shape the worldâ€™s data with speed, cost-efficiency, and creative freedom.\n* * *\n## Quick Links\n  * **[GitHub Repo](https://github.com/unclecode/crawl4ai)**\n  * **[Installation Guide](https://docs.crawl4ai.com/core/installation/)**\n  * **[Quick Start](https://docs.crawl4ai.com/core/quickstart/)**\n  * **[API Reference](https://docs.crawl4ai.com/api/async-webcrawler/)**\n  * **[Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)**\n\n\nThank you for joining me on this journey. Letâ€™s keep building an **open, democratic** approach to data extraction and AI together.\nHappy Crawling!  \nâ€” _Unclecode, Founder & Maintainer of Crawl4AI_\n#### On this page\n  * [ðŸš€ Crawl4AI Cloud API â€” Closed Beta (Launching Soon)](https://docs.crawl4ai.com/#crawl4ai-cloud-api-closed-beta-launching-soon)\n  * [ðŸ†• AI Assistant Skill Now Available!](https://docs.crawl4ai.com/#ai-assistant-skill-now-available)\n  * [ðŸ¤– Crawl4AI Skill for Claude & AI Assistants](https://docs.crawl4ai.com/#toc-heading-2--crawl4ai-skill-for-claude--ai-assistants)\n  * [ðŸŽ¯ New: Adaptive Web Crawling](https://docs.crawl4ai.com/#new-adaptive-web-crawling)\n  * [Quick Start](https://docs.crawl4ai.com/#quick-start)\n  * [Video Tutorial](https://docs.crawl4ai.com/#video-tutorial)\n  * [What Does Crawl4AI Do?](https://docs.crawl4ai.com/#what-does-crawl4ai-do)\n  * [Documentation Structure](https://docs.crawl4ai.com/#documentation-structure)\n  * [How You Can Support](https://docs.crawl4ai.com/#how-you-can-support)\n  * [Quick Links](https://docs.crawl4ai.com/#quick-links)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/adaptive-strategies",
    "depth": 1,
    "title": "Adaptive Strategies - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "a6bc0dc2dbff334e7d8632cba91145a7",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * Adaptive Strategies\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Advanced Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/#advanced-adaptive-strategies)\n  * [Overview](https://docs.crawl4ai.com/advanced/adaptive-strategies/#overview)\n  * [The Three-Layer Scoring System](https://docs.crawl4ai.com/advanced/adaptive-strategies/#the-three-layer-scoring-system)\n  * [Link Ranking Algorithm](https://docs.crawl4ai.com/advanced/adaptive-strategies/#link-ranking-algorithm)\n  * [Domain-Specific Configurations](https://docs.crawl4ai.com/advanced/adaptive-strategies/#domain-specific-configurations)\n  * [Performance Optimization](https://docs.crawl4ai.com/advanced/adaptive-strategies/#performance-optimization)\n  * [Debugging & Analysis](https://docs.crawl4ai.com/advanced/adaptive-strategies/#debugging-analysis)\n  * [Custom Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/#custom-strategies)\n  * [Best Practices](https://docs.crawl4ai.com/advanced/adaptive-strategies/#best-practices)\n  * [Next Steps](https://docs.crawl4ai.com/advanced/adaptive-strategies/#next-steps)\n\n\n# Advanced Adaptive Strategies\n## Overview\nWhile the default adaptive crawling configuration works well for most use cases, understanding the underlying strategies and scoring mechanisms allows you to fine-tune the crawler for specific domains and requirements.\n## The Three-Layer Scoring System\n### 1. Coverage Score\nCoverage measures how comprehensively your knowledge base covers the query terms and related concepts.\n#### Mathematical Foundation\n```\nCoverage(K, Q) = Î£(t âˆˆ Q) score(t, K) / |Q|\n\nwhere score(t, K) = doc_coverage(t) Ã— (1 + freq_boost(t))\nCopy\n```\n\n#### Components\n  * **Document Coverage** : Percentage of documents containing the term\n  * **Frequency Boost** : Logarithmic bonus for term frequency\n  * **Query Decomposition** : Handles multi-word queries intelligently\n\n\n#### Tuning Coverage\n```\n# For technical documentation with specific terminology\nconfig = AdaptiveConfig(\n    confidence_threshold=0.85,  # Require high coverage\n    top_k_links=5              # Cast wider net\n)\n\n# For general topics with synonyms\nconfig = AdaptiveConfig(\n    confidence_threshold=0.6,   # Lower threshold\n    top_k_links=2              # More focused\n)\nCopy\n```\n\n### 2. Consistency Score\nConsistency evaluates whether the information across pages is coherent and non-contradictory.\n#### How It Works\n  1. Extracts key statements from each document\n  2. Compares statements across documents\n  3. Measures agreement vs. contradiction\n  4. Returns normalized score (0-1)\n\n\n#### Practical Impact\n  * **High consistency ( >0.8)**: Information is reliable and coherent\n  * **Medium consistency (0.5-0.8)** : Some variation, but generally aligned\n  * **Low consistency ( <0.5)**: Conflicting information, need more sources\n\n\n### 3. Saturation Score\nSaturation detects when new pages stop providing novel information.\n#### Detection Algorithm\n```\n# Tracks new unique terms per page\nnew_terms_page_1 = 50\nnew_terms_page_2 = 30  # 60% of first\nnew_terms_page_3 = 15  # 50% of second\nnew_terms_page_4 = 5   # 33% of third\n# Saturation detected: rapidly diminishing returns\nCopy\n```\n\n#### Configuration\n```\nconfig = AdaptiveConfig(\n    min_gain_threshold=0.1  # Stop if <10% new information\n)\nCopy\n```\n\n## Link Ranking Algorithm\n### Expected Information Gain\nEach uncrawled link is scored based on:\n```\nExpectedGain(link) = Relevance Ã— Novelty Ã— Authority\nCopy\n```\n\n#### 1. Relevance Scoring\nUses BM25 algorithm on link preview text:\n```\nrelevance = BM25(link.preview_text, query)\nCopy\n```\n\nFactors: - Term frequency in preview - Inverse document frequency - Preview length normalization\n#### 2. Novelty Estimation\nMeasures how different the link appears from already-crawled content:\n```\nnovelty = 1 - max_similarity(preview, knowledge_base)\nCopy\n```\n\nPrevents crawling duplicate or highly similar pages.\n#### 3. Authority Calculation\nURL structure and domain analysis:\n```\nauthority = f(domain_rank, url_depth, url_structure)\nCopy\n```\n\nFactors: - Domain reputation - URL depth (fewer slashes = higher authority) - Clean URL structure\n## Domain-Specific Configurations\n### Technical Documentation\n```\ntech_doc_config = AdaptiveConfig(\n    confidence_threshold=0.85,\n    max_pages=30,\n    top_k_links=3,\n    min_gain_threshold=0.05  # Keep crawling for small gains\n)\nCopy\n```\n\nRationale: - High threshold ensures comprehensive coverage - Lower gain threshold captures edge cases - Moderate link following for depth\n### News & Articles\n```\nnews_config = AdaptiveConfig(\n    confidence_threshold=0.6,\n    max_pages=10,\n    top_k_links=5,\n    min_gain_threshold=0.15  # Stop quickly on repetition\n)\nCopy\n```\n\nRationale: - Lower threshold (articles often repeat information) - Higher gain threshold (avoid duplicate stories) - More links per page (explore different perspectives)\n### E-commerce\n```\necommerce_config = AdaptiveConfig(\n    confidence_threshold=0.7,\n    max_pages=20,\n    top_k_links=2,\n    min_gain_threshold=0.1\n)\nCopy\n```\n\nRationale: - Balanced threshold for product variations - Focused link following (avoid infinite products) - Standard gain threshold\n### Research & Academic\n```\nresearch_config = AdaptiveConfig(\n    confidence_threshold=0.9,\n    max_pages=50,\n    top_k_links=4,\n    min_gain_threshold=0.02  # Very low - capture citations\n)\nCopy\n```\n\nRationale: - Very high threshold for completeness - Many pages allowed for thorough research - Very low gain threshold to capture references\n## Performance Optimization\n### Memory Management\n```\n# For large crawls, use streaming\nconfig = AdaptiveConfig(\n    max_pages=100,\n    save_state=True,\n    state_path=\"large_crawl.json\"\n)\n\n# Periodically clean state\nif len(state.knowledge_base) > 1000:\n    # Keep only the top 500 most relevant docs\n    top_content = adaptive.get_relevant_content(top_k=500)\n    keep_indices = {d[\"index\"] for d in top_content}\n    state.knowledge_base = [\n        doc for i, doc in enumerate(state.knowledge_base) if i in keep_indices\n    ]\nCopy\n```\n\n### Parallel Processing\n```\n# Use multiple start points\nstart_urls = [\n    \"https://docs.example.com/intro\",\n    \"https://docs.example.com/api\",\n    \"https://docs.example.com/guides\"\n]\n\n# Crawl in parallel\ntasks = [\n    adaptive.digest(url, query)\n    for url in start_urls\n]\nresults = await asyncio.gather(*tasks)\nCopy\n```\n\n## Debugging & Analysis\n### Enable Verbose Logging\n```\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG)\nadaptive = AdaptiveCrawler(crawler, config, verbose=True)\nCopy\n```\n\n### Analyze Crawl Patterns\n```\n# After crawling\nstate = await adaptive.digest(start_url, query)\n\n# Analyze link selection\nprint(\"Link selection order:\")\nfor i, url in enumerate(state.crawl_order):\n    print(f\"{i+1}. {url}\")\n\n# Analyze term discovery\nprint(\"\\nTerm discovery rate:\")\nfor i, new_terms in enumerate(state.new_terms_history):\n    print(f\"Page {i+1}: {new_terms} new terms\")\n\n# Analyze score progression\nprint(\"\\nScore progression:\")\nprint(f\"Coverage: {state.metrics['coverage_history']}\")\nprint(f\"Saturation: {state.metrics['saturation_history']}\")\nCopy\n```\n\n### Export for Analysis\n```\n# Export detailed metrics\nimport json\n\nmetrics = {\n    \"query\": query,\n    \"total_pages\": len(state.crawled_urls),\n    \"confidence\": adaptive.confidence,\n    \"coverage_stats\": adaptive.coverage_stats,\n    \"crawl_order\": state.crawl_order,\n    \"term_frequencies\": dict(state.term_frequencies),\n    \"new_terms_history\": state.new_terms_history\n}\n\nwith open(\"crawl_analysis.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\nCopy\n```\n\n## Custom Strategies\n### Implementing a Custom Strategy\n```\nfrom crawl4ai.adaptive_crawler import CrawlStrategy\n\nclass DomainSpecificStrategy(CrawlStrategy):\n    def calculate_coverage(self, state: CrawlState) -> float:\n        # Custom coverage calculation\n        # e.g., weight certain terms more heavily\n        pass\n\n    def calculate_consistency(self, state: CrawlState) -> float:\n        # Custom consistency logic\n        # e.g., domain-specific validation\n        pass\n\n    def rank_links(self, links: List[Link], state: CrawlState) -> List[Link]:\n        # Custom link ranking\n        # e.g., prioritize specific URL patterns\n        pass\n\n# Use custom strategy\nadaptive = AdaptiveCrawler(\n    crawler,\n    config=config,\n    strategy=DomainSpecificStrategy()\n)\nCopy\n```\n\n### Combining Strategies\n```\nclass HybridStrategy(CrawlStrategy):\n    def __init__(self):\n        self.strategies = [\n            TechnicalDocStrategy(),\n            SemanticSimilarityStrategy(),\n            URLPatternStrategy()\n        ]\n\n    def calculate_confidence(self, state: CrawlState) -> float:\n        # Weighted combination of strategies\n        scores = [s.calculate_confidence(state) for s in self.strategies]\n        weights = [0.5, 0.3, 0.2]\n        return sum(s * w for s, w in zip(scores, weights))\nCopy\n```\n\n## Best Practices\n### 1. Start Conservative\nBegin with default settings and adjust based on results:\n```\n# Start with defaults\nresult = await adaptive.digest(url, query)\n\n# Analyze and adjust\nif adaptive.confidence < 0.7:\n    config.max_pages += 10\n    config.confidence_threshold -= 0.1\nCopy\n```\n\n### 2. Monitor Resource Usage\n```\nimport psutil\n\n# Check memory before large crawls\nmemory_percent = psutil.virtual_memory().percent\nif memory_percent > 80:\n    config.max_pages = min(config.max_pages, 20)\nCopy\n```\n\n### 3. Use Domain Knowledge\n```\n# For API documentation\nif \"api\" in start_url:\n    config.top_k_links = 2  # APIs have clear structure\n\n# For blogs\nif \"blog\" in start_url:\n    config.min_gain_threshold = 0.2  # Avoid similar posts\nCopy\n```\n\n### 4. Validate Results\n```\n# Always validate the knowledge base\nrelevant_content = adaptive.get_relevant_content(top_k=10)\n\n# Check coverage\nquery_terms = set(query.lower().split())\ncovered_terms = set()\n\nfor doc in relevant_content:\n    content_lower = doc['content'].lower()\n    for term in query_terms:\n        if term in content_lower:\n            covered_terms.add(term)\n\ncoverage_ratio = len(covered_terms) / len(query_terms)\nprint(f\"Query term coverage: {coverage_ratio:.0%}\")\nCopy\n```\n\n## Next Steps\n  * Explore [Custom Strategy Implementation](https://docs.crawl4ai.com/advanced/tutorials/custom-adaptive-strategies.md)\n  * Learn about [Knowledge Base Management](https://docs.crawl4ai.com/advanced/tutorials/knowledge-base-management.md)\n  * See [Performance Benchmarks](https://docs.crawl4ai.com/advanced/benchmarks/adaptive-performance.md)\n\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n\n\nESC to close\n#### On this page\n  * [Overview](https://docs.crawl4ai.com/advanced/adaptive-strategies/#overview)\n  * [The Three-Layer Scoring System](https://docs.crawl4ai.com/advanced/adaptive-strategies/#the-three-layer-scoring-system)\n  * [1. Coverage Score](https://docs.crawl4ai.com/advanced/adaptive-strategies/#1-coverage-score)\n  * [Mathematical Foundation](https://docs.crawl4ai.com/advanced/adaptive-strategies/#mathematical-foundation)\n  * [Components](https://docs.crawl4ai.com/advanced/adaptive-strategies/#components)\n  * [Tuning Coverage](https://docs.crawl4ai.com/advanced/adaptive-strategies/#tuning-coverage)\n  * [2. Consistency Score](https://docs.crawl4ai.com/advanced/adaptive-strategies/#2-consistency-score)\n  * [How It Works](https://docs.crawl4ai.com/advanced/adaptive-strategies/#how-it-works)\n  * [Practical Impact](https://docs.crawl4ai.com/advanced/adaptive-strategies/#practical-impact)\n  * [3. Saturation Score](https://docs.crawl4ai.com/advanced/adaptive-strategies/#3-saturation-score)\n  * [Detection Algorithm](https://docs.crawl4ai.com/advanced/adaptive-strategies/#detection-algorithm)\n  * [Configuration](https://docs.crawl4ai.com/advanced/adaptive-strategies/#configuration)\n  * [Link Ranking Algorithm](https://docs.crawl4ai.com/advanced/adaptive-strategies/#link-ranking-algorithm)\n  * [Expected Information Gain](https://docs.crawl4ai.com/advanced/adaptive-strategies/#expected-information-gain)\n  * [1. Relevance Scoring](https://docs.crawl4ai.com/advanced/adaptive-strategies/#1-relevance-scoring)\n  * [2. Novelty Estimation](https://docs.crawl4ai.com/advanced/adaptive-strategies/#2-novelty-estimation)\n  * [3. Authority Calculation](https://docs.crawl4ai.com/advanced/adaptive-strategies/#3-authority-calculation)\n  * [Domain-Specific Configurations](https://docs.crawl4ai.com/advanced/adaptive-strategies/#domain-specific-configurations)\n  * [Technical Documentation](https://docs.crawl4ai.com/advanced/adaptive-strategies/#technical-documentation)\n  * [News & Articles](https://docs.crawl4ai.com/advanced/adaptive-strategies/#news-articles)\n  * [E-commerce](https://docs.crawl4ai.com/advanced/adaptive-strategies/#e-commerce)\n  * [Research & Academic](https://docs.crawl4ai.com/advanced/adaptive-strategies/#research-academic)\n  * [Performance Optimization](https://docs.crawl4ai.com/advanced/adaptive-strategies/#performance-optimization)\n  * [Memory Management](https://docs.crawl4ai.com/advanced/adaptive-strategies/#memory-management)\n  * [Parallel Processing](https://docs.crawl4ai.com/advanced/adaptive-strategies/#parallel-processing)\n  * [Debugging & Analysis](https://docs.crawl4ai.com/advanced/adaptive-strategies/#debugging-analysis)\n  * [Enable Verbose Logging](https://docs.crawl4ai.com/advanced/adaptive-strategies/#enable-verbose-logging)\n  * [Analyze Crawl Patterns](https://docs.crawl4ai.com/advanced/adaptive-strategies/#analyze-crawl-patterns)\n  * [Export for Analysis](https://docs.crawl4ai.com/advanced/adaptive-strategies/#export-for-analysis)\n  * [Custom Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/#custom-strategies)\n  * [Implementing a Custom Strategy](https://docs.crawl4ai.com/advanced/adaptive-strategies/#implementing-a-custom-strategy)\n  * [Combining Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/#combining-strategies)\n  * [Best Practices](https://docs.crawl4ai.com/advanced/adaptive-strategies/#best-practices)\n  * [1. Start Conservative](https://docs.crawl4ai.com/advanced/adaptive-strategies/#1-start-conservative)\n  * [2. Monitor Resource Usage](https://docs.crawl4ai.com/advanced/adaptive-strategies/#2-monitor-resource-usage)\n  * [3. Use Domain Knowledge](https://docs.crawl4ai.com/advanced/adaptive-strategies/#3-use-domain-knowledge)\n  * [4. Validate Results](https://docs.crawl4ai.com/advanced/adaptive-strategies/#4-validate-results)\n  * [Next Steps](https://docs.crawl4ai.com/advanced/adaptive-strategies/#next-steps)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/file-downloading",
    "depth": 1,
    "title": "File Downloading - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "2bb5bd0acf1e99ed4ac634ca2d67b3b0",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/file-downloading/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * File Downloading\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Download Handling in Crawl4AI](https://docs.crawl4ai.com/advanced/file-downloading/#download-handling-in-crawl4ai)\n  * [Enabling Downloads](https://docs.crawl4ai.com/advanced/file-downloading/#enabling-downloads)\n  * [Specifying Download Location](https://docs.crawl4ai.com/advanced/file-downloading/#specifying-download-location)\n  * [Triggering Downloads](https://docs.crawl4ai.com/advanced/file-downloading/#triggering-downloads)\n  * [Accessing Downloaded Files](https://docs.crawl4ai.com/advanced/file-downloading/#accessing-downloaded-files)\n  * [Example: Downloading Multiple Files](https://docs.crawl4ai.com/advanced/file-downloading/#example-downloading-multiple-files)\n  * [Important Considerations](https://docs.crawl4ai.com/advanced/file-downloading/#important-considerations)\n\n\n# Download Handling in Crawl4AI\nThis guide explains how to use Crawl4AI to handle file downloads during crawling. You'll learn how to trigger downloads, specify download locations, and access downloaded files.\n## Enabling Downloads\nTo enable downloads, set the `accept_downloads` parameter in the `BrowserConfig` object and pass it to the crawler.\n```\nfrom crawl4ai.async_configs import BrowserConfig, AsyncWebCrawler\n\nasync def main():\n    config = BrowserConfig(accept_downloads=True)  # Enable downloads globally\n    async with AsyncWebCrawler(config=config) as crawler:\n        # ... your crawling logic ...\n\nasyncio.run(main())\nCopy\n```\n\n## Specifying Download Location\nSpecify the download directory using the `downloads_path` attribute in the `BrowserConfig` object. If not provided, Crawl4AI defaults to creating a \"downloads\" directory inside the `.crawl4ai` folder in your home directory.\n```\nfrom crawl4ai.async_configs import BrowserConfig\nimport os\n\ndownloads_path = os.path.join(os.getcwd(), \"my_downloads\")  # Custom download path\nos.makedirs(downloads_path, exist_ok=True)\n\nconfig = BrowserConfig(accept_downloads=True, downloads_path=downloads_path)\n\nasync def main():\n    async with AsyncWebCrawler(config=config) as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        # ...\nCopy\n```\n\n## Triggering Downloads\nDownloads are typically triggered by user interactions on a web page, such as clicking a download button. Use `js_code` in `CrawlerRunConfig` to simulate these actions and `wait_for` to allow sufficient time for downloads to start.\n```\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nconfig = CrawlerRunConfig(\n    js_code=\"\"\"\n        const downloadLink = document.querySelector('a[href$=\".exe\"]');\n        if (downloadLink) {\n            downloadLink.click();\n        }\n    \"\"\",\n    wait_for=5  # Wait 5 seconds for the download to start\n)\n\nresult = await crawler.arun(url=\"https://www.python.org/downloads/\", config=config)\nCopy\n```\n\n## Accessing Downloaded Files\nThe `downloaded_files` attribute of the `CrawlResult` object contains paths to downloaded files.\n```\nif result.downloaded_files:\n    print(\"Downloaded files:\")\n    for file_path in result.downloaded_files:\n        print(f\"- {file_path}\")\n        file_size = os.path.getsize(file_path)\n        print(f\"- File size: {file_size} bytes\")\nelse:\n    print(\"No files downloaded.\")\nCopy\n```\n\n## Example: Downloading Multiple Files\n```\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nimport os\nfrom pathlib import Path\n\nasync def download_multiple_files(url: str, download_path: str):\n    config = BrowserConfig(accept_downloads=True, downloads_path=download_path)\n    async with AsyncWebCrawler(config=config) as crawler:\n        run_config = CrawlerRunConfig(\n            js_code=\"\"\"\n                const downloadLinks = document.querySelectorAll('a[download]');\n                for (const link of downloadLinks) {\n                    link.click();\n                    // Delay between clicks\n                    await new Promise(r => setTimeout(r, 2000));  \n                }\n            \"\"\",\n            wait_for=10  # Wait for all downloads to start\n        )\n        result = await crawler.arun(url=url, config=run_config)\n\n        if result.downloaded_files:\n            print(\"Downloaded files:\")\n            for file in result.downloaded_files:\n                print(f\"- {file}\")\n        else:\n            print(\"No files downloaded.\")\n\n# Usage\ndownload_path = os.path.join(Path.home(), \".crawl4ai\", \"downloads\")\nos.makedirs(download_path, exist_ok=True)\n\nasyncio.run(download_multiple_files(\"https://www.python.org/downloads/windows/\", download_path))\nCopy\n```\n\n## Important Considerations\n  * **Browser Context:** Downloads are managed within the browser context. Ensure `js_code` correctly targets the download triggers on the webpage.\n  * **Timing:** Use `wait_for` in `CrawlerRunConfig` to manage download timing.\n  * **Error Handling:** Handle errors to manage failed downloads or incorrect paths gracefully.\n  * **Security:** Scan downloaded files for potential security threats before use.\n\n\nThis revised guide ensures consistency with the `Crawl4AI` codebase by using `BrowserConfig` and `CrawlerRunConfig` for all download-related configurations. Let me know if further adjustments are needed!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/file-downloading/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/file-downloading/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/file-downloading/)\n\n\nESC to close\n#### On this page\n  * [Enabling Downloads](https://docs.crawl4ai.com/advanced/file-downloading/#enabling-downloads)\n  * [Specifying Download Location](https://docs.crawl4ai.com/advanced/file-downloading/#specifying-download-location)\n  * [Triggering Downloads](https://docs.crawl4ai.com/advanced/file-downloading/#triggering-downloads)\n  * [Accessing Downloaded Files](https://docs.crawl4ai.com/advanced/file-downloading/#accessing-downloaded-files)\n  * [Example: Downloading Multiple Files](https://docs.crawl4ai.com/advanced/file-downloading/#example-downloading-multiple-files)\n  * [Important Considerations](https://docs.crawl4ai.com/advanced/file-downloading/#important-considerations)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/lazy-loading",
    "depth": 1,
    "title": "Lazy Loading - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "a73d455017e996820c72e2fdd6bbe469",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/lazy-loading/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * Lazy Loading\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Handling Lazy-Loaded Images](https://docs.crawl4ai.com/advanced/lazy-loading/#handling-lazy-loaded-images)\n  * [Example: Ensuring Lazy Images Appear](https://docs.crawl4ai.com/advanced/lazy-loading/#example-ensuring-lazy-images-appear)\n  * [Combining with Other Link & Media Filters](https://docs.crawl4ai.com/advanced/lazy-loading/#combining-with-other-link-media-filters)\n  * [Tips & Troubleshooting](https://docs.crawl4ai.com/advanced/lazy-loading/#tips-troubleshooting)\n\n\n## Handling Lazy-Loaded Images\nMany websites now load images **lazily** as you scroll. If you need to ensure they appear in your final crawl (and in `result.media`), consider:\n1. **`wait_for_images=True`**â€“ Wait for images to fully load.  \n2. **`scan_full_page`**â€“ Force the crawler to scroll the entire page, triggering lazy loads.  \n3. **`scroll_delay`**â€“ Add small delays between scroll steps.\n**Note** : If the site requires multiple â€œLoad Moreâ€ triggers or complex interactions, see the [Page Interaction docs](https://docs.crawl4ai.com/core/page-interaction/). For sites with virtual scrolling (Twitter/Instagram style), see the [Virtual Scroll docs](https://docs.crawl4ai.com/advanced/virtual-scroll/).\n### Example: Ensuring Lazy Images Appear\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig\nfrom crawl4ai.async_configs import CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Force the crawler to wait until images are fully loaded\n        wait_for_images=True,\n\n        # Option 1: If you want to automatically scroll the page to load images\n        scan_full_page=True,  # Tells the crawler to try scrolling the entire page\n        scroll_delay=0.5,     # Delay (seconds) between scroll steps\n\n        # Option 2: If the site uses a 'Load More' or JS triggers for images,\n        # you can also specify js_code or wait_for logic here.\n\n        cache_mode=CacheMode.BYPASS,\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        result = await crawler.arun(\"https://www.example.com/gallery\", config=config)\n\n        if result.success:\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n            for i, img in enumerate(images[:5]):\n                print(f\"[Image {i}] URL: {img['src']}, Score: {img.get('score','N/A')}\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Explanation** :\n  * **`wait_for_images=True`**  \nThe crawler tries to ensure images have finished loading before finalizing the HTML.\n  * **`scan_full_page=True`**  \nTells the crawler to attempt scrolling from top to bottom. Each scroll step helps trigger lazy loading.\n  * **`scroll_delay=0.5`**  \nPause half a second between each scroll step. Helps the site load images before continuing.\n\n\n**When to Use** :\n  * **Lazy-Loading** : If images appear only when the user scrolls into view, `scan_full_page` + `scroll_delay` helps the crawler see them. \n  * **Heavier Pages** : If a page is extremely long, be mindful that scanning the entire page can be slow. Adjust `scroll_delay` or the max scroll steps as needed.\n\n\n* * *\n## Combining with Other Link & Media Filters\nYou can still combine **lazy-load** logic with the usual **exclude_external_images** , **exclude_domains** , or link filtration:\n```\nconfig = CrawlerRunConfig(\n    wait_for_images=True,\n    scan_full_page=True,\n    scroll_delay=0.5,\n\n    # Filter out external images if you only want local ones\n    exclude_external_images=True,\n\n    # Exclude certain domains for links\n    exclude_domains=[\"spammycdn.com\"],\n)\nCopy\n```\n\nThis approach ensures you see **all** images from the main domain while ignoring external ones, and the crawler physically scrolls the entire page so that lazy-loading triggers.\n* * *\n## Tips & Troubleshooting\n1. **Long Pages**  \n- Setting `scan_full_page=True` on extremely long or infinite-scroll pages can be resource-intensive.  \n- Consider using [hooks](https://docs.crawl4ai.com/core/page-interaction/) or specialized logic to load specific sections or â€œLoad Moreâ€ triggers repeatedly.\n2. **Mixed Image Behavior**  \n- Some sites load images in batches as you scroll. If youâ€™re missing images, increase your `scroll_delay` or call multiple partial scrolls in a loop with JS code or hooks.\n3. **Combining with Dynamic Wait**  \n- If the site has a placeholder that only changes to a real image after a certain event, you might do `wait_for=\"css:img.loaded\"` or a custom JS `wait_for`.\n4. **Caching**  \n- If `cache_mode` is enabled, repeated crawls might skip some network fetches. If you suspect caching is missing new images, set `cache_mode=CacheMode.BYPASS` for fresh fetches.\n* * *\nWith **lazy-loading** support, **wait_for_images** , and **scan_full_page** settings, you can capture the entire gallery or feed of images you expectâ€”even if the site only loads them as the user scrolls. Combine these with the standard media filtering and domain exclusion for a complete link & media handling strategy.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/lazy-loading/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/lazy-loading/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/lazy-loading/)\n\n\nESC to close\n#### On this page\n  * [Handling Lazy-Loaded Images](https://docs.crawl4ai.com/advanced/lazy-loading/#handling-lazy-loaded-images)\n  * [Example: Ensuring Lazy Images Appear](https://docs.crawl4ai.com/advanced/lazy-loading/#example-ensuring-lazy-images-appear)\n  * [Combining with Other Link & Media Filters](https://docs.crawl4ai.com/advanced/lazy-loading/#combining-with-other-link-media-filters)\n  * [Tips & Troubleshooting](https://docs.crawl4ai.com/advanced/lazy-loading/#tips-troubleshooting)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/crawl-dispatcher",
    "depth": 1,
    "title": "Crawl Dispatcher - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "73465168327582dc18a02fe550de5e63",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * Crawl Dispatcher\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/#crawl-dispatcher)\n\n\n# Crawl Dispatcher\nWeâ€™re excited to announce a **Crawl Dispatcher** module that can handle **thousands** of crawling tasks simultaneously. By efficiently managing system resources (memory, CPU, network), this dispatcher ensures high-performance data extraction at scale. It also provides **real-time monitoring** of each crawlerâ€™s status, memory usage, and overall progress.\nStay tunedâ€”this feature is **coming soon** in an upcoming release of Crawl4AI! For the latest news, keep an eye on our changelogs and follow [@unclecode](https://twitter.com/unclecode) on X.\nBelow is a **sample** of how the dispatcherâ€™s performance monitor might look in action:\n![Crawl Dispatcher Performance Monitor](https://docs.crawl4ai.com/assets/images/dispatcher.png)\nWe canâ€™t wait to bring you this streamlined, **scalable** approach to multi-URL crawlingâ€”**watch this space** for updates!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n\n\nESC to close\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/advanced-features",
    "depth": 1,
    "title": "Overview - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "cd7e3e28e7bb9d393ce5483f8d1240d2",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/advanced-features/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * Overview\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Overview of Some Important Advanced Features](https://docs.crawl4ai.com/advanced/advanced-features/#overview-of-some-important-advanced-features)\n  * [1. Proxy Usage](https://docs.crawl4ai.com/advanced/advanced-features/#1-proxy-usage)\n  * [2. Capturing PDFs & Screenshots](https://docs.crawl4ai.com/advanced/advanced-features/#2-capturing-pdfs-screenshots)\n  * [3. Handling SSL Certificates](https://docs.crawl4ai.com/advanced/advanced-features/#3-handling-ssl-certificates)\n  * [4. Custom Headers](https://docs.crawl4ai.com/advanced/advanced-features/#4-custom-headers)\n  * [5. Session Persistence & Local Storage](https://docs.crawl4ai.com/advanced/advanced-features/#5-session-persistence-local-storage)\n  * [6. Robots.txt Compliance](https://docs.crawl4ai.com/advanced/advanced-features/#6-robotstxt-compliance)\n  * [Putting It All Together](https://docs.crawl4ai.com/advanced/advanced-features/#putting-it-all-together)\n  * [7. Anti-Bot Features (Stealth Mode & Undetected Browser)](https://docs.crawl4ai.com/advanced/advanced-features/#7-anti-bot-features-stealth-mode-undetected-browser)\n  * [Conclusion & Next Steps](https://docs.crawl4ai.com/advanced/advanced-features/#conclusion-next-steps)\n\n\n# Overview of Some Important Advanced Features\n(Proxy, PDF, Screenshot, SSL, Headers, & Storage State)\nCrawl4AI offers multiple power-user features that go beyond simple crawling. This tutorial covers:\n1. **Proxy Usage**  \n2. **Capturing PDFs & Screenshots**  \n3. **Handling SSL Certificates**  \n4. **Custom Headers**  \n5. **Session Persistence & Local Storage**  \n6. **Robots.txt Compliance**\n> **Prerequisites**  \n>  - You have a basic grasp of [AsyncWebCrawler Basics](https://docs.crawl4ai.com/core/simple-crawling/)  \n>  - You know how to run or configure your Python environment with Playwright installed\n* * *\n## 1. Proxy Usage\nIf you need to route your crawl traffic through a proxyâ€”whether for IP rotation, geo-testing, or privacyâ€”Crawl4AI supports it via `BrowserConfig.proxy_config`.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True\n    )\n    crawler_cfg = CrawlerRunConfig(\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.whatismyip.com/\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Page fetched via proxy.\")\n            print(\"Page HTML snippet:\", result.html[:200])\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Points**  \n- **`proxy_config`**expects a dict with`server` and optional auth credentials.  \n- Many commercial proxies provide an HTTP/HTTPS â€œgatewayâ€ server that you specify in `server`.  \n- If your proxy doesnâ€™t need auth, omit `username`/`password`.\n* * *\n## 2. Capturing PDFs & Screenshots\nSometimes you need a visual record of a page or a PDF â€œprintout.â€ Crawl4AI can do both in one pass:\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig\n\nasync def main():\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        screenshot=True,\n        pdf=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n            config=run_config\n        )\n        if result.success:\n            print(f\"Screenshot data present: {result.screenshot is not None}\")\n            print(f\"PDF data present: {result.pdf is not None}\")\n\n            if result.screenshot:\n                print(f\"[OK] Screenshot captured, size: {len(result.screenshot)} bytes\")\n                with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            else:\n                print(\"[WARN] Screenshot data is None.\")\n\n            if result.pdf:\n                print(f\"[OK] PDF captured, size: {len(result.pdf)} bytes\")\n                with open(\"wikipedia_page.pdf\", \"wb\") as f:\n                    f.write(result.pdf)\n            else:\n                print(\"[WARN] PDF data is None.\")\n\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Why PDF + Screenshot?**  \n- Large or complex pages can be slow or error-prone with â€œtraditionalâ€ full-page screenshots.  \n- Exporting a PDF is more reliable for very long pages. Crawl4AI automatically converts the first PDF page into an image if you request both. \n**Relevant Parameters**  \n- **`pdf=True`**: Exports the current page as a PDF (base64-encoded in`result.pdf`).  \n- **`screenshot=True`**: Creates a screenshot (base64-encoded in`result.screenshot`).  \n- **`scan_full_page`**or advanced hooking can further refine how the crawler captures content.\n* * *\n## 3. Handling SSL Certificates\nIf you need to verify or export a siteâ€™s SSL certificateâ€”for compliance, debugging, or data analysisâ€”Crawl4AI can fetch it during the crawl:\n```\nimport asyncio, os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = os.path.join(os.getcwd(), \"tmp\")\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n\n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            print(\"\\nCertificate Information:\")\n            print(f\"Issuer (CN): {cert.issuer.get('CN', '')}\")\n            print(f\"Valid until: {cert.valid_until}\")\n            print(f\"Fingerprint: {cert.fingerprint}\")\n\n            # Export in multiple formats:\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n\n            print(\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\")\n        else:\n            print(\"[ERROR] No certificate or crawl failed.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Points**  \n- **`fetch_ssl_certificate=True`**triggers certificate retrieval.  \n- `result.ssl_certificate` includes methods (`to_json`, `to_pem`, `to_der`) for saving in various formats (handy for server config, Java keystores, etc.).\n* * *\n## 4. Custom Headers\nSometimes you need to set custom headers (e.g., language preferences, authentication tokens, or specialized user-agent strings). You can do this in multiple ways:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Option 1: Set headers at the crawler strategy level\n    crawler1 = AsyncWebCrawler(\n        # The underlying strategy can accept headers in its constructor\n        crawler_strategy=None  # We'll override below for clarity\n    )\n    crawler1.crawler_strategy.update_user_agent(\"MyCustomUA/1.0\")\n    crawler1.crawler_strategy.set_custom_headers({\n        \"Accept-Language\": \"fr-FR,fr;q=0.9\"\n    })\n    result1 = await crawler1.arun(\"https://www.example.com\")\n    print(\"Example 1 result success:\", result1.success)\n\n    # Option 2: Pass headers directly to `arun()`\n    crawler2 = AsyncWebCrawler()\n    result2 = await crawler2.arun(\n        url=\"https://www.example.com\",\n        headers={\"Accept-Language\": \"es-ES,es;q=0.9\"}\n    )\n    print(\"Example 2 result success:\", result2.success)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Notes**  \n- Some sites may react differently to certain headers (e.g., `Accept-Language`).  \n- If you need advanced user-agent randomization or client hints, see [Identity-Based Crawling (Anti-Bot)](https://docs.crawl4ai.com/advanced/identity-based-crawling/) or use `UserAgentGenerator`.\n* * *\n## 5. Session Persistence & Local Storage\nCrawl4AI can preserve cookies and localStorage so you can continue where you left offâ€”ideal for logging into sites or skipping repeated auth flows.\n### 5.1 `storage_state`\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    storage_dict = {\n        \"cookies\": [\n            {\n                \"name\": \"session\",\n                \"value\": \"abcd1234\",\n                \"domain\": \"example.com\",\n                \"path\": \"/\",\n                \"expires\": 1699999999.0,\n                \"httpOnly\": False,\n                \"secure\": False,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"origins\": [\n            {\n                \"origin\": \"https://example.com\",\n                \"localStorage\": [\n                    {\"name\": \"token\", \"value\": \"my_auth_token\"}\n                ]\n            }\n        ]\n    }\n\n    # Provide the storage state as a dictionary to start \"already logged in\"\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=storage_dict\n    ) as crawler:\n        result = await crawler.arun(\"https://example.com/protected\")\n        if result.success:\n            print(\"Protected page content length:\", len(result.html))\n        else:\n            print(\"Failed to crawl protected page\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 5.2 Exporting & Reusing State\nYou can sign in once, export the browser context, and reuse it laterâ€”without re-entering credentials.\n  * **`await context.storage_state(path=\"my_storage.json\")`**: Exports cookies, localStorage, etc. to a file.\n  * Provide `storage_state=\"my_storage.json\"` on subsequent runs to skip the login step.\n\n\n**See** : [Detailed session management tutorial](https://docs.crawl4ai.com/advanced/session-management/) or [Explanations â†’ Browser Context & Managed Browser](https://docs.crawl4ai.com/advanced/identity-based-crawling/) for more advanced scenarios (like multi-step logins, or capturing after interactive pages).\n* * *\n## 6. Robots.txt Compliance\nCrawl4AI supports respecting robots.txt rules with efficient caching:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Enable robots.txt checking in config\n    config = CrawlerRunConfig(\n        check_robots_txt=True  # Will check and respect robots.txt rules\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://example.com\",\n            config=config\n        )\n\n        if not result.success and result.status_code == 403:\n            print(\"Access denied by robots.txt\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Points** - Robots.txt files are cached locally for efficiency - Cache is stored in `~/.crawl4ai/robots/robots_cache.db` - Cache has a default TTL of 7 days - If robots.txt can't be fetched, crawling is allowed - Returns 403 status code if URL is disallowed\n* * *\n## Putting It All Together\nHereâ€™s a snippet that combines multiple â€œadvancedâ€ features (proxy, PDF, screenshot, SSL, custom headers, and session reuse) into one run. Normally, youâ€™d tailor each setting to your projectâ€™s needs.\n```\nimport os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # 1. Browser config with proxy + headless\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True,\n    )\n\n    # 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches\n    crawler_cfg = CrawlerRunConfig(\n        pdf=True,\n        screenshot=True,\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS,\n        headers={\"Accept-Language\": \"en-US,en;q=0.8\"},\n        storage_state=\"my_storage.json\",  # Reuse session from a previous sign-in\n        verbose=True,\n    )\n\n    # 3. Crawl\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url = \"https://secure.example.com/protected\", \n            config=crawler_cfg\n        )\n\n        if result.success:\n            print(\"[OK] Crawled the secure page. Links found:\", len(result.links.get(\"internal\", [])))\n\n            # Save PDF & screenshot\n            if result.pdf:\n                with open(\"result.pdf\", \"wb\") as f:\n                    f.write(b64decode(result.pdf))\n            if result.screenshot:\n                with open(\"result.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n\n            # Check SSL cert\n            if result.ssl_certificate:\n                print(\"SSL Issuer CN:\", result.ssl_certificate.issuer.get(\"CN\", \"\"))\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n* * *\n## 7. Anti-Bot Features (Stealth Mode & Undetected Browser)\nCrawl4AI provides two powerful features to bypass bot detection:\n### 7.1 Stealth Mode\nStealth mode uses playwright-stealth to modify browser fingerprints and behaviors. Enable it with a simple flag:\n```\nbrowser_config = BrowserConfig(\n    enable_stealth=True,  # Activates stealth mode\n    headless=False\n)\nCopy\n```\n\n**When to use** : Sites with basic bot detection (checking navigator.webdriver, plugins, etc.)\n### 7.2 Undetected Browser\nFor advanced bot detection, use the undetected browser adapter:\n```\nfrom crawl4ai import UndetectedAdapter\nfrom crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n\n# Create undetected adapter\nadapter = UndetectedAdapter()\nstrategy = AsyncPlaywrightCrawlerStrategy(\n    browser_config=browser_config,\n    browser_adapter=adapter\n)\n\nasync with AsyncWebCrawler(crawler_strategy=strategy, config=browser_config) as crawler:\n    # Your crawling code\nCopy\n```\n\n**When to use** : Sites with sophisticated bot detection (Cloudflare, DataDome, etc.)\n### 7.3 Combining Both\nFor maximum evasion, combine stealth mode with undetected browser:\n```\nbrowser_config = BrowserConfig(\n    enable_stealth=True,  # Enable stealth\n    headless=False\n)\n\nadapter = UndetectedAdapter()  # Use undetected browser\nCopy\n```\n\n### Choosing the Right Approach\nDetection Level | Recommended Approach  \n---|---  \nNo protection | Regular browser  \nBasic checks | Regular + Stealth mode  \nAdvanced protection | Undetected browser  \nMaximum evasion | Undetected + Stealth mode  \n**Best Practice** : Start with regular browser + stealth mode. Only use undetected browser if needed, as it may be slightly slower.\nSee [Undetected Browser Mode](https://docs.crawl4ai.com/advanced/undetected-browser/) for detailed examples.\n* * *\n## Conclusion & Next Steps\nYou've now explored several **advanced** features:\n  * **Proxy Usage**\n  * **PDF & Screenshot** capturing for large or critical pages \n  * **SSL Certificate** retrieval & exporting \n  * **Custom Headers** for language or specialized requests \n  * **Session Persistence** via storage state\n  * **Robots.txt Compliance**\n  * **Anti-Bot Features** (Stealth Mode & Undetected Browser)\n\n\nWith these power tools, you can build robust scraping workflows that mimic real user behavior, handle secure sites, capture detailed snapshots, manage sessions across multiple runs, and bypass bot detectionâ€”streamlining your entire data collection pipeline.\n**Note** : In future versions, we may enable stealth mode and undetected browser by default. For now, users should explicitly enable these features when needed.\n**Last Updated** : 2025-01-17\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/advanced-features/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/advanced-features/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/advanced-features/)\n\n\nESC to close\n#### On this page\n  * [1. Proxy Usage](https://docs.crawl4ai.com/advanced/advanced-features/#1-proxy-usage)\n  * [2. Capturing PDFs & Screenshots](https://docs.crawl4ai.com/advanced/advanced-features/#2-capturing-pdfs-screenshots)\n  * [3. Handling SSL Certificates](https://docs.crawl4ai.com/advanced/advanced-features/#3-handling-ssl-certificates)\n  * [4. Custom Headers](https://docs.crawl4ai.com/advanced/advanced-features/#4-custom-headers)\n  * [5. Session Persistence & Local Storage](https://docs.crawl4ai.com/advanced/advanced-features/#5-session-persistence-local-storage)\n  * [5.1 storage_state](https://docs.crawl4ai.com/advanced/advanced-features/#51-storage_state)\n  * [5.2 Exporting & Reusing State](https://docs.crawl4ai.com/advanced/advanced-features/#52-exporting-reusing-state)\n  * [6. Robots.txt Compliance](https://docs.crawl4ai.com/advanced/advanced-features/#6-robotstxt-compliance)\n  * [Putting It All Together](https://docs.crawl4ai.com/advanced/advanced-features/#putting-it-all-together)\n  * [7. Anti-Bot Features (Stealth Mode & Undetected Browser)](https://docs.crawl4ai.com/advanced/advanced-features/#7-anti-bot-features-stealth-mode-undetected-browser)\n  * [7.1 Stealth Mode](https://docs.crawl4ai.com/advanced/advanced-features/#71-stealth-mode)\n  * [7.2 Undetected Browser](https://docs.crawl4ai.com/advanced/advanced-features/#72-undetected-browser)\n  * [7.3 Combining Both](https://docs.crawl4ai.com/advanced/advanced-features/#73-combining-both)\n  * [Choosing the Right Approach](https://docs.crawl4ai.com/advanced/advanced-features/#choosing-the-right-approach)\n  * [Conclusion & Next Steps](https://docs.crawl4ai.com/advanced/advanced-features/#conclusion-next-steps)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/hooks-auth",
    "depth": 1,
    "title": "Hooks & Auth - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "02b080be08d6089746f80d2d1ec1f2a4",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/hooks-auth/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * Hooks & Auth\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Hooks & Auth in AsyncWebCrawler](https://docs.crawl4ai.com/advanced/hooks-auth/#hooks-auth-in-asyncwebcrawler)\n  * [Example: Using Hooks in AsyncWebCrawler](https://docs.crawl4ai.com/advanced/hooks-auth/#example-using-hooks-in-asyncwebcrawler)\n  * [Hook Lifecycle Summary](https://docs.crawl4ai.com/advanced/hooks-auth/#hook-lifecycle-summary)\n  * [When to Handle Authentication](https://docs.crawl4ai.com/advanced/hooks-auth/#when-to-handle-authentication)\n  * [Additional Considerations](https://docs.crawl4ai.com/advanced/hooks-auth/#additional-considerations)\n  * [Conclusion](https://docs.crawl4ai.com/advanced/hooks-auth/#conclusion)\n\n\n# Hooks & Auth in AsyncWebCrawler\nCrawl4AIâ€™s **hooks** let you customize the crawler at specific points in the pipeline:\n1. **`on_browser_created`**â€“ After browser creation.  \n2. **`on_page_context_created`**â€“ After a new context & page are created.  \n3. **`before_goto`**â€“ Just before navigating to a page.  \n4. **`after_goto`**â€“ Right after navigation completes.  \n5. **`on_user_agent_updated`**â€“ Whenever the user agent changes.  \n6. **`on_execution_started`**â€“ Once custom JavaScript execution begins.  \n7. **`before_retrieve_html`**â€“ Just before the crawler retrieves final HTML.  \n8. **`before_return_html`**â€“ Right before returning the HTML content.\n**Important** : Avoid heavy tasks in `on_browser_created` since you donâ€™t yet have a page context. If you need to _log in_ , do so in **`on_page_context_created`**.\n> note \"Important Hook Usage Warning\" **Avoid Misusing Hooks** : Do not manipulate page objects in the wrong hook or at the wrong time, as it can crash the pipeline or produce incorrect results. A common mistake is attempting to handle authentication prematurelyâ€”such as creating or closing pages in `on_browser_created`. \n> **Use the Right Hook for Auth** : If you need to log in or set tokens, use `on_page_context_created`. This ensures you have a valid page/context to work with, without disrupting the main crawling flow.\n> **Identity-Based Crawling** : For robust auth, consider identity-based crawling (or passing a session ID) to preserve state. Run your initial login steps in a separate, well-defined process, then feed that session to your main crawlâ€”rather than shoehorning complex authentication into early hooks. Check out [Identity-Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/) for more details.\n> **Be Cautious** : Overwriting or removing elements in the wrong hook can compromise the final crawl. Keep hooks focused on smaller tasks (like route filters, custom headers), and let your main logic (crawling, data extraction) proceed normally.\nBelow is an example demonstration.\n* * *\n## Example: Using Hooks in AsyncWebCrawler\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom playwright.async_api import Page, BrowserContext\n\nasync def main():\n    print(\"ðŸ”— Hooks Example: Demonstrating recommended usage\")\n\n    # 1) Configure the browser\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=True\n    )\n\n    # 2) Configure the crawler run\n    crawler_run_config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"body\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3) Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n\n    #\n    # Define Hook Functions\n    #\n\n    async def on_browser_created(browser, **kwargs):\n        # Called once the browser instance is created (but no pages or contexts yet)\n        print(\"[HOOK] on_browser_created - Browser created successfully!\")\n        # Typically, do minimal setup here if needed\n        return browser\n\n    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):\n        # Called right after a new page + context are created (ideal for auth or route config).\n        print(\"[HOOK] on_page_context_created - Setting up page & context.\")\n\n        # Example 1: Route filtering (e.g., block images)\n        async def route_filter(route):\n            if route.request.resource_type == \"image\":\n                print(f\"[HOOK] Blocking image request: {route.request.url}\")\n                await route.abort()\n            else:\n                await route.continue_()\n\n        await context.route(\"**\", route_filter)\n\n        # Example 2: (Optional) Simulate a login scenario\n        # (We do NOT create or close pages here, just do quick steps if needed)\n        # e.g., await page.goto(\"https://example.com/login\")\n        # e.g., await page.fill(\"input[name='username']\", \"testuser\")\n        # e.g., await page.fill(\"input[name='password']\", \"password123\")\n        # e.g., await page.click(\"button[type='submit']\")\n        # e.g., await page.wait_for_selector(\"#welcome\")\n        # e.g., await context.add_cookies([...])\n        # Then continue\n\n        # Example 3: Adjust the viewport\n        await page.set_viewport_size({\"width\": 1080, \"height\": 600})\n        return page\n\n    async def before_goto(\n        page: Page, context: BrowserContext, url: str, **kwargs\n    ):\n        # Called before navigating to each URL.\n        print(f\"[HOOK] before_goto - About to navigate: {url}\")\n        # e.g., inject custom headers\n        await page.set_extra_http_headers({\n            \"Custom-Header\": \"my-value\"\n        })\n        return page\n\n    async def after_goto(\n        page: Page, context: BrowserContext, \n        url: str, response, **kwargs\n    ):\n        # Called after navigation completes.\n        print(f\"[HOOK] after_goto - Successfully loaded: {url}\")\n        # e.g., wait for a certain element if we want to verify\n        try:\n            await page.wait_for_selector('.content', timeout=1000)\n            print(\"[HOOK] Found .content element!\")\n        except:\n            print(\"[HOOK] .content not found, continuing anyway.\")\n        return page\n\n    async def on_user_agent_updated(\n        page: Page, context: BrowserContext, \n        user_agent: str, **kwargs\n    ):\n        # Called whenever the user agent updates.\n        print(f\"[HOOK] on_user_agent_updated - New user agent: {user_agent}\")\n        return page\n\n    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):\n        # Called after custom JavaScript execution begins.\n        print(\"[HOOK] on_execution_started - JS code is running!\")\n        return page\n\n    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):\n        # Called before final HTML retrieval.\n        print(\"[HOOK] before_retrieve_html - We can do final actions\")\n        # Example: Scroll again\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n        return page\n\n    async def before_return_html(\n        page: Page, context: BrowserContext, html: str, **kwargs\n    ):\n        # Called just before returning the HTML in the result.\n        print(f\"[HOOK] before_return_html - HTML length: {len(html)}\")\n        return page\n\n    #\n    # Attach Hooks\n    #\n\n    crawler.crawler_strategy.set_hook(\"on_browser_created\", on_browser_created)\n    crawler.crawler_strategy.set_hook(\n        \"on_page_context_created\", on_page_context_created\n    )\n    crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n    crawler.crawler_strategy.set_hook(\"after_goto\", after_goto)\n    crawler.crawler_strategy.set_hook(\n        \"on_user_agent_updated\", on_user_agent_updated\n    )\n    crawler.crawler_strategy.set_hook(\n        \"on_execution_started\", on_execution_started\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_retrieve_html\", before_retrieve_html\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_return_html\", before_return_html\n    )\n\n    await crawler.start()\n\n    # 4) Run the crawler on an example page\n    url = \"https://example.com\"\n    result = await crawler.arun(url, config=crawler_run_config)\n\n    if result.success:\n        print(\"\\nCrawled URL:\", result.url)\n        print(\"HTML length:\", len(result.html))\n    else:\n        print(\"Error:\", result.error_message)\n\n    await crawler.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## Hook Lifecycle Summary\n1. **`on_browser_created`**:  \n- Browser is up, but **no** pages or contexts yet.  \n- Light setup onlyâ€”donâ€™t try to open or close pages here (that belongs in `on_page_context_created`).\n2. **`on_page_context_created`**:  \n- Perfect for advanced **auth** or route blocking.  \n- You have a **page** + **context** ready but havenâ€™t navigated to the target URL yet.\n3. **`before_goto`**:  \n- Right before navigation. Typically used for setting **custom headers** or logging the target URL.\n4. **`after_goto`**:  \n- After page navigation is done. Good place for verifying content or waiting on essential elements. \n5. **`on_user_agent_updated`**:  \n- Whenever the user agent changes (for stealth or different UA modes).\n6. **`on_execution_started`**:  \n- If you set `js_code` or run custom scripts, this runs once your JS is about to start.\n7. **`before_retrieve_html`**:  \n- Just before the final HTML snapshot is taken. Often you do a final scroll or lazy-load triggers here.\n8. **`before_return_html`**:  \n- The last hook before returning HTML to the `CrawlResult`. Good for logging HTML length or minor modifications.\n* * *\n## When to Handle Authentication\n**Recommended** : Use **`on_page_context_created`**if you need to:\n  * Navigate to a login page or fill forms\n  * Set cookies or localStorage tokens\n  * Block resource routes to avoid ads\n\n\nThis ensures the newly created context is under your control **before** `arun()` navigates to the main URL.\n* * *\n## Additional Considerations\n  * **Session Management** : If you want multiple `arun()` calls to reuse a single session, pass `session_id=` in your `CrawlerRunConfig`. Hooks remain the same. \n  * **Performance** : Hooks can slow down crawling if they do heavy tasks. Keep them concise. \n  * **Error Handling** : If a hook fails, the overall crawl might fail. Catch exceptions or handle them gracefully. \n  * **Concurrency** : If you run `arun_many()`, each URL triggers these hooks in parallel. Ensure your hooks are thread/async-safe.\n\n\n* * *\n## Conclusion\nHooks provide **fine-grained** control over:\n  * **Browser** creation (light tasks only)\n  * **Page** and **context** creation (auth, route blocking)\n  * **Navigation** phases\n  * **Final HTML** retrieval\n\n\nFollow the recommended usage: - **Login** or advanced tasks in `on_page_context_created`  \n- **Custom headers** or logs in `before_goto` / `after_goto`  \n- **Scrolling** or final checks in `before_retrieve_html` / `before_return_html`\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/hooks-auth/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/hooks-auth/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/hooks-auth/)\n\n\nESC to close\n#### On this page\n  * [Example: Using Hooks in AsyncWebCrawler](https://docs.crawl4ai.com/advanced/hooks-auth/#example-using-hooks-in-asyncwebcrawler)\n  * [Hook Lifecycle Summary](https://docs.crawl4ai.com/advanced/hooks-auth/#hook-lifecycle-summary)\n  * [When to Handle Authentication](https://docs.crawl4ai.com/advanced/hooks-auth/#when-to-handle-authentication)\n  * [Additional Considerations](https://docs.crawl4ai.com/advanced/hooks-auth/#additional-considerations)\n  * [Conclusion](https://docs.crawl4ai.com/advanced/hooks-auth/#conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/identity-based-crawling",
    "depth": 1,
    "title": "Identity Based Crawling - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "fa83bfc42d359b61591651e9900b2737",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * Identity Based Crawling\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Preserve Your Identity with Crawl4AI](https://docs.crawl4ai.com/advanced/identity-based-crawling/#preserve-your-identity-with-crawl4ai)\n  * [1. Managed Browsers: Your Digital Identity Solution](https://docs.crawl4ai.com/advanced/identity-based-crawling/#1-managed-browsers-your-digital-identity-solution)\n  * [3. Using Managed Browsers in Crawl4AI](https://docs.crawl4ai.com/advanced/identity-based-crawling/#3-using-managed-browsers-in-crawl4ai)\n  * [4. Magic Mode: Simplified Automation](https://docs.crawl4ai.com/advanced/identity-based-crawling/#4-magic-mode-simplified-automation)\n  * [5. Comparing Managed Browsers vs. Magic Mode](https://docs.crawl4ai.com/advanced/identity-based-crawling/#5-comparing-managed-browsers-vs-magic-mode)\n  * [6. Using the BrowserProfiler Class](https://docs.crawl4ai.com/advanced/identity-based-crawling/#6-using-the-browserprofiler-class)\n  * [7. Locale, Timezone, and Geolocation Control](https://docs.crawl4ai.com/advanced/identity-based-crawling/#7-locale-timezone-and-geolocation-control)\n  * [8. Summary](https://docs.crawl4ai.com/advanced/identity-based-crawling/#8-summary)\n\n\n# Preserve Your Identity with Crawl4AI\nCrawl4AI empowers you to navigate and interact with the web using your **authentic digital identity** , ensuring youâ€™re recognized as a human and not mistaken for a bot. This tutorial covers:\n1. **Managed Browsers** â€“ The recommended approach for persistent profiles and identity-based crawling.  \n2. **Magic Mode** â€“ A simplified fallback solution for quick automation without persistent identity.\n* * *\n## 1. Managed Browsers: Your Digital Identity Solution\n**Managed Browsers** let developers create and use **persistent browser profiles**. These profiles store local storage, cookies, and other session data, letting you browse as your **real self** â€”complete with logins, preferences, and cookies.\n### Key Benefits\n  * **Authentic Browsing Experience** : Retain session data and browser fingerprints as though youâ€™re a normal user. \n  * **Effortless Configuration** : Once you log in or solve CAPTCHAs in your chosen data directory, you can re-run crawls without repeating those steps. \n  * **Empowered Data Access** : If you can see the data in your own browser, you can automate its retrieval with your genuine identity.\n\n\n* * *\nBelow is a **partial update** to your **Managed Browsers** tutorial, specifically the section about **creating a user-data directory** using **Playwrightâ€™s Chromium** binary rather than a system-wide Chrome/Edge. Weâ€™ll show how to **locate** that binary and launch it with a `--user-data-dir` argument to set up your profile. You can then point `BrowserConfig.user_data_dir` to that folder for subsequent crawls.\n* * *\n### Creating a User Data Directory (Command-Line Approach via Playwright)\nIf you installed Crawl4AI (which installs Playwright under the hood), you already have a Playwright-managed Chromium on your system. Follow these steps to launch that **Chromium** from your command line, specifying a **custom** data directory:\n1. **Find** the Playwright Chromium binary: - On most systems, installed browsers go under a `~/.cache/ms-playwright/` folder or similar path.  \n- To see an overview of installed browsers, run: \n```\npython -m playwright install --dry-run\nCopy\n```\n\nor \n```\nplaywright install --dry-run\nCopy\n```\n\n(depending on your environment). This shows where Playwright keeps Chromium.\n  * For instance, you might see a path like: \n```\n~/.cache/ms-playwright/chromium-1234/chrome-linux/chrome\nCopy\n```\n\non Linux, or a corresponding folder on macOS/Windows.\n\n\n2. **Launch** the Playwright Chromium binary with a **custom** user-data directory: \n```\n# Linux example\n~/.cache/ms-playwright/chromium-1234/chrome-linux/chrome \\\n    --user-data-dir=/home/<you>/my_chrome_profile\nCopy\n```\n\n```\n# macOS example (Playwrightâ€™s internal binary)\n~/Library/Caches/ms-playwright/chromium-1234/chrome-mac/Chromium.app/Contents/MacOS/Chromium \\\n    --user-data-dir=/Users/<you>/my_chrome_profile\nCopy\n```\n\n```\n# Windows example (PowerShell/cmd)\n\"C:\\Users\\<you>\\AppData\\Local\\ms-playwright\\chromium-1234\\chrome-win\\chrome.exe\" ^\n    --user-data-dir=\"C:\\Users\\<you>\\my_chrome_profile\"\nCopy\n```\n\n**Replace** the path with the actual subfolder indicated in your `ms-playwright` cache structure.  \n- This **opens** a fresh Chromium with your new or existing data folder.  \n- **Log into** any sites or configure your browser the way you want.  \n- **Close** when doneâ€”your profile data is saved in that folder.\n3. **Use** that folder in **`BrowserConfig.user_data_dir`**:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nbrowser_config = BrowserConfig(\n    headless=True,\n    use_managed_browser=True,\n    user_data_dir=\"/home/<you>/my_chrome_profile\",\n    browser_type=\"chromium\"\n)\nCopy\n```\n\n- Next time you run your code, it reuses that folderâ€”**preserving** your session data, cookies, local storage, etc.\n* * *\n### Creating a Profile Using the Crawl4AI CLI (Easiest)\nIf you prefer a guided, interactive setup, use the built-in CLI to create and manage persistent browser profiles.\n1.â €Launch the profile manager: \n```\ncrwl profiles\nCopy\n```\n\n2.â €Choose \"Create new profile\" and enter a profile name. A Chromium window opens so you can log in to sites and configure settings. When finished, return to the terminal and press `q` to save the profile.\n3.â €Profiles are saved under `~/.crawl4ai/profiles/<profile_name>` (for example: `/home/<you>/.crawl4ai/profiles/test_profile_1`) along with a `storage_state.json` for cookies and session data.\n4.â €Optionally, choose \"List profiles\" in the CLI to view available profiles and their paths.\n5.â €Use the saved path with `BrowserConfig.user_data_dir`: \n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nprofile_path = \"/home/<you>/.crawl4ai/profiles/test_profile_1\"\n\nbrowser_config = BrowserConfig(\n    headless=True,\n    use_managed_browser=True,\n    user_data_dir=profile_path,\n    browser_type=\"chromium\",\n)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com/private\")\nCopy\n```\n\nThe CLI also supports listing and deleting profiles, and even testing a crawl directly from the menu.\n* * *\n## 3. Using Managed Browsers in Crawl4AI\nOnce you have a data directory with your session data, pass it to **`BrowserConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # 1) Reference your persistent data directory\n    browser_config = BrowserConfig(\n        headless=True,             # 'True' for automated runs\n        verbose=True,\n        use_managed_browser=True,  # Enables persistent browser strategy\n        browser_type=\"chromium\",\n        user_data_dir=\"/path/to/my-chrome-profile\"\n    )\n\n    # 2) Standard crawl config\n    crawl_config = CrawlerRunConfig(\n        wait_for=\"css:.logged-in-content\"\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com/private\", config=crawl_config)\n        if result.success:\n            print(\"Successfully accessed private data with your identity!\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### Workflow\n1. **Login** externally (via CLI or your normal Chrome with `--user-data-dir=...`).  \n2. **Close** that browser.  \n3. **Use** the same folder in `user_data_dir=` in Crawl4AI.  \n4. **Crawl** â€“ The site sees your identity as if youâ€™re the same user who just logged in.\n* * *\n## 4. Magic Mode: Simplified Automation\nIf you **donâ€™t** need a persistent profile or identity-based approach, **Magic Mode** offers a quick way to simulate human-like browsing without storing long-term data.\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(\n            magic=True,  # Simplifies a lot of interaction\n            remove_overlay_elements=True,\n            page_timeout=60000\n        )\n    )\nCopy\n```\n\n**Magic Mode** :\n  * Simulates a user-like experience \n  * Randomizes user agent & navigator\n  * Randomizes interactions & timings \n  * Masks automation signals \n  * Attempts pop-up handling \n\n\n**But** itâ€™s no substitute for **true** user-based sessions if you want a fully legitimate identity-based solution.\n* * *\n## 5. Comparing Managed Browsers vs. Magic Mode\nFeature | **Managed Browsers** | **Magic Mode**  \n---|---|---  \n**Session Persistence** | Full localStorage/cookies retained in user_data_dir | No persistent data (fresh each run)  \n**Genuine Identity** | Real user profile with full rights & preferences | Emulated user-like patterns, but no actual identity  \n**Complex Sites** | Best for login-gated sites or heavy config | Simple tasks, minimal login or config needed  \n**Setup** | External creation of user_data_dir, then use in Crawl4AI | Single-line approach (`magic=True`)  \n**Reliability** | Extremely consistent (same data across runs) | Good for smaller tasks, can be less stable  \n* * *\n## 6. Using the BrowserProfiler Class\nCrawl4AI provides a dedicated `BrowserProfiler` class for managing browser profiles, making it easy to create, list, and delete profiles for identity-based browsing.\n### Creating and Managing Profiles with BrowserProfiler\nThe `BrowserProfiler` class offers a comprehensive API for browser profile management:\n```\nimport asyncio\nfrom crawl4ai import BrowserProfiler\n\nasync def manage_profiles():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n\n    # Create a profile interactively - opens a browser window\n    profile_path = await profiler.create_profile(\n        profile_name=\"my-login-profile\"  # Optional: name your profile\n    )\n\n    print(f\"Profile saved at: {profile_path}\")\n\n    # List all available profiles\n    profiles = profiler.list_profiles()\n\n    for profile in profiles:\n        print(f\"Profile: {profile['name']}\")\n        print(f\"  Path: {profile['path']}\")\n        print(f\"  Created: {profile['created']}\")\n        print(f\"  Browser type: {profile['type']}\")\n\n    # Get a specific profile path by name\n    specific_profile = profiler.get_profile_path(\"my-login-profile\")\n\n    # Delete a profile when no longer needed\n    success = profiler.delete_profile(\"old-profile-name\")\n\nasyncio.run(manage_profiles())\nCopy\n```\n\n**How profile creation works:** 1. A browser window opens for you to interact with 2. You log in to websites, set preferences, etc. 3. When you're done, press 'q' in the terminal to close the browser 4. The profile is saved in the Crawl4AI profiles directory 5. You can use the returned path with `BrowserConfig.user_data_dir`\n### Interactive Profile Management\nThe `BrowserProfiler` also offers an interactive management console that guides you through profile creation, listing, and deletion:\n```\nimport asyncio\nfrom crawl4ai import BrowserProfiler, AsyncWebCrawler, BrowserConfig\n\n# Define a function to use a profile for crawling\nasync def crawl_with_profile(profile_path, url):\n    browser_config = BrowserConfig(\n        headless=True,\n        use_managed_browser=True,\n        user_data_dir=profile_path\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url)\n        return result\n\nasync def main():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n\n    # Launch the interactive profile manager\n    # Passing the crawl function as a callback adds a \"crawl with profile\" option\n    await profiler.interactive_manager(crawl_callback=crawl_with_profile)\n\nasyncio.run(main())\nCopy\n```\n\n### Legacy Methods\nFor backward compatibility, the previous methods on `ManagedBrowser` are still available, but they delegate to the new `BrowserProfiler` class:\n```\nfrom crawl4ai.browser_manager import ManagedBrowser\n\n# These methods still work but use BrowserProfiler internally\nprofiles = ManagedBrowser.list_profiles()\nCopy\n```\n\n### Complete Example\nSee the full example in `docs/examples/identity_based_browsing.py` for a complete demonstration of creating and using profiles for authenticated browsing using the new `BrowserProfiler` class.\n* * *\n## 7. Locale, Timezone, and Geolocation Control\nIn addition to using persistent profiles, Crawl4AI supports customizing your browser's locale, timezone, and geolocation settings. These features enhance your identity-based browsing experience by allowing you to control how websites perceive your location and regional settings.\n### Setting Locale and Timezone\nYou can set the browser's locale and timezone through `CrawlerRunConfig`:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(\n            # Set browser locale (language and region formatting)\n            locale=\"fr-FR\",  # French (France)\n\n            # Set browser timezone\n            timezone_id=\"Europe/Paris\",\n\n            # Other normal options...\n            magic=True,\n            page_timeout=60000\n        )\n    )\nCopy\n```\n\n**How it works:** - `locale` affects language preferences, date formats, number formats, etc. - `timezone_id` affects JavaScript's Date object and time-related functionality - These settings are applied when creating the browser context and maintained throughout the session\n### Configuring Geolocation\nControl the GPS coordinates reported by the browser's geolocation API:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, GeolocationConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://maps.google.com\",  # Or any location-aware site\n        config=CrawlerRunConfig(\n            # Configure precise GPS coordinates\n            geolocation=GeolocationConfig(\n                latitude=48.8566,   # Paris coordinates\n                longitude=2.3522,\n                accuracy=100        # Accuracy in meters (optional)\n            ),\n\n            # This site will see you as being in Paris\n            page_timeout=60000\n        )\n    )\nCopy\n```\n\n**Important notes:** - When `geolocation` is specified, the browser is automatically granted permission to access location - Websites using the Geolocation API will receive the exact coordinates you specify - This affects map services, store locators, delivery services, etc. - Combined with the appropriate `locale` and `timezone_id`, you can create a fully consistent location profile\n### Combining with Managed Browsers\nThese settings work perfectly with managed browsers for a complete identity solution:\n```\nfrom crawl4ai import (\n    AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, \n    GeolocationConfig\n)\n\nbrowser_config = BrowserConfig(\n    use_managed_browser=True,\n    user_data_dir=\"/path/to/my-profile\",\n    browser_type=\"chromium\"\n)\n\ncrawl_config = CrawlerRunConfig(\n    # Location settings\n    locale=\"es-MX\",                  # Spanish (Mexico)\n    timezone_id=\"America/Mexico_City\",\n    geolocation=GeolocationConfig(\n        latitude=19.4326,            # Mexico City\n        longitude=-99.1332\n    )\n)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\", config=crawl_config)\nCopy\n```\n\nCombining persistent profiles with precise geolocation and region settings gives you complete control over your digital identity.\n## 8. Summary\n  * **Create** your user-data directory either:\n  * By launching Chrome/Chromium externally with `--user-data-dir=/some/path`\n  * Or by using the built-in `BrowserProfiler.create_profile()` method\n  * Or through the interactive interface with `profiler.interactive_manager()`\n  * **Log in** or configure sites as needed, then close the browser\n  * **Reference** that folder in `BrowserConfig(user_data_dir=\"...\")` + `use_managed_browser=True`\n  * **Customize** identity aspects with `locale`, `timezone_id`, and `geolocation`\n  * **List and reuse** profiles with `BrowserProfiler.list_profiles()`\n  * **Manage** your profiles with the dedicated `BrowserProfiler` class\n  * Enjoy **persistent** sessions that reflect your real identity\n  * If you only need quick, ephemeral automation, **Magic Mode** might suffice\n\n\n**Recommended** : Always prefer a **Managed Browser** for robust, identity-based crawling and simpler interactions with complex sites. Use **Magic Mode** for quick tasks or prototypes where persistent data is unnecessary.\nWith these approaches, you preserve your **authentic** browsing environment, ensuring the site sees you exactly as a normal userâ€”no repeated logins or wasted time.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n\n\nESC to close\n#### On this page\n  * [1. Managed Browsers: Your Digital Identity Solution](https://docs.crawl4ai.com/advanced/identity-based-crawling/#1-managed-browsers-your-digital-identity-solution)\n  * [Key Benefits](https://docs.crawl4ai.com/advanced/identity-based-crawling/#key-benefits)\n  * [Creating a User Data Directory (Command-Line Approach via Playwright)](https://docs.crawl4ai.com/advanced/identity-based-crawling/#creating-a-user-data-directory-command-line-approach-via-playwright)\n  * [Creating a Profile Using the Crawl4AI CLI (Easiest)](https://docs.crawl4ai.com/advanced/identity-based-crawling/#creating-a-profile-using-the-crawl4ai-cli-easiest)\n  * [3. Using Managed Browsers in Crawl4AI](https://docs.crawl4ai.com/advanced/identity-based-crawling/#3-using-managed-browsers-in-crawl4ai)\n  * [Workflow](https://docs.crawl4ai.com/advanced/identity-based-crawling/#workflow)\n  * [4. Magic Mode: Simplified Automation](https://docs.crawl4ai.com/advanced/identity-based-crawling/#4-magic-mode-simplified-automation)\n  * [5. Comparing Managed Browsers vs. Magic Mode](https://docs.crawl4ai.com/advanced/identity-based-crawling/#5-comparing-managed-browsers-vs-magic-mode)\n  * [6. Using the BrowserProfiler Class](https://docs.crawl4ai.com/advanced/identity-based-crawling/#6-using-the-browserprofiler-class)\n  * [Creating and Managing Profiles with BrowserProfiler](https://docs.crawl4ai.com/advanced/identity-based-crawling/#creating-and-managing-profiles-with-browserprofiler)\n  * [Interactive Profile Management](https://docs.crawl4ai.com/advanced/identity-based-crawling/#interactive-profile-management)\n  * [Legacy Methods](https://docs.crawl4ai.com/advanced/identity-based-crawling/#legacy-methods)\n  * [Complete Example](https://docs.crawl4ai.com/advanced/identity-based-crawling/#complete-example)\n  * [7. Locale, Timezone, and Geolocation Control](https://docs.crawl4ai.com/advanced/identity-based-crawling/#7-locale-timezone-and-geolocation-control)\n  * [Setting Locale and Timezone](https://docs.crawl4ai.com/advanced/identity-based-crawling/#setting-locale-and-timezone)\n  * [Configuring Geolocation](https://docs.crawl4ai.com/advanced/identity-based-crawling/#configuring-geolocation)\n  * [Combining with Managed Browsers](https://docs.crawl4ai.com/advanced/identity-based-crawling/#combining-with-managed-browsers)\n  * [8. Summary](https://docs.crawl4ai.com/advanced/identity-based-crawling/#8-summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/multi-url-crawling",
    "depth": 1,
    "title": "Multi-URL Crawling - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "b1b80a24fd2460ba4a6e6f100cc311e2",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * Multi-URL Crawling\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Advanced Multi-URL Crawling with Dispatchers](https://docs.crawl4ai.com/advanced/multi-url-crawling/#advanced-multi-url-crawling-with-dispatchers)\n  * [1. Introduction](https://docs.crawl4ai.com/advanced/multi-url-crawling/#1-introduction)\n  * [2. Core Components](https://docs.crawl4ai.com/advanced/multi-url-crawling/#2-core-components)\n  * [3. Available Dispatchers](https://docs.crawl4ai.com/advanced/multi-url-crawling/#3-available-dispatchers)\n  * [4. Usage Examples](https://docs.crawl4ai.com/advanced/multi-url-crawling/#4-usage-examples)\n  * [5. Dispatch Results](https://docs.crawl4ai.com/advanced/multi-url-crawling/#5-dispatch-results)\n  * [6. URL-Specific Configurations](https://docs.crawl4ai.com/advanced/multi-url-crawling/#6-url-specific-configurations)\n  * [7. Summary](https://docs.crawl4ai.com/advanced/multi-url-crawling/#7-summary)\n\n\n# Advanced Multi-URL Crawling with Dispatchers\n> **Heads Up** : Crawl4AI supports advanced dispatchers for **parallel** or **throttled** crawling, providing dynamic rate limiting and memory usage checks. The built-in `arun_many()` function uses these dispatchers to handle concurrency efficiently.\n## 1. Introduction\nWhen crawling many URLs:\n  * **Basic** : Use `arun()` in a loop (simple but less efficient)\n  * **Better** : Use `arun_many()`, which efficiently handles multiple URLs with proper concurrency control\n  * **Best** : Customize dispatcher behavior for your specific needs (memory management, rate limits, etc.)\n\n\n**Why Dispatchers?**\n  * **Adaptive** : Memory-based dispatchers can pause or slow down based on system resources\n  * **Rate-limiting** : Built-in rate limiting with exponential backoff for 429/503 responses\n  * **Real-time Monitoring** : Live dashboard of ongoing tasks, memory usage, and performance\n  * **Flexibility** : Choose between memory-adaptive or semaphore-based concurrency\n\n\n* * *\n## 2. Core Components\n### 2.1 Rate Limiter\n```\nclass RateLimiter:\n    def __init__(\n        # Random delay range between requests\n        base_delay: Tuple[float, float] = (1.0, 3.0),  \n\n        # Maximum backoff delay\n        max_delay: float = 60.0,                        \n\n        # Retries before giving up\n        max_retries: int = 3,                          \n\n        # Status codes triggering backoff\n        rate_limit_codes: List[int] = [429, 503]        \n    )\nCopy\n```\n\nHereâ€™s the revised and simplified explanation of the **RateLimiter** , focusing on constructor parameters and adhering to your markdown style and mkDocs guidelines.\n#### RateLimiter Constructor Parameters\nThe **RateLimiter** is a utility that helps manage the pace of requests to avoid overloading servers or getting blocked due to rate limits. It operates internally to delay requests and handle retries but can be configured using its constructor parameters.\n**Parameters of the`RateLimiter` constructor:**\n1. **`base_delay`**(`Tuple[float, float]` , default: `(1.0, 3.0)`)  \nThe range for a random delay (in seconds) between consecutive requests to the same domain.\n  * A random delay is chosen between `base_delay[0]` and `base_delay[1]` for each request. \n  * This prevents sending requests at a predictable frequency, reducing the chances of triggering rate limits.\n\n\n**Example:**  \nIf `base_delay = (2.0, 5.0)`, delays could be randomly chosen as `2.3s`, `4.1s`, etc.\n* * *\n2. **`max_delay`**(`float` , default: `60.0`)  \nThe maximum allowable delay when rate-limiting errors occur.\n  * When servers return rate-limit responses (e.g., 429 or 503), the delay increases exponentially with jitter. \n  * The `max_delay` ensures the delay doesnâ€™t grow unreasonably high, capping it at this value.\n\n\n**Example:**  \nFor a `max_delay = 30.0`, even if backoff calculations suggest a delay of `45s`, it will cap at `30s`.\n* * *\n3. **`max_retries`**(`int` , default: `3`)  \nThe maximum number of retries for a request if rate-limiting errors occur.\n  * After encountering a rate-limit response, the `RateLimiter` retries the request up to this number of times. \n  * If all retries fail, the request is marked as failed, and the process continues.\n\n\n**Example:**  \nIf `max_retries = 3`, the system retries a failed request three times before giving up.\n* * *\n4. **`rate_limit_codes`**(`List[int]` , default: `[429, 503]`)  \nA list of HTTP status codes that trigger the rate-limiting logic.\n  * These status codes indicate the server is overwhelmed or actively limiting requests. \n  * You can customize this list to include other codes based on specific server behavior.\n\n\n**Example:**  \nIf `rate_limit_codes = [429, 503, 504]`, the crawler will back off on these three error codes.\n* * *\n**How to Use the`RateLimiter` :**\nHereâ€™s an example of initializing and using a `RateLimiter` in your project:\n```\nfrom crawl4ai import RateLimiter\n\n# Create a RateLimiter with custom settings\nrate_limiter = RateLimiter(\n    base_delay=(2.0, 4.0),  # Random delay between 2-4 seconds\n    max_delay=30.0,         # Cap delay at 30 seconds\n    max_retries=5,          # Retry up to 5 times on rate-limiting errors\n    rate_limit_codes=[429, 503]  # Handle these HTTP status codes\n)\n\n# RateLimiter will handle delays and retries internally\n# No additional setup is required for its operation\nCopy\n```\n\nThe `RateLimiter` integrates seamlessly with dispatchers like `MemoryAdaptiveDispatcher` and `SemaphoreDispatcher`, ensuring requests are paced correctly without user intervention. Its internal mechanisms manage delays and retries to avoid overwhelming servers while maximizing efficiency.\n### 2.2 Crawler Monitor\nThe CrawlerMonitor provides real-time visibility into crawling operations:\n```\nfrom crawl4ai import CrawlerMonitor, DisplayMode\nmonitor = CrawlerMonitor(\n    # Maximum rows in live display\n    max_visible_rows=15,          \n\n    # DETAILED or AGGREGATED view\n    display_mode=DisplayMode.DETAILED  \n)\nCopy\n```\n\n**Display Modes** :\n  1. **DETAILED** : Shows individual task status, memory usage, and timing\n  2. **AGGREGATED** : Displays summary statistics and overall progress\n\n\n* * *\n## 3. Available Dispatchers\n### 3.1 MemoryAdaptiveDispatcher (Default)\nAutomatically manages concurrency based on system memory usage:\n```\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\n\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=90.0,  # Pause if memory exceeds this\n    check_interval=1.0,             # How often to check memory\n    max_session_permit=10,          # Maximum concurrent tasks\n    rate_limiter=RateLimiter(       # Optional rate limiting\n        base_delay=(1.0, 2.0),\n        max_delay=30.0,\n        max_retries=2\n    ),\n    monitor=CrawlerMonitor(         # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)\nCopy\n```\n\n**Constructor Parameters:**\n1. **`memory_threshold_percent`**(`float` , default: `90.0`)  \nSpecifies the memory usage threshold (as a percentage). If system memory usage exceeds this value, the dispatcher pauses crawling to prevent system overload.\n2. **`check_interval`**(`float` , default: `1.0`)  \nThe interval (in seconds) at which the dispatcher checks system memory usage.\n3. **`max_session_permit`**(`int` , default: `10`)  \nThe maximum number of concurrent crawling tasks allowed. This ensures resource limits are respected while maintaining concurrency.\n4. **`memory_wait_timeout`**(`float` , default: `600.0`) Optional timeout (in seconds). If memory usage exceeds `memory_threshold_percent` for longer than this duration, a `MemoryError` is raised.\n5. **`rate_limiter`**(`RateLimiter` , default: `None`)  \nOptional rate-limiting logic to avoid server-side blocking (e.g., for handling 429 or 503 errors). See **RateLimiter** for details.\n6. **`monitor`**(`CrawlerMonitor` , default: `None`)  \nOptional monitoring for real-time task tracking and performance insights. See **CrawlerMonitor** for details.\n* * *\n### 3.2 SemaphoreDispatcher\nProvides simple concurrency control with a fixed limit:\n```\nfrom crawl4ai.async_dispatcher import SemaphoreDispatcher\n\ndispatcher = SemaphoreDispatcher(\n    max_session_permit=20,         # Maximum concurrent tasks\n    rate_limiter=RateLimiter(      # Optional rate limiting\n        base_delay=(0.5, 1.0),\n        max_delay=10.0\n    ),\n    monitor=CrawlerMonitor(        # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)\nCopy\n```\n\n**Constructor Parameters:**\n1. **`max_session_permit`**(`int` , default: `20`)  \nThe maximum number of concurrent crawling tasks allowed, irrespective of semaphore slots.\n2. **`rate_limiter`**(`RateLimiter` , default: `None`)  \nOptional rate-limiting logic to avoid overwhelming servers. See **RateLimiter** for details.\n3. **`monitor`**(`CrawlerMonitor` , default: `None`)  \nOptional monitoring for tracking task progress and resource usage. See **CrawlerMonitor** for details.\n* * *\n## 4. Usage Examples\n### 4.1 Batch Processing (Default)\n```\nasync def crawl_batch():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=False  # Default: get all results at once\n    )\n\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Get all results at once\n        results = await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        )\n\n        # Process all results after completion\n        for result in results:\n            if result.success:\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\nCopy\n```\n\n**Review:**  \n- **Purpose:** Executes a batch crawl with all URLs processed together after crawling is complete.  \n- **Dispatcher:** Uses `MemoryAdaptiveDispatcher` to manage concurrency and system memory.  \n- **Stream:** Disabled (`stream=False`), so all results are collected at once for post-processing.  \n- **Best Use Case:** When you need to analyze results in bulk rather than individually during the crawl.\n* * *\n### 4.2 Streaming Mode\n```\nasync def crawl_streaming():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Process results as they become available\n        async for result in await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        ):\n            if result.success:\n                # Process each result immediately\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\nCopy\n```\n\n**Review:**  \n- **Purpose:** Enables streaming to process results as soon as theyâ€™re available.  \n- **Dispatcher:** Uses `MemoryAdaptiveDispatcher` for concurrency and memory management.  \n- **Stream:** Enabled (`stream=True`), allowing real-time processing during crawling.  \n- **Best Use Case:** When you need to act on results immediately, such as for real-time analytics or progressive data storage.\n* * *\n### 4.3 Semaphore-based Crawling\n```\nasync def crawl_with_semaphore(urls):\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    dispatcher = SemaphoreDispatcher(\n        semaphore_count=5,\n        rate_limiter=RateLimiter(\n            base_delay=(0.5, 1.0),\n            max_delay=10.0\n        ),\n        monitor=CrawlerMonitor(\n            max_visible_rows=15,\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        results = await crawler.arun_many(\n            urls, \n            config=run_config,\n            dispatcher=dispatcher\n        )\n        return results\nCopy\n```\n\n**Review:**  \n- **Purpose:** Uses `SemaphoreDispatcher` to limit concurrency with a fixed number of slots.  \n- **Dispatcher:** Configured with a semaphore to control parallel crawling tasks.  \n- **Rate Limiter:** Prevents servers from being overwhelmed by pacing requests.  \n- **Best Use Case:** When you want precise control over the number of concurrent requests, independent of system memory.\n* * *\n### 4.4 Robots.txt Consideration\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    urls = [\n        \"https://example1.com\",\n        \"https://example2.com\",\n        \"https://example3.com\"\n    ]\n\n    config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,  # Will respect robots.txt for each URL\n        semaphore_count=3      # Max concurrent requests\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        async for result in crawler.arun_many(urls, config=config):\n            if result.success:\n                print(f\"Successfully crawled {result.url}\")\n            elif result.status_code == 403 and \"robots.txt\" in result.error_message:\n                print(f\"Skipped {result.url} - blocked by robots.txt\")\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Review:**  \n- **Purpose:** Ensures compliance with `robots.txt` rules for ethical and legal web crawling.  \n- **Configuration:** Set `check_robots_txt=True` to validate each URL against `robots.txt` before crawling.  \n- **Dispatcher:** Handles requests with concurrency limits (`semaphore_count=3`).  \n- **Best Use Case:** When crawling websites that strictly enforce robots.txt policies or for responsible crawling practices.\n* * *\n## 5. Dispatch Results\nEach crawl result includes dispatch information:\n```\n@dataclass\nclass DispatchResult:\n    task_id: str\n    memory_usage: float\n    peak_memory: float\n    start_time: datetime\n    end_time: datetime\n    error_message: str = \"\"\nCopy\n```\n\nAccess via `result.dispatch_result`:\n```\nfor result in results:\n    if result.success:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}\")\n        print(f\"Memory: {dr.memory_usage:.1f}MB\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")\nCopy\n```\n\n## 6. URL-Specific Configurations\nWhen crawling diverse content types, you often need different configurations for different URLs. For example: - PDFs need specialized extraction - Blog pages benefit from content filtering - Dynamic sites need JavaScript execution - API endpoints need JSON parsing\n### 6.1 Basic URL Pattern Matching\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MatchMode\nfrom crawl4ai.processors.pdf import PDFContentScrapingStrategy\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def crawl_mixed_content():\n    # Configure different strategies for different content\n    configs = [\n        # PDF files - specialized extraction\n        CrawlerRunConfig(\n            url_matcher=\"*.pdf\",\n            scraping_strategy=PDFContentScrapingStrategy()\n        ),\n\n        # Blog/article pages - content filtering\n        CrawlerRunConfig(\n            url_matcher=[\"*/blog/*\", \"*/article/*\"],\n            markdown_generator=DefaultMarkdownGenerator(\n                content_filter=PruningContentFilter(threshold=0.48)\n            )\n        ),\n\n        # Dynamic pages - JavaScript execution\n        CrawlerRunConfig(\n            url_matcher=lambda url: 'github.com' in url,\n            js_code=\"window.scrollTo(0, 500);\"\n        ),\n\n        # API endpoints - JSON extraction\n        CrawlerRunConfig(\n            url_matcher=lambda url: 'api' in url or url.endswith('.json'),\n            # Custome settings for JSON extraction\n        ),\n\n        # Default config for everything else\n        CrawlerRunConfig()  # No url_matcher means it matches ALL URLs (fallback)\n    ]\n\n    # Mixed URLs\n    urls = [\n        \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\",\n        \"https://blog.python.org/\",\n        \"https://github.com/microsoft/playwright\",\n        \"https://httpbin.org/json\",\n        \"https://example.com/\"\n    ]\n\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun_many(\n            urls=urls,\n            config=configs  # Pass list of configs\n        )\n\n        for result in results:\n            print(f\"{result.url}: {len(result.markdown)} chars\")\nCopy\n```\n\n### 6.2 Advanced Pattern Matching\n**Important** : A `CrawlerRunConfig` without `url_matcher` (or with `url_matcher=None`) matches ALL URLs. This makes it perfect as a default/fallback configuration.\nThe `url_matcher` parameter supports three types of patterns:\n#### Glob Patterns (Strings)\n```\n# Simple patterns\n\"*.pdf\"                    # Any PDF file\n\"*/api/*\"                  # Any URL with /api/ in path\n\"https://*.example.com/*\"  # Subdomain matching\n\"*://example.com/blog/*\"   # Any protocol\nCopy\n```\n\n#### Custom Functions\n```\n# Complex logic with lambdas\nlambda url: url.startswith('https://') and 'secure' in url\nlambda url: len(url) > 50 and url.count('/') > 5\nlambda url: any(domain in url for domain in ['api.', 'data.', 'feed.'])\nCopy\n```\n\n#### Mixed Lists with AND/OR Logic\n```\n# Combine multiple conditions\nCrawlerRunConfig(\n    url_matcher=[\n        \"https://*\",                        # Must be HTTPS\n        lambda url: 'internal' in url,      # Must contain 'internal'\n        lambda url: not url.endswith('.pdf') # Must not be PDF\n    ],\n    match_mode=MatchMode.AND  # ALL conditions must match\n)\nCopy\n```\n\n### 6.3 Practical Example: News Site Crawler\n```\nasync def crawl_news_site():\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        rate_limiter=RateLimiter(base_delay=(1.0, 2.0))\n    )\n\n    configs = [\n        # Homepage - light extraction\n        CrawlerRunConfig(\n            url_matcher=lambda url: url.rstrip('/') == 'https://news.ycombinator.com',\n            css_selector=\"nav, .headline\",\n            extraction_strategy=None\n        ),\n\n        # Article pages - full extraction\n        CrawlerRunConfig(\n            url_matcher=\"*/article/*\",\n            extraction_strategy=CosineStrategy(\n                semantic_filter=\"article content\",\n                word_count_threshold=100\n            ),\n            screenshot=True,\n            excluded_tags=[\"nav\", \"aside\", \"footer\"]\n        ),\n\n        # Author pages - metadata focus\n        CrawlerRunConfig(\n            url_matcher=\"*/author/*\",\n            extraction_strategy=JsonCssExtractionStrategy({\n                \"name\": \"h1.author-name\",\n                \"bio\": \".author-bio\",\n                \"articles\": \"article.post-card h2\"\n            })\n        ),\n\n        # Everything else\n        CrawlerRunConfig()\n    ]\n\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun_many(\n            urls=news_urls,\n            config=configs,\n            dispatcher=dispatcher\n        )\nCopy\n```\n\n### 6.4 Best Practices\n  1. **Order Matters** : Configs are evaluated in order - put specific patterns before general ones\n  2. **Default Config Behavior** : \n  3. A config without `url_matcher` matches ALL URLs\n  4. Always include a default config as the last item if you want to handle all URLs\n  5. Without a default config, unmatched URLs will fail with \"No matching configuration found\"\n  6. **Test Your Patterns** : Use the config's `is_match()` method to test patterns: \n```\nconfig = CrawlerRunConfig(url_matcher=\"*.pdf\")\nprint(config.is_match(\"https://example.com/doc.pdf\"))  # True\n\ndefault_config = CrawlerRunConfig()  # No url_matcher\nprint(default_config.is_match(\"https://any-url.com\"))  # True - matches everything!\nCopy\n```\n\n  7. **Optimize for Performance** : \n  8. Disable JS for static content\n  9. Skip screenshots for data APIs\n  10. Use appropriate extraction strategies\n\n\n## 7. Summary\n1. **Two Dispatcher Types** :\n  * MemoryAdaptiveDispatcher (default): Dynamic concurrency based on memory\n  * SemaphoreDispatcher: Fixed concurrency limit\n\n\n2. **Optional Components** :\n  * RateLimiter: Smart request pacing and backoff\n  * CrawlerMonitor: Real-time progress visualization\n\n\n3. **Key Benefits** :\n  * Automatic memory management\n  * Built-in rate limiting\n  * Live progress monitoring\n  * Flexible concurrency control\n\n\nChoose the dispatcher that best fits your needs:\n  * **MemoryAdaptiveDispatcher** : For large crawls or limited resources\n  * **SemaphoreDispatcher** : For simple, fixed-concurrency scenarios\n\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n\n\nESC to close\n#### On this page\n  * [1. Introduction](https://docs.crawl4ai.com/advanced/multi-url-crawling/#1-introduction)\n  * [2. Core Components](https://docs.crawl4ai.com/advanced/multi-url-crawling/#2-core-components)\n  * [2.1 Rate Limiter](https://docs.crawl4ai.com/advanced/multi-url-crawling/#21-rate-limiter)\n  * [RateLimiter Constructor Parameters](https://docs.crawl4ai.com/advanced/multi-url-crawling/#ratelimiter-constructor-parameters)\n  * [2.2 Crawler Monitor](https://docs.crawl4ai.com/advanced/multi-url-crawling/#22-crawler-monitor)\n  * [3. Available Dispatchers](https://docs.crawl4ai.com/advanced/multi-url-crawling/#3-available-dispatchers)\n  * [3.1 MemoryAdaptiveDispatcher (Default)](https://docs.crawl4ai.com/advanced/multi-url-crawling/#31-memoryadaptivedispatcher-default)\n  * [3.2 SemaphoreDispatcher](https://docs.crawl4ai.com/advanced/multi-url-crawling/#32-semaphoredispatcher)\n  * [4. Usage Examples](https://docs.crawl4ai.com/advanced/multi-url-crawling/#4-usage-examples)\n  * [4.1 Batch Processing (Default)](https://docs.crawl4ai.com/advanced/multi-url-crawling/#41-batch-processing-default)\n  * [4.2 Streaming Mode](https://docs.crawl4ai.com/advanced/multi-url-crawling/#42-streaming-mode)\n  * [4.3 Semaphore-based Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/#43-semaphore-based-crawling)\n  * [4.4 Robots.txt Consideration](https://docs.crawl4ai.com/advanced/multi-url-crawling/#44-robotstxt-consideration)\n  * [5. Dispatch Results](https://docs.crawl4ai.com/advanced/multi-url-crawling/#5-dispatch-results)\n  * [6. URL-Specific Configurations](https://docs.crawl4ai.com/advanced/multi-url-crawling/#6-url-specific-configurations)\n  * [6.1 Basic URL Pattern Matching](https://docs.crawl4ai.com/advanced/multi-url-crawling/#61-basic-url-pattern-matching)\n  * [6.2 Advanced Pattern Matching](https://docs.crawl4ai.com/advanced/multi-url-crawling/#62-advanced-pattern-matching)\n  * [Glob Patterns (Strings)](https://docs.crawl4ai.com/advanced/multi-url-crawling/#glob-patterns-strings)\n  * [Custom Functions](https://docs.crawl4ai.com/advanced/multi-url-crawling/#custom-functions)\n  * [Mixed Lists with AND/OR Logic](https://docs.crawl4ai.com/advanced/multi-url-crawling/#mixed-lists-with-andor-logic)\n  * [6.3 Practical Example: News Site Crawler](https://docs.crawl4ai.com/advanced/multi-url-crawling/#63-practical-example-news-site-crawler)\n  * [6.4 Best Practices](https://docs.crawl4ai.com/advanced/multi-url-crawling/#64-best-practices)\n  * [7. Summary](https://docs.crawl4ai.com/advanced/multi-url-crawling/#7-summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/network-console-capture",
    "depth": 1,
    "title": "Network & Console Capture - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "964f73d68326ad4a3aa1bc10c209b365",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/network-console-capture/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * Network & Console Capture\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Network Requests & Console Message Capturing](https://docs.crawl4ai.com/advanced/network-console-capture/#network-requests-console-message-capturing)\n  * [Configuration](https://docs.crawl4ai.com/advanced/network-console-capture/#configuration)\n  * [Example Usage](https://docs.crawl4ai.com/advanced/network-console-capture/#example-usage)\n  * [Captured Data Structure](https://docs.crawl4ai.com/advanced/network-console-capture/#captured-data-structure)\n  * [Key Benefits](https://docs.crawl4ai.com/advanced/network-console-capture/#key-benefits)\n  * [Use Cases](https://docs.crawl4ai.com/advanced/network-console-capture/#use-cases)\n\n\n# Network Requests & Console Message Capturing\nCrawl4AI can capture all network requests and browser console messages during a crawl, which is invaluable for debugging, security analysis, or understanding page behavior.\n## Configuration\nTo enable network and console capturing, use these configuration options:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Enable both network request capture and console message capture\nconfig = CrawlerRunConfig(\n    capture_network_requests=True,  # Capture all network requests and responses\n    capture_console_messages=True   # Capture all browser console output\n)\nCopy\n```\n\n## Example Usage\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Enable both network request capture and console message capture\n    config = CrawlerRunConfig(\n        capture_network_requests=True,\n        capture_console_messages=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=config\n        )\n\n        if result.success:\n            # Analyze network requests\n            if result.network_requests:\n                print(f\"Captured {len(result.network_requests)} network events\")\n\n                # Count request types\n                request_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"request\"])\n                response_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"response\"])\n                failed_count = len([r for r in result.network_requests if r.get(\"event_type\") == \"request_failed\"])\n\n                print(f\"Requests: {request_count}, Responses: {response_count}, Failed: {failed_count}\")\n\n                # Find API calls\n                api_calls = [r for r in result.network_requests \n                            if r.get(\"event_type\") == \"request\" and \"api\" in r.get(\"url\", \"\")]\n                if api_calls:\n                    print(f\"Detected {len(api_calls)} API calls:\")\n                    for call in api_calls[:3]:  # Show first 3\n                        print(f\"  - {call.get('method')} {call.get('url')}\")\n\n            # Analyze console messages\n            if result.console_messages:\n                print(f\"Captured {len(result.console_messages)} console messages\")\n\n                # Group by type\n                message_types = {}\n                for msg in result.console_messages:\n                    msg_type = msg.get(\"type\", \"unknown\")\n                    message_types[msg_type] = message_types.get(msg_type, 0) + 1\n\n                print(\"Message types:\", message_types)\n\n                # Show errors (often the most important)\n                errors = [msg for msg in result.console_messages if msg.get(\"type\") == \"error\"]\n                if errors:\n                    print(f\"Found {len(errors)} console errors:\")\n                    for err in errors[:2]:  # Show first 2\n                        print(f\"  - {err.get('text', '')[:100]}\")\n\n            # Export all captured data to a file for detailed analysis\n            with open(\"network_capture.json\", \"w\") as f:\n                json.dump({\n                    \"url\": result.url,\n                    \"network_requests\": result.network_requests or [],\n                    \"console_messages\": result.console_messages or []\n                }, f, indent=2)\n\n            print(\"Exported detailed capture data to network_capture.json\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Captured Data Structure\n### Network Requests\nThe `result.network_requests` contains a list of dictionaries, each representing a network event with these common fields:\nField | Description  \n---|---  \n`event_type` | Type of event: `\"request\"`, `\"response\"`, or `\"request_failed\"`  \n`url` | The URL of the request  \n`timestamp` | Unix timestamp when the event was captured  \n#### Request Event Fields\n```\n{\n  \"event_type\": \"request\",\n  \"url\": \"https://example.com/api/data.json\",\n  \"method\": \"GET\",\n  \"headers\": {\"User-Agent\": \"...\", \"Accept\": \"...\"},\n  \"post_data\": \"key=value&otherkey=value\",\n  \"resource_type\": \"fetch\",\n  \"is_navigation_request\": false,\n  \"timestamp\": 1633456789.123\n}\nCopy\n```\n\n#### Response Event Fields\n```\n{\n  \"event_type\": \"response\",\n  \"url\": \"https://example.com/api/data.json\",\n  \"status\": 200,\n  \"status_text\": \"OK\",\n  \"headers\": {\"Content-Type\": \"application/json\", \"Cache-Control\": \"...\"},\n  \"from_service_worker\": false,\n  \"request_timing\": {\"requestTime\": 1234.56, \"receiveHeadersEnd\": 1234.78},\n  \"timestamp\": 1633456789.456\n}\nCopy\n```\n\n#### Failed Request Event Fields\n```\n{\n  \"event_type\": \"request_failed\",\n  \"url\": \"https://example.com/missing.png\",\n  \"method\": \"GET\",\n  \"resource_type\": \"image\",\n  \"failure_text\": \"net::ERR_ABORTED 404\",\n  \"timestamp\": 1633456789.789\n}\nCopy\n```\n\n### Console Messages\nThe `result.console_messages` contains a list of dictionaries, each representing a console message with these common fields:\nField | Description  \n---|---  \n`type` | Message type: `\"log\"`, `\"error\"`, `\"warning\"`, `\"info\"`, etc.  \n`text` | The message text  \n`timestamp` | Unix timestamp when the message was captured  \n#### Console Message Example\n```\n{\n  \"type\": \"error\",\n  \"text\": \"Uncaught TypeError: Cannot read property 'length' of undefined\",\n  \"location\": \"https://example.com/script.js:123:45\",\n  \"timestamp\": 1633456790.123\n}\nCopy\n```\n\n## Key Benefits\n  * **Full Request Visibility** : Capture all network activity including:\n  * Requests (URLs, methods, headers, post data)\n  * Responses (status codes, headers, timing)\n  * Failed requests (with error messages)\n  * **Console Message Access** : View all JavaScript console output:\n  * Log messages\n  * Warnings\n  * Errors with stack traces\n  * Developer debugging information\n  * **Debugging Power** : Identify issues such as:\n  * Failed API calls or resource loading\n  * JavaScript errors affecting page functionality\n  * CORS or other security issues\n  * Hidden API endpoints and data flows\n  * **Security Analysis** : Detect:\n  * Unexpected third-party requests\n  * Data leakage in request payloads\n  * Suspicious script behavior\n  * **Performance Insights** : Analyze:\n  * Request timing data\n  * Resource loading patterns\n  * Potential bottlenecks\n\n\n## Use Cases\n  1. **API Discovery** : Identify hidden endpoints and data flows in single-page applications\n  2. **Debugging** : Track down JavaScript errors affecting page functionality\n  3. **Security Auditing** : Detect unwanted third-party requests or data leakage\n  4. **Performance Analysis** : Identify slow-loading resources\n  5. **Ad/Tracker Analysis** : Detect and catalog advertising or tracking calls\n\n\nThis capability is especially valuable for complex sites with heavy JavaScript, single-page applications, or when you need to understand the exact communication happening between a browser and servers.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/network-console-capture/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/network-console-capture/)\n\n\nESC to close\n#### On this page\n  * [Configuration](https://docs.crawl4ai.com/advanced/network-console-capture/#configuration)\n  * [Example Usage](https://docs.crawl4ai.com/advanced/network-console-capture/#example-usage)\n  * [Captured Data Structure](https://docs.crawl4ai.com/advanced/network-console-capture/#captured-data-structure)\n  * [Network Requests](https://docs.crawl4ai.com/advanced/network-console-capture/#network-requests)\n  * [Request Event Fields](https://docs.crawl4ai.com/advanced/network-console-capture/#request-event-fields)\n  * [Response Event Fields](https://docs.crawl4ai.com/advanced/network-console-capture/#response-event-fields)\n  * [Failed Request Event Fields](https://docs.crawl4ai.com/advanced/network-console-capture/#failed-request-event-fields)\n  * [Console Messages](https://docs.crawl4ai.com/advanced/network-console-capture/#console-messages)\n  * [Console Message Example](https://docs.crawl4ai.com/advanced/network-console-capture/#console-message-example)\n  * [Key Benefits](https://docs.crawl4ai.com/advanced/network-console-capture/#key-benefits)\n  * [Use Cases](https://docs.crawl4ai.com/advanced/network-console-capture/#use-cases)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/pdf-parsing",
    "depth": 1,
    "title": "PDF Parsing - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "065a676d027d46aad06ab8210d551781",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * PDF Parsing\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [PDF Processing Strategies](https://docs.crawl4ai.com/advanced/pdf-parsing/#pdf-processing-strategies)\n  * [PDFCrawlerStrategy](https://docs.crawl4ai.com/advanced/pdf-parsing/#pdfcrawlerstrategy)\n  * [PDFContentScrapingStrategy](https://docs.crawl4ai.com/advanced/pdf-parsing/#pdfcontentscrapingstrategy)\n\n\n# PDF Processing Strategies\nCrawl4AI provides specialized strategies for handling and extracting content from PDF files. These strategies allow you to seamlessly integrate PDF processing into your crawling workflows, whether the PDFs are hosted online or stored locally.\n## `PDFCrawlerStrategy`\n### Overview\n`PDFCrawlerStrategy` is an implementation of `AsyncCrawlerStrategy` designed specifically for PDF documents. Instead of interpreting the input URL as an HTML webpage, this strategy treats it as a pointer to a PDF file. It doesn't perform deep crawling or HTML parsing itself but rather prepares the PDF source for a dedicated PDF scraping strategy. Its primary role is to identify the PDF source (web URL or local file) and pass it along the processing pipeline in a way that `AsyncWebCrawler` can handle.\n### When to Use\nUse `PDFCrawlerStrategy` when you need to: - Process PDF files using the `AsyncWebCrawler`. - Handle PDFs from both web URLs (e.g., `https://example.com/document.pdf`) and local file paths (e.g., `file:///path/to/your/document.pdf`). - Integrate PDF content extraction into a unified `CrawlResult` object, allowing consistent handling of PDF data alongside web page data.\n### Key Methods and Their Behavior\n  * **`__init__(self, logger: AsyncLogger = None)`**:\n    * Initializes the strategy.\n    * `logger`: An optional `AsyncLogger` instance (from `crawl4ai.async_logger`) for logging purposes.\n  * **`async crawl(self, url: str, **kwargs) -> AsyncCrawlResponse`**:\n    * This method is called by the `AsyncWebCrawler` during the `arun` process.\n    * It takes the `url` (which should point to a PDF) and creates a minimal `AsyncCrawlResponse`.\n    * The `html` attribute of this response is typically empty or a placeholder, as the actual PDF content processing is deferred to the `PDFContentScrapingStrategy` (or a similar PDF-aware scraping strategy).\n    * It sets `response_headers` to indicate \"application/pdf\" and `status_code` to 200.\n  * **`async close(self)`**:\n    * A method for cleaning up any resources used by the strategy. For `PDFCrawlerStrategy`, this is usually minimal.\n  * **`async __aenter__(self)`/`async __aexit__(self, exc_type, exc_val, exc_tb)`** :\n    * Enables asynchronous context management for the strategy, allowing it to be used with `async with`.\n\n\n### Example Usage\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy\n\nasync def main():\n    # Initialize the PDF crawler strategy\n    pdf_crawler_strategy = PDFCrawlerStrategy()\n\n    # PDFCrawlerStrategy is typically used in conjunction with PDFContentScrapingStrategy\n    # The scraping strategy handles the actual PDF content extraction\n    pdf_scraping_strategy = PDFContentScrapingStrategy()\n    run_config = CrawlerRunConfig(scraping_strategy=pdf_scraping_strategy)\n\n    async with AsyncWebCrawler(crawler_strategy=pdf_crawler_strategy) as crawler:\n        # Example with a remote PDF URL\n        pdf_url = \"https://arxiv.org/pdf/2310.06825.pdf\" # A public PDF from arXiv\n\n        print(f\"Attempting to process PDF: {pdf_url}\")\n        result = await crawler.arun(url=pdf_url, config=run_config)\n\n        if result.success:\n            print(f\"Successfully processed PDF: {result.url}\")\n            print(f\"Metadata Title: {result.metadata.get('title', 'N/A')}\")\n            # Further processing of result.markdown, result.media, etc.\n            # would be done here, based on what PDFContentScrapingStrategy extracts.\n            if result.markdown and hasattr(result.markdown, 'raw_markdown'):\n                print(f\"Extracted text (first 200 chars): {result.markdown.raw_markdown[:200]}...\")\n            else:\n                print(\"No markdown (text) content extracted.\")\n        else:\n            print(f\"Failed to process PDF: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### Pros and Cons\n**Pros:** - Enables `AsyncWebCrawler` to handle PDF sources directly using familiar `arun` calls. - Provides a consistent interface for specifying PDF sources (URLs or local paths). - Abstracts the source handling, allowing a separate scraping strategy to focus on PDF content parsing.\n**Cons:** - Does not perform any PDF data extraction itself; it strictly relies on a compatible scraping strategy (like `PDFContentScrapingStrategy`) to process the PDF. - Has limited utility on its own; most of its value comes from being paired with a PDF-specific content scraping strategy.\n* * *\n## `PDFContentScrapingStrategy`\n### Overview\n`PDFContentScrapingStrategy` is an implementation of `ContentScrapingStrategy` designed to extract text, metadata, and optionally images from PDF documents. It is intended to be used in conjunction with a crawler strategy that can provide it with a PDF source, such as `PDFCrawlerStrategy`. This strategy uses the `NaivePDFProcessorStrategy` internally to perform the low-level PDF parsing.\n### When to Use\nUse `PDFContentScrapingStrategy` when your `AsyncWebCrawler` (often configured with `PDFCrawlerStrategy`) needs to: - Extract textual content page by page from a PDF document. - Retrieve standard metadata embedded within the PDF (e.g., title, author, subject, creation date, page count). - Optionally, extract images contained within the PDF pages. These images can be saved to a local directory or made available for further processing. - Produce a `ScrapingResult` that can be converted into a `CrawlResult`, making PDF content accessible in a manner similar to HTML web content (e.g., text in `result.markdown`, metadata in `result.metadata`).\n### Key Configuration Attributes\nWhen initializing `PDFContentScrapingStrategy`, you can configure its behavior using the following attributes: - **`extract_images: bool = False`**: If`True` , the strategy will attempt to extract images from the PDF. - **`save_images_locally: bool = False`**: If`True` (and `extract_images` is also `True`), extracted images will be saved to disk in the `image_save_dir`. If `False`, image data might be available in another form (e.g., base64, depending on the underlying processor) but not saved as separate files by this strategy. - **`image_save_dir: str = None`**: Specifies the directory where extracted images should be saved if`save_images_locally` is `True`. If `None`, a default or temporary directory might be used. - **`batch_size: int = 4`**: Defines how many PDF pages are processed in a single batch. This can be useful for managing memory when dealing with very large PDF documents. -**`logger: AsyncLogger = None`**: An optional`AsyncLogger` instance for logging.\n### Key Methods and Their Behavior\n  * **`__init__(self, save_images_locally: bool = False, extract_images: bool = False, image_save_dir: str = None, batch_size: int = 4, logger: AsyncLogger = None)`**:\n    * Initializes the strategy with configurations for image handling, batch processing, and logging. It sets up an internal `NaivePDFProcessorStrategy` instance which performs the actual PDF parsing.\n  * **`scrap(self, url: str, html: str, **params) -> ScrapingResult`**:\n    * This is the primary synchronous method called by the crawler (via `ascrap`) to process the PDF.\n    * `url`: The path or URL to the PDF file (provided by `PDFCrawlerStrategy` or similar).\n    * `html`: Typically an empty string when used with `PDFCrawlerStrategy`, as the content is a PDF, not HTML.\n    * It first ensures the PDF is accessible locally (downloads it to a temporary file if `url` is remote).\n    * It then uses its internal PDF processor to extract text, metadata, and images (if configured).\n    * The extracted information is compiled into a `ScrapingResult` object:\n      * `cleaned_html`: Contains an HTML-like representation of the PDF, where each page's content is often wrapped in a `<div>` with page number information.\n      * `media`: A dictionary where `media[\"images\"]` will contain information about extracted images if `extract_images` was `True`.\n      * `links`: A dictionary where `links[\"urls\"]` can contain URLs found within the PDF content.\n      * `metadata`: A dictionary holding PDF metadata (e.g., title, author, num_pages).\n  * **`async ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult`**:\n    * The asynchronous version of `scrap`. Under the hood, it typically runs the synchronous `scrap` method in a separate thread using `asyncio.to_thread` to avoid blocking the event loop.\n  * **`_get_pdf_path(self, url: str) -> str`**:\n    * A private helper method to manage PDF file access. If the `url` is remote (http/https), it downloads the PDF to a temporary local file and returns its path. If `url` indicates a local file (`file://` or a direct path), it resolves and returns the local path.\n\n\n### Example Usage\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy\nimport os # For creating image directory\n\nasync def main():\n    # Define the directory for saving extracted images\n    image_output_dir = \"./my_pdf_images\"\n    os.makedirs(image_output_dir, exist_ok=True)\n\n    # Configure the PDF content scraping strategy\n    # Enable image extraction and specify where to save them\n    pdf_scraping_cfg = PDFContentScrapingStrategy(\n        extract_images=True,\n        save_images_locally=True,\n        image_save_dir=image_output_dir,\n        batch_size=2 # Process 2 pages at a time for demonstration\n    )\n\n    # The PDFCrawlerStrategy is needed to tell AsyncWebCrawler how to \"crawl\" a PDF\n    pdf_crawler_cfg = PDFCrawlerStrategy()\n\n    # Configure the overall crawl run\n    run_cfg = CrawlerRunConfig(\n        scraping_strategy=pdf_scraping_cfg # Use our PDF scraping strategy\n    )\n\n    # Initialize the crawler with the PDF-specific crawler strategy\n    async with AsyncWebCrawler(crawler_strategy=pdf_crawler_cfg) as crawler:\n        pdf_url = \"https://arxiv.org/pdf/2310.06825.pdf\" # Example PDF\n\n        print(f\"Starting PDF processing for: {pdf_url}\")\n        result = await crawler.arun(url=pdf_url, config=run_cfg)\n\n        if result.success:\n            print(\"\\n--- PDF Processing Successful ---\")\n            print(f\"Processed URL: {result.url}\")\n\n            print(\"\\n--- Metadata ---\")\n            for key, value in result.metadata.items():\n                print(f\"  {key.replace('_', ' ').title()}: {value}\")\n\n            if result.markdown and hasattr(result.markdown, 'raw_markdown'):\n                print(f\"\\n--- Extracted Text (Markdown Snippet) ---\")\n                print(result.markdown.raw_markdown[:500].strip() + \"...\")\n            else:\n                print(\"\\nNo text (markdown) content extracted.\")\n\n            if result.media and result.media.get(\"images\"):\n                print(f\"\\n--- Image Extraction ---\")\n                print(f\"Extracted {len(result.media['images'])} image(s).\")\n                for i, img_info in enumerate(result.media[\"images\"][:2]): # Show info for first 2 images\n                    print(f\"  Image {i+1}:\")\n                    print(f\"    Page: {img_info.get('page')}\")\n                    print(f\"    Format: {img_info.get('format', 'N/A')}\")\n                    if img_info.get('path'):\n                        print(f\"    Saved at: {img_info.get('path')}\")\n            else:\n                print(\"\\nNo images were extracted (or extract_images was False).\")\n        else:\n            print(f\"\\n--- PDF Processing Failed ---\")\n            print(f\"Error: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### Pros and Cons\n**Pros:** - Provides a comprehensive way to extract text, metadata, and (optionally) images from PDF documents. - Handles both remote PDFs (via URL) and local PDF files. - Configurable image extraction allows saving images to disk or accessing their data. - Integrates smoothly with the `CrawlResult` object structure, making PDF-derived data accessible in a way consistent with web-scraped data. - The `batch_size` parameter can help in managing memory consumption when processing large or numerous PDF pages.\n**Cons:** - Extraction quality and performance can vary significantly depending on the PDF's complexity, encoding, and whether it's image-based (scanned) or text-based. - Image extraction can be resource-intensive (both CPU and disk space if `save_images_locally` is true). - Relies on `NaivePDFProcessorStrategy` internally, which might have limitations with very complex layouts, encrypted PDFs, or forms compared to more sophisticated PDF parsing libraries. Scanned PDFs will not yield text unless an OCR step is performed (which is not part of this strategy by default). - Link extraction from PDFs can be basic and depends on how hyperlinks are embedded in the document.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n\n\nESC to close\n#### On this page\n  * [PDFCrawlerStrategy](https://docs.crawl4ai.com/advanced/pdf-parsing/#pdfcrawlerstrategy)\n  * [Overview](https://docs.crawl4ai.com/advanced/pdf-parsing/#overview)\n  * [When to Use](https://docs.crawl4ai.com/advanced/pdf-parsing/#when-to-use)\n  * [Key Methods and Their Behavior](https://docs.crawl4ai.com/advanced/pdf-parsing/#key-methods-and-their-behavior)\n  * [Example Usage](https://docs.crawl4ai.com/advanced/pdf-parsing/#example-usage)\n  * [Pros and Cons](https://docs.crawl4ai.com/advanced/pdf-parsing/#pros-and-cons)\n  * [PDFContentScrapingStrategy](https://docs.crawl4ai.com/advanced/pdf-parsing/#pdfcontentscrapingstrategy)\n  * [Overview](https://docs.crawl4ai.com/advanced/pdf-parsing/#overview_1)\n  * [When to Use](https://docs.crawl4ai.com/advanced/pdf-parsing/#when-to-use_1)\n  * [Key Configuration Attributes](https://docs.crawl4ai.com/advanced/pdf-parsing/#key-configuration-attributes)\n  * [Key Methods and Their Behavior](https://docs.crawl4ai.com/advanced/pdf-parsing/#key-methods-and-their-behavior_1)\n  * [Example Usage](https://docs.crawl4ai.com/advanced/pdf-parsing/#example-usage_1)\n  * [Pros and Cons](https://docs.crawl4ai.com/advanced/pdf-parsing/#pros-and-cons_1)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/proxy-security",
    "depth": 1,
    "title": "Proxy & Security - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "dbe29f2c9654ab6c5a6752da0fab2ea9",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/proxy-security/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * Proxy & Security\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/#proxy-security)\n  * [Understanding Proxy Configuration](https://docs.crawl4ai.com/advanced/proxy-security/#understanding-proxy-configuration)\n  * [Basic Proxy Setup](https://docs.crawl4ai.com/advanced/proxy-security/#basic-proxy-setup)\n  * [Supported Proxy Formats](https://docs.crawl4ai.com/advanced/proxy-security/#supported-proxy-formats)\n  * [Authenticated Proxies](https://docs.crawl4ai.com/advanced/proxy-security/#authenticated-proxies)\n  * [Environment Variable Configuration](https://docs.crawl4ai.com/advanced/proxy-security/#environment-variable-configuration)\n  * [Rotating Proxies](https://docs.crawl4ai.com/advanced/proxy-security/#rotating-proxies)\n  * [SSL Certificate Analysis](https://docs.crawl4ai.com/advanced/proxy-security/#ssl-certificate-analysis)\n  * [Security Best Practices](https://docs.crawl4ai.com/advanced/proxy-security/#security-best-practices)\n  * [Migration from Deprecated proxy Parameter](https://docs.crawl4ai.com/advanced/proxy-security/#migration-from-deprecated-proxy-parameter)\n  * [Troubleshooting](https://docs.crawl4ai.com/advanced/proxy-security/#troubleshooting)\n\n\n# Proxy & Security\nThis guide covers proxy configuration and security features in Crawl4AI, including SSL certificate analysis and proxy rotation strategies.\n## Understanding Proxy Configuration\nCrawl4AI recommends configuring proxies per request through `CrawlerRunConfig.proxy_config`. This gives you precise control, enables rotation strategies, and keeps examples simple enough to copy, paste, and run.\n## Basic Proxy Setup\nConfigure proxies that apply to each crawl operation:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, ProxyConfig\n\nrun_config = CrawlerRunConfig(proxy_config=ProxyConfig(server=\"http://proxy.example.com:8080\"))\n# run_config = CrawlerRunConfig(proxy_config={\"server\": \"http://proxy.example.com:8080\"})\n# run_config = CrawlerRunConfig(proxy_config=\"http://proxy.example.com:8080\")\n\n\nasync def main():\n    browser_config = BrowserConfig()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=run_config)\n        print(f\"Success: {result.success} -> {result.url}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\nWhy request-level?\n`CrawlerRunConfig.proxy_config` keeps each request self-contained, so swapping proxies or rotation strategies is just a matter of building a new run configuration.\n## Supported Proxy Formats\nThe `ProxyConfig.from_string()` method supports multiple formats:\n```\nfrom crawl4ai import ProxyConfig\n\n# HTTP proxy with authentication\nproxy1 = ProxyConfig.from_string(\"http://user:pass@192.168.1.1:8080\")\n\n# HTTPS proxy\nproxy2 = ProxyConfig.from_string(\"https://proxy.example.com:8080\")\n\n# SOCKS5 proxy\nproxy3 = ProxyConfig.from_string(\"socks5://proxy.example.com:1080\")\n\n# Simple IP:port format\nproxy4 = ProxyConfig.from_string(\"192.168.1.1:8080\")\n\n# IP:port:user:pass format\nproxy5 = ProxyConfig.from_string(\"192.168.1.1:8080:user:pass\")\nCopy\n```\n\n## Authenticated Proxies\nFor proxies requiring authentication:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler,BrowserConfig, CrawlerRunConfig, ProxyConfig\n\nrun_config = CrawlerRunConfig(\n    proxy_config=ProxyConfig(\n        server=\"http://proxy.example.com:8080\",\n        username=\"your_username\",\n        password=\"your_password\",\n    )\n)\n# Or dictionary style:\n# run_config = CrawlerRunConfig(proxy_config={\n#     \"server\": \"http://proxy.example.com:8080\",\n#     \"username\": \"your_username\",\n#     \"password\": \"your_password\",\n# })\n\n\nasync def main():\n    browser_config = BrowserConfig()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=run_config)\n        print(f\"Success: {result.success} -> {result.url}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Environment Variable Configuration\nLoad proxies from environment variables for easy configuration:\n```\nimport os\nfrom crawl4ai import ProxyConfig, CrawlerRunConfig\n\n# Set environment variable\nos.environ[\"PROXIES\"] = \"ip1:port1:user1:pass1,ip2:port2:user2:pass2,ip3:port3\"\n\n# Load all proxies\nproxies = ProxyConfig.from_env()\nprint(f\"Loaded {len(proxies)} proxies\")\n\n# Use first proxy\nif proxies:\n    run_config = CrawlerRunConfig(proxy_config=proxies[0])\nCopy\n```\n\n## Rotating Proxies\nCrawl4AI supports automatic proxy rotation to distribute requests across multiple proxy servers. Rotation is applied per request using a rotation strategy on `CrawlerRunConfig`.\n### Proxy Rotation (recommended)\n```\nimport asyncio\nimport re\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, ProxyConfig\nfrom crawl4ai.proxy_strategy import RoundRobinProxyStrategy\n\nasync def main():\n    # Load proxies from environment\n    proxies = ProxyConfig.from_env()\n    if not proxies:\n        print(\"No proxies found! Set PROXIES environment variable.\")\n        return\n\n    # Create rotation strategy\n    proxy_strategy = RoundRobinProxyStrategy(proxies)\n\n    # Configure per-request with proxy rotation\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        proxy_rotation_strategy=proxy_strategy,\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        urls = [\"https://httpbin.org/ip\"] * (len(proxies) * 2)  # Test each proxy twice\n\n        print(f\"ðŸš€ Testing {len(proxies)} proxies with rotation...\")\n        results = await crawler.arun_many(urls=urls, config=run_config)\n\n        for i, result in enumerate(results):\n            if result.success:\n                # Extract IP from response\n                ip_match = re.search(r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}', result.html)\n                if ip_match:\n                    detected_ip = ip_match.group(0)\n                    proxy_index = i % len(proxies)\n                    expected_ip = proxies[proxy_index].ip\n\n                    print(f\"âœ… Request {i+1}: Proxy {proxy_index+1} -> IP {detected_ip}\")\n                    if detected_ip == expected_ip:\n                        print(\"   ðŸŽ¯ IP matches proxy configuration\")\n                    else:\n                        print(f\"   âš ï¸  IP mismatch (expected {expected_ip})\")\n                else:\n                    print(f\"âŒ Request {i+1}: Could not extract IP from response\")\n            else:\n                print(f\"âŒ Request {i+1}: Failed - {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## SSL Certificate Analysis\nCombine proxy usage with SSL certificate inspection for enhanced security analysis. SSL certificate fetching is configured per request via `CrawlerRunConfig`.\n### Per-Request SSL Certificate Analysis\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nrun_config = CrawlerRunConfig(\n    proxy_config={\n        \"server\": \"http://proxy.example.com:8080\",\n        \"username\": \"user\",\n        \"password\": \"pass\",\n    },\n    fetch_ssl_certificate=True,  # Enable SSL certificate analysis for this request\n)\n\n\nasync def main():\n    browser_config = BrowserConfig()\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=run_config)\n\n        if result.success:\n            print(f\"âœ… Crawled via proxy: {result.url}\")\n\n            # Analyze SSL certificate\n            if result.ssl_certificate:\n                cert = result.ssl_certificate\n                print(\"ðŸ”’ SSL Certificate Info:\")\n                print(f\"   Issuer: {cert.issuer}\")\n                print(f\"   Subject: {cert.subject}\")\n                print(f\"   Valid until: {cert.valid_until}\")\n                print(f\"   Fingerprint: {cert.fingerprint}\")\n\n                # Export certificate\n                cert.to_json(\"certificate.json\")\n                print(\"ðŸ’¾ Certificate exported to certificate.json\")\n            else:\n                print(\"âš ï¸  No SSL certificate information available\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Security Best Practices\n### 1. Proxy Rotation for Anonymity\n```\nfrom crawl4ai import CrawlerRunConfig, ProxyConfig\nfrom crawl4ai.proxy_strategy import RoundRobinProxyStrategy\n\n# Use multiple proxies to avoid IP blocking\nproxies = ProxyConfig.from_env(\"PROXIES\")\nstrategy = RoundRobinProxyStrategy(proxies)\n\n# Configure rotation per request (recommended)\nrun_config = CrawlerRunConfig(proxy_rotation_strategy=strategy)\n\n# For a fixed proxy across all requests, just reuse the same run_config instance\nstatic_run_config = run_config\nCopy\n```\n\n### 2. SSL Certificate Verification\n```\nfrom crawl4ai import CrawlerRunConfig\n\n# Always verify SSL certificates when possible\n# Per-request (affects specific requests)\nrun_config = CrawlerRunConfig(fetch_ssl_certificate=True)\nCopy\n```\n\n### 3. Environment Variable Security\n```\n# Use environment variables for sensitive proxy credentials\n# Avoid hardcoding usernames/passwords in code\nexport PROXIES=\"ip1:port1:user1:pass1,ip2:port2:user2:pass2\"\nCopy\n```\n\n### 4. SOCKS5 for Enhanced Security\n```\nfrom crawl4ai import CrawlerRunConfig\n\n# Prefer SOCKS5 proxies for better protocol support\nrun_config = CrawlerRunConfig(proxy_config=\"socks5://proxy.example.com:1080\")\nCopy\n```\n\n## Migration from Deprecated `proxy` Parameter\n  * \"Deprecation Notice\" The legacy `proxy` argument on `BrowserConfig` is deprecated. Configure proxies through `CrawlerRunConfig.proxy_config` so each request fully describes its network settings.\n\n\n```\n# Old (deprecated) approach\n# from crawl4ai import BrowserConfig\n# browser_config = BrowserConfig(proxy=\"http://proxy.example.com:8080\")\n\n# New (preferred) approach\nfrom crawl4ai import CrawlerRunConfig\nrun_config = CrawlerRunConfig(proxy_config=\"http://proxy.example.com:8080\")\nCopy\n```\n\n### Safe Logging of Proxies\n```\nfrom crawl4ai import ProxyConfig\n\ndef safe_proxy_repr(proxy: ProxyConfig):\n    if getattr(proxy, \"username\", None):\n        return f\"{proxy.server} (auth: ****)\"\n    return proxy.server\nCopy\n```\n\n## Troubleshooting\n### Common Issues\n  * \"Proxy connection failed\"\n    * Verify the proxy server is reachable from your network.\n    * Double-check authentication credentials.\n    * Ensure the protocol matches (`http`, `https`, or `socks5`).\n  * \"SSL certificate errors\"\n    * Some proxies break SSL inspection; switch proxies if you see repeated failures.\n    * Consider temporarily disabling certificate fetching to isolate the issue.\n  * \"Environment variables not loading\"\n    * Confirm `PROXIES` (or your custom env var) is set before running the script.\n    * Check formatting: `ip:port:user:pass,ip:port:user:pass`.\n  * \"Proxy rotation not working\"\n    * Ensure `ProxyConfig.from_env()` actually loaded entries (`len(proxies) > 0`).\n    * Attach `proxy_rotation_strategy` to `CrawlerRunConfig`.\n    * Validate the proxy definitions you pass into the strategy.\n\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/proxy-security/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/proxy-security/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/proxy-security/)\n\n\nESC to close\n#### On this page\n  * [Understanding Proxy Configuration](https://docs.crawl4ai.com/advanced/proxy-security/#understanding-proxy-configuration)\n  * [Basic Proxy Setup](https://docs.crawl4ai.com/advanced/proxy-security/#basic-proxy-setup)\n  * [Supported Proxy Formats](https://docs.crawl4ai.com/advanced/proxy-security/#supported-proxy-formats)\n  * [Authenticated Proxies](https://docs.crawl4ai.com/advanced/proxy-security/#authenticated-proxies)\n  * [Environment Variable Configuration](https://docs.crawl4ai.com/advanced/proxy-security/#environment-variable-configuration)\n  * [Rotating Proxies](https://docs.crawl4ai.com/advanced/proxy-security/#rotating-proxies)\n  * [Proxy Rotation (recommended)](https://docs.crawl4ai.com/advanced/proxy-security/#proxy-rotation-recommended)\n  * [SSL Certificate Analysis](https://docs.crawl4ai.com/advanced/proxy-security/#ssl-certificate-analysis)\n  * [Per-Request SSL Certificate Analysis](https://docs.crawl4ai.com/advanced/proxy-security/#per-request-ssl-certificate-analysis)\n  * [Security Best Practices](https://docs.crawl4ai.com/advanced/proxy-security/#security-best-practices)\n  * [1. Proxy Rotation for Anonymity](https://docs.crawl4ai.com/advanced/proxy-security/#1-proxy-rotation-for-anonymity)\n  * [2. SSL Certificate Verification](https://docs.crawl4ai.com/advanced/proxy-security/#2-ssl-certificate-verification)\n  * [3. Environment Variable Security](https://docs.crawl4ai.com/advanced/proxy-security/#3-environment-variable-security)\n  * [4. SOCKS5 for Enhanced Security](https://docs.crawl4ai.com/advanced/proxy-security/#4-socks5-for-enhanced-security)\n  * [Migration from Deprecated proxy Parameter](https://docs.crawl4ai.com/advanced/proxy-security/#migration-from-deprecated-proxy-parameter)\n  * [Safe Logging of Proxies](https://docs.crawl4ai.com/advanced/proxy-security/#safe-logging-of-proxies)\n  * [Troubleshooting](https://docs.crawl4ai.com/advanced/proxy-security/#troubleshooting)\n  * [Common Issues](https://docs.crawl4ai.com/advanced/proxy-security/#common-issues)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/session-management",
    "depth": 1,
    "title": "Session Management - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "c14368fd02cc9839e150290d980eb8d9",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/session-management/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * Session Management\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Session Management](https://docs.crawl4ai.com/advanced/session-management/#session-management)\n  * [Basic Session Usage](https://docs.crawl4ai.com/advanced/session-management/#basic-session-usage)\n  * [Dynamic Content with Sessions](https://docs.crawl4ai.com/advanced/session-management/#dynamic-content-with-sessions)\n  * [Example 1: Basic Session-Based Crawling](https://docs.crawl4ai.com/advanced/session-management/#example-1-basic-session-based-crawling)\n  * [Advanced Technique 1: Custom Execution Hooks](https://docs.crawl4ai.com/advanced/session-management/#advanced-technique-1-custom-execution-hooks)\n  * [Advanced Technique 2: Integrated JavaScript Execution and Waiting](https://docs.crawl4ai.com/advanced/session-management/#advanced-technique-2-integrated-javascript-execution-and-waiting)\n\n\n# Session Management\nSession management in Crawl4AI is a powerful feature that allows you to maintain state across multiple requests, making it particularly suitable for handling complex multi-step crawling tasks. It enables you to reuse the same browser tab (or page object) across sequential actions and crawls, which is beneficial for:\n  * **Performing JavaScript actions before and after crawling.**\n  * **Executing multiple sequential crawls faster** without needing to reopen tabs or allocate memory repeatedly.\n\n\n**Note:** This feature is designed for sequential workflows and is not suitable for parallel operations.\n* * *\n#### Basic Session Usage\nUse `BrowserConfig` and `CrawlerRunConfig` to maintain state with a `session_id`:\n```\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n\n    # Define configurations\n    config1 = CrawlerRunConfig(\n        url=\"https://example.com/page1\", session_id=session_id\n    )\n    config2 = CrawlerRunConfig(\n        url=\"https://example.com/page2\", session_id=session_id\n    )\n\n    # First request\n    result1 = await crawler.arun(config=config1)\n\n    # Subsequent request using the same session\n    result2 = await crawler.arun(config=config2)\n\n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\nCopy\n```\n\n* * *\n#### Dynamic Content with Sessions\nHere's an example of crawling GitHub commits across multiple pages while preserving session state:\n```\nfrom crawl4ai.async_configs import CrawlerRunConfig\nfrom crawl4ai import JsonCssExtractionStrategy\nfrom crawl4ai.cache_context import CacheMode\n\nasync def crawl_dynamic_content():\n    url = \"https://github.com/microsoft/TypeScript/commits/main\"\n    session_id = \"wait_for_session\"\n    all_commits = []\n\n    js_next_page = \"\"\"\n    const commits = document.querySelectorAll('li[data-testid=\"commit-row-item\"] h4');\n    if (commits.length > 0) {\n        window.lastCommit = commits[0].textContent.trim();\n    }\n    const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n    if (button) {button.click(); console.log('button clicked') }\n    \"\"\"\n\n    wait_for = \"\"\"() => {\n        const commits = document.querySelectorAll('li[data-testid=\"commit-row-item\"] h4');\n        if (commits.length === 0) return false;\n        const firstCommit = commits[0].textContent.trim();\n        return firstCommit !== window.lastCommit;\n    }\"\"\"\n\n    schema = {\n        \"name\": \"Commit Extractor\",\n        \"baseSelector\": \"li[data-testid='commit-row-item']\",\n        \"fields\": [\n            {\n                \"name\": \"title\",\n                \"selector\": \"h4 a\",\n                \"type\": \"text\",\n                \"transform\": \"strip\",\n            },\n        ],\n    }\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n\n    browser_config = BrowserConfig(\n        verbose=True,\n        headless=False,\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        for page in range(3):\n            crawler_config = CrawlerRunConfig(\n                session_id=session_id,\n                css_selector=\"li[data-testid='commit-row-item']\",\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS,\n                capture_console_messages=True,\n            )\n\n            result = await crawler.arun(url=url, config=crawler_config)\n\n            if result.console_messages:\n                print(f\"Page {page + 1} console messages:\", result.console_messages)\n\n            if result.extracted_content:\n                # print(f\"Page {page + 1} result:\", result.extracted_content)\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n            else:\n                print(f\"Page {page + 1}: No content extracted\")\n\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\nCopy\n```\n\n* * *\n## Example 1: Basic Session-Based Crawling\nA simple example using session-based crawling:\n```\nimport asyncio\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nfrom crawl4ai.cache_context import CacheMode\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"dynamic_content_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\nCopy\n```\n\nThis example shows: 1. Reusing the same `session_id` across multiple requests. 2. Executing JavaScript to load more content dynamically. 3. Properly closing the session to free resources.\n* * *\n## Advanced Technique 1: Custom Execution Hooks\n> Warning: You might feel confused by the end of the next few examples ðŸ˜…, so make sure you are comfortable with the order of the parts before you start this.\nUse custom hooks to handle complex scenarios, such as waiting for content to load dynamically:\n```\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\").strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear: {e}\")\n\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"commit_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        js_next_page = \"\"\"document.querySelector('a.pagination-next').click();\"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(advanced_session_crawl_with_hooks())\nCopy\n```\n\nThis technique ensures new content loads before the next action.\n* * *\n## Advanced Technique 2: Integrated JavaScript Execution and Waiting\nCombine JavaScript execution and waiting logic for concise handling of dynamic content:\n```\nasync def integrated_js_and_wait_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"integrated_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n\n        js_next_page_and_wait = \"\"\"\n        (async () => {\n            const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim();\n            const initialCommit = getCurrentCommit();\n            document.querySelector('a.pagination-next').click();\n            while (getCurrentCommit() === initialCommit) {\n                await new Promise(resolve => setTimeout(resolve, 100));\n            }\n        })();\n        \"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page_and_wait if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(integrated_js_and_wait_crawl())\nCopy\n```\n\n* * *\n#### Common Use Cases for Sessions\n1. **Authentication Flows** : Login and interact with secured pages.\n2. **Pagination Handling** : Navigate through multiple pages.\n3. **Form Submissions** : Fill forms, submit, and process results.\n4. **Multi-step Processes** : Complete workflows that span multiple actions.\n5. **Dynamic Content Navigation** : Handle JavaScript-rendered or event-triggered content.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/session-management/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/session-management/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/session-management/)\n\n\nESC to close\n#### On this page\n  * [Basic Session Usage](https://docs.crawl4ai.com/advanced/session-management/#basic-session-usage)\n  * [Dynamic Content with Sessions](https://docs.crawl4ai.com/advanced/session-management/#dynamic-content-with-sessions)\n  * [Example 1: Basic Session-Based Crawling](https://docs.crawl4ai.com/advanced/session-management/#example-1-basic-session-based-crawling)\n  * [Advanced Technique 1: Custom Execution Hooks](https://docs.crawl4ai.com/advanced/session-management/#advanced-technique-1-custom-execution-hooks)\n  * [Advanced Technique 2: Integrated JavaScript Execution and Waiting](https://docs.crawl4ai.com/advanced/session-management/#advanced-technique-2-integrated-javascript-execution-and-waiting)\n  * [Common Use Cases for Sessions](https://docs.crawl4ai.com/advanced/session-management/#common-use-cases-for-sessions)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/undetected-browser",
    "depth": 1,
    "title": "Undetected Browser - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "8c64e182a2fa5f4fd79014d2f6e6e778",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/undetected-browser/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * Undetected Browser\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Undetected Browser Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#undetected-browser-mode)\n  * [Overview](https://docs.crawl4ai.com/advanced/undetected-browser/#overview)\n  * [Anti-Bot Features Comparison](https://docs.crawl4ai.com/advanced/undetected-browser/#anti-bot-features-comparison)\n  * [When to Use Each Approach](https://docs.crawl4ai.com/advanced/undetected-browser/#when-to-use-each-approach)\n  * [Stealth Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#stealth-mode)\n  * [Undetected Browser Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#undetected-browser-mode_1)\n  * [Combining Both Features](https://docs.crawl4ai.com/advanced/undetected-browser/#combining-both-features)\n  * [Examples](https://docs.crawl4ai.com/advanced/undetected-browser/#examples)\n  * [Browser Adapter Pattern](https://docs.crawl4ai.com/advanced/undetected-browser/#browser-adapter-pattern)\n  * [Best Practices](https://docs.crawl4ai.com/advanced/undetected-browser/#best-practices)\n  * [Advanced Usage Tips](https://docs.crawl4ai.com/advanced/undetected-browser/#advanced-usage-tips)\n  * [Installation](https://docs.crawl4ai.com/advanced/undetected-browser/#installation)\n  * [Limitations](https://docs.crawl4ai.com/advanced/undetected-browser/#limitations)\n  * [Troubleshooting](https://docs.crawl4ai.com/advanced/undetected-browser/#troubleshooting)\n  * [Future Plans](https://docs.crawl4ai.com/advanced/undetected-browser/#future-plans)\n  * [Conclusion](https://docs.crawl4ai.com/advanced/undetected-browser/#conclusion)\n  * [See Also](https://docs.crawl4ai.com/advanced/undetected-browser/#see-also)\n\n\n# Undetected Browser Mode\n## Overview\nCrawl4AI offers two powerful anti-bot features to help you access websites with bot detection:\n  1. **Stealth Mode** - Uses playwright-stealth to modify browser fingerprints and behaviors\n  2. **Undetected Browser Mode** - Advanced browser adapter with deep-level patches for sophisticated bot detection\n\n\nThis guide covers both features and helps you choose the right approach for your needs.\n## Anti-Bot Features Comparison\nFeature | Regular Browser | Stealth Mode | Undetected Browser  \n---|---|---|---  \nWebDriver Detection | âŒ | âœ… | âœ…  \nNavigator Properties | âŒ | âœ… | âœ…  \nPlugin Emulation | âŒ | âœ… | âœ…  \nCDP Detection | âŒ | Partial | âœ…  \nDeep Browser Patches | âŒ | âŒ | âœ…  \nPerformance Impact | None | Minimal | Moderate  \nSetup Complexity | None | None | Minimal  \n## When to Use Each Approach\n### Use Regular Browser + Stealth Mode When:\n  * Sites have basic bot detection (checking navigator.webdriver, plugins, etc.)\n  * You need good performance with basic protection\n  * Sites check for common automation indicators\n\n\n### Use Undetected Browser When:\n  * Sites employ sophisticated bot detection services (Cloudflare, DataDome, etc.)\n  * Stealth mode alone isn't sufficient\n  * You're willing to trade some performance for better evasion\n\n\n### Best Practice: Progressive Enhancement\n  1. **Start with** : Regular browser + Stealth mode\n  2. **If blocked** : Switch to Undetected browser\n  3. **If still blocked** : Combine Undetected browser + Stealth mode\n\n\n## Stealth Mode\nStealth mode is the simpler anti-bot solution that works with both regular and undetected browsers:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\n# Enable stealth mode with regular browser\nbrowser_config = BrowserConfig(\n    enable_stealth=True,  # Simple flag to enable\n    headless=False       # Better for avoiding detection\n)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(\"https://example.com\")\nCopy\n```\n\n### What Stealth Mode Does:\n  * Removes `navigator.webdriver` flag\n  * Modifies browser fingerprints\n  * Emulates realistic plugin behavior\n  * Adjusts navigator properties\n  * Fixes common automation leaks\n\n\n## Undetected Browser Mode\nFor sites with sophisticated bot detection that stealth mode can't bypass, use the undetected browser adapter:\n### Key Features\n  * **Drop-in Replacement** : Uses the same API as regular browser mode\n  * **Enhanced Stealth** : Built-in patches to evade common detection methods\n  * **Browser Adapter Pattern** : Seamlessly switch between regular and undetected modes\n  * **Automatic Installation** : `crawl4ai-setup` installs all necessary browser dependencies\n\n\n### Quick Start\n```\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler, \n    BrowserConfig, \n    CrawlerRunConfig,\n    UndetectedAdapter\n)\nfrom crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n\nasync def main():\n    # Create the undetected adapter\n    undetected_adapter = UndetectedAdapter()\n\n    # Create browser config\n    browser_config = BrowserConfig(\n        headless=False,  # Headless mode can be detected easier\n        verbose=True,\n    )\n\n    # Create the crawler strategy with undetected adapter\n    crawler_strategy = AsyncPlaywrightCrawlerStrategy(\n        browser_config=browser_config,\n        browser_adapter=undetected_adapter\n    )\n\n    # Create the crawler with our custom strategy\n    async with AsyncWebCrawler(\n        crawler_strategy=crawler_strategy,\n        config=browser_config\n    ) as crawler:\n        # Your crawling code here\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=CrawlerRunConfig()\n        )\n        print(result.markdown[:500])\n\nasyncio.run(main())\nCopy\n```\n\n## Combining Both Features\nFor maximum evasion, combine stealth mode with undetected browser:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, UndetectedAdapter\nfrom crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n\n# Create browser config with stealth enabled\nbrowser_config = BrowserConfig(\n    enable_stealth=True,  # Enable stealth mode\n    headless=False\n)\n\n# Create undetected adapter\nadapter = UndetectedAdapter()\n\n# Create strategy with both features\nstrategy = AsyncPlaywrightCrawlerStrategy(\n    browser_config=browser_config,\n    browser_adapter=adapter\n)\n\nasync with AsyncWebCrawler(\n    crawler_strategy=strategy,\n    config=browser_config\n) as crawler:\n    result = await crawler.arun(\"https://protected-site.com\")\nCopy\n```\n\n## Examples\n### Example 1: Basic Stealth Mode\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def test_stealth_mode():\n    # Simple stealth mode configuration\n    browser_config = BrowserConfig(\n        enable_stealth=True,\n        headless=False\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://bot.sannysoft.com\",\n            config=CrawlerRunConfig(screenshot=True)\n        )\n\n        if result.success:\n            print(\"âœ“ Successfully accessed bot detection test site\")\n            # Save screenshot to verify detection results\n            if result.screenshot:\n                import base64\n                with open(\"stealth_test.png\", \"wb\") as f:\n                    f.write(base64.b64decode(result.screenshot))\n                print(\"âœ“ Screenshot saved - check for green (passed) tests\")\n\nasyncio.run(test_stealth_mode())\nCopy\n```\n\n### Example 2: Undetected Browser Mode\n```\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    BrowserConfig,\n    CrawlerRunConfig,\n    UndetectedAdapter\n)\nfrom crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy\n\n\nasync def main():\n    # Create browser config\n    browser_config = BrowserConfig(\n        headless=False,\n        verbose=True,\n    )\n\n    # Create the undetected adapter\n    undetected_adapter = UndetectedAdapter()\n\n    # Create the crawler strategy with the undetected adapter\n    crawler_strategy = AsyncPlaywrightCrawlerStrategy(\n        browser_config=browser_config,\n        browser_adapter=undetected_adapter\n    )\n\n    # Create the crawler with our custom strategy\n    async with AsyncWebCrawler(\n        crawler_strategy=crawler_strategy,\n        config=browser_config\n    ) as crawler:\n        # Configure the crawl\n        crawler_config = CrawlerRunConfig(\n            markdown_generator=DefaultMarkdownGenerator(\n                content_filter=PruningContentFilter()\n            ),\n            capture_console_messages=True,  # Test adapter console capture\n        )\n\n        # Test on a site that typically detects bots\n        print(\"Testing undetected adapter...\")\n        result: CrawlResult = await crawler.arun(\n            url=\"https://www.helloworld.org\", \n            config=crawler_config\n        )\n\n        print(f\"Status: {result.status_code}\")\n        print(f\"Success: {result.success}\")\n        print(f\"Console messages captured: {len(result.console_messages or [])}\")\n        print(f\"Markdown content (first 500 chars):\\n{result.markdown.raw_markdown[:500]}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Browser Adapter Pattern\nThe undetected browser support is implemented using an adapter pattern, allowing seamless switching between different browser implementations:\n```\n# Regular browser adapter (default)\nfrom crawl4ai import PlaywrightAdapter\nregular_adapter = PlaywrightAdapter()\n\n# Undetected browser adapter\nfrom crawl4ai import UndetectedAdapter\nundetected_adapter = UndetectedAdapter()\nCopy\n```\n\nThe adapter handles: - JavaScript execution - Console message capture - Error handling - Browser-specific optimizations\n## Best Practices\n  1. **Avoid Headless Mode** : Detection is easier in headless mode \n```\nbrowser_config = BrowserConfig(headless=False)\nCopy\n```\n\n  2. **Use Reasonable Delays** : Don't rush through pages \n```\ncrawler_config = CrawlerRunConfig(\n    wait_time=3.0,  # Wait 3 seconds after page load\n    delay_before_return_html=2.0  # Additional delay\n)\nCopy\n```\n\n  3. **Rotate User Agents** : You can customize user agents \n```\nbrowser_config = BrowserConfig(\n    headers={\"User-Agent\": \"your-user-agent\"}\n)\nCopy\n```\n\n  4. **Handle Failures Gracefully** : Some sites may still detect and block \n```\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\nCopy\n```\n\n\n\n## Advanced Usage Tips\n### Progressive Detection Handling\n```\nasync def crawl_with_progressive_evasion(url):\n    # Step 1: Try regular browser with stealth\n    browser_config = BrowserConfig(\n        enable_stealth=True,\n        headless=False\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url)\n        if result.success and \"Access Denied\" not in result.html:\n            return result\n\n    # Step 2: If blocked, try undetected browser\n    print(\"Regular + stealth blocked, trying undetected browser...\")\n\n    adapter = UndetectedAdapter()\n    strategy = AsyncPlaywrightCrawlerStrategy(\n        browser_config=browser_config,\n        browser_adapter=adapter\n    )\n\n    async with AsyncWebCrawler(\n        crawler_strategy=strategy,\n        config=browser_config\n    ) as crawler:\n        result = await crawler.arun(url)\n        return result\nCopy\n```\n\n## Installation\nThe undetected browser dependencies are automatically installed when you run:\n```\ncrawl4ai-setup\nCopy\n```\n\nThis command installs all necessary browser dependencies for both regular and undetected modes.\n## Limitations\n  * **Performance** : Slightly slower than regular mode due to additional patches\n  * **Headless Detection** : Some sites can still detect headless mode\n  * **Resource Usage** : May use more resources than regular mode\n  * **Not 100% Guaranteed** : Advanced anti-bot services are constantly evolving\n\n\n## Troubleshooting\n### Browser Not Found\nRun the setup command: \n```\ncrawl4ai-setup\nCopy\n```\n\n### Detection Still Occurring\nTry combining with other features: \n```\ncrawler_config = CrawlerRunConfig(\n    simulate_user=True,  # Add user simulation\n    magic=True,  # Enable magic mode\n    wait_time=5.0,  # Longer waits\n)\nCopy\n```\n\n### Performance Issues\nIf experiencing slow performance: \n```\n# Use selective undetected mode only for protected sites\nif is_protected_site(url):\n    adapter = UndetectedAdapter()\nelse:\n    adapter = PlaywrightAdapter()  # Default adapter\nCopy\n```\n\n## Future Plans\n**Note** : In future versions of Crawl4AI, we may enable stealth mode and undetected browser by default to provide better out-of-the-box success rates. For now, users should explicitly enable these features when needed.\n## Conclusion\nCrawl4AI provides flexible anti-bot solutions:\n  1. **Start Simple** : Use regular browser + stealth mode for most sites\n  2. **Escalate if Needed** : Switch to undetected browser for sophisticated protection\n  3. **Combine for Maximum Effect** : Use both features together when facing the toughest challenges\n\n\nRemember: - Always respect robots.txt and website terms of service - Use appropriate delays to avoid overwhelming servers - Consider the performance trade-offs of each approach - Test progressively to find the minimum necessary evasion level\n## See Also\n  * [Advanced Features](https://docs.crawl4ai.com/advanced/advanced-features/) - Overview of all advanced features\n  * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/) - Using proxies with anti-bot features\n  * [Session Management](https://docs.crawl4ai.com/advanced/session-management/) - Maintaining sessions across requests\n  * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/) - Additional anti-detection strategies\n\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/undetected-browser/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/undetected-browser/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/undetected-browser/)\n\n\nESC to close\n#### On this page\n  * [Overview](https://docs.crawl4ai.com/advanced/undetected-browser/#overview)\n  * [Anti-Bot Features Comparison](https://docs.crawl4ai.com/advanced/undetected-browser/#anti-bot-features-comparison)\n  * [When to Use Each Approach](https://docs.crawl4ai.com/advanced/undetected-browser/#when-to-use-each-approach)\n  * [Use Regular Browser + Stealth Mode When:](https://docs.crawl4ai.com/advanced/undetected-browser/#use-regular-browser-stealth-mode-when)\n  * [Use Undetected Browser When:](https://docs.crawl4ai.com/advanced/undetected-browser/#use-undetected-browser-when)\n  * [Best Practice: Progressive Enhancement](https://docs.crawl4ai.com/advanced/undetected-browser/#best-practice-progressive-enhancement)\n  * [Stealth Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#stealth-mode)\n  * [What Stealth Mode Does:](https://docs.crawl4ai.com/advanced/undetected-browser/#what-stealth-mode-does)\n  * [Undetected Browser Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#undetected-browser-mode_1)\n  * [Key Features](https://docs.crawl4ai.com/advanced/undetected-browser/#key-features)\n  * [Quick Start](https://docs.crawl4ai.com/advanced/undetected-browser/#quick-start)\n  * [Combining Both Features](https://docs.crawl4ai.com/advanced/undetected-browser/#combining-both-features)\n  * [Examples](https://docs.crawl4ai.com/advanced/undetected-browser/#examples)\n  * [Example 1: Basic Stealth Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#example-1-basic-stealth-mode)\n  * [Example 2: Undetected Browser Mode](https://docs.crawl4ai.com/advanced/undetected-browser/#example-2-undetected-browser-mode)\n  * [Browser Adapter Pattern](https://docs.crawl4ai.com/advanced/undetected-browser/#browser-adapter-pattern)\n  * [Best Practices](https://docs.crawl4ai.com/advanced/undetected-browser/#best-practices)\n  * [Advanced Usage Tips](https://docs.crawl4ai.com/advanced/undetected-browser/#advanced-usage-tips)\n  * [Progressive Detection Handling](https://docs.crawl4ai.com/advanced/undetected-browser/#progressive-detection-handling)\n  * [Installation](https://docs.crawl4ai.com/advanced/undetected-browser/#installation)\n  * [Limitations](https://docs.crawl4ai.com/advanced/undetected-browser/#limitations)\n  * [Troubleshooting](https://docs.crawl4ai.com/advanced/undetected-browser/#troubleshooting)\n  * [Browser Not Found](https://docs.crawl4ai.com/advanced/undetected-browser/#browser-not-found)\n  * [Detection Still Occurring](https://docs.crawl4ai.com/advanced/undetected-browser/#detection-still-occurring)\n  * [Performance Issues](https://docs.crawl4ai.com/advanced/undetected-browser/#performance-issues)\n  * [Future Plans](https://docs.crawl4ai.com/advanced/undetected-browser/#future-plans)\n  * [Conclusion](https://docs.crawl4ai.com/advanced/undetected-browser/#conclusion)\n  * [See Also](https://docs.crawl4ai.com/advanced/undetected-browser/#see-also)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/async-webcrawler",
    "depth": 1,
    "title": "AsyncWebCrawler - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "c1dc177f4c4bf2a6dbbad11ea580bc61",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/async-webcrawler/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * AsyncWebCrawler\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/#asyncwebcrawler)\n  * [1. Constructor Overview](https://docs.crawl4ai.com/api/async-webcrawler/#1-constructor-overview)\n  * [2. Lifecycle: Start/Close or Context Manager](https://docs.crawl4ai.com/api/async-webcrawler/#2-lifecycle-startclose-or-context-manager)\n  * [3. Primary Method: arun()](https://docs.crawl4ai.com/api/async-webcrawler/#3-primary-method-arun)\n  * [4. Batch Processing: arun_many()](https://docs.crawl4ai.com/api/async-webcrawler/#4-batch-processing-arun_many)\n  * [7. Best Practices & Migration Notes](https://docs.crawl4ai.com/api/async-webcrawler/#7-best-practices-migration-notes)\n  * [8. Summary](https://docs.crawl4ai.com/api/async-webcrawler/#8-summary)\n\n\n# AsyncWebCrawler\nThe **`AsyncWebCrawler`**is the core class for asynchronous web crawling in Crawl4AI. You typically create it**once** , optionally customize it with a **`BrowserConfig`**(e.g., headless, user agent), then**run** multiple **`arun()`**calls with different**`CrawlerRunConfig`**objects.\n**Recommended usage** :\n1. **Create** a `BrowserConfig` for global browser settings. \n2. **Instantiate** `AsyncWebCrawler(config=browser_config)`. \n3. **Use** the crawler in an async context manager (`async with`) or manage start/close manually. \n4. **Call** `arun(url, config=crawler_run_config)` for each page you want.\n* * *\n## 1. Constructor Overview\n```\nclass AsyncWebCrawler:\n    def __init__(\n        self,\n        crawler_strategy: Optional[AsyncCrawlerStrategy] = None,\n        config: Optional[BrowserConfig] = None,\n        always_bypass_cache: bool = False,           # deprecated\n        always_by_pass_cache: Optional[bool] = None, # also deprecated\n        base_directory: str = ...,\n        thread_safe: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Create an AsyncWebCrawler instance.\n\n        Args:\n            crawler_strategy: \n                (Advanced) Provide a custom crawler strategy if needed.\n            config: \n                A BrowserConfig object specifying how the browser is set up.\n            always_bypass_cache: \n                (Deprecated) Use CrawlerRunConfig.cache_mode instead.\n            base_directory:     \n                Folder for storing caches/logs (if relevant).\n            thread_safe: \n                If True, attempts some concurrency safeguards.â€€Usually False.\n            **kwargs: \n                Additional legacy or debugging parameters.\n        \"\"\"\n    )\n\n### Typical Initialization\n\n```python\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    verbose=True\n)\n\ncrawler = AsyncWebCrawler(config=browser_cfg)\nCopy\n```\n\n**Notes** :\n  * **Legacy** parameters like `always_bypass_cache` remain for backward compatibility, but prefer to set **caching** in `CrawlerRunConfig`.\n\n\n* * *\n## 2. Lifecycle: Start/Close or Context Manager\n### 2.1 Context Manager (Recommended)\n```\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    # The crawler automatically starts/closes resources\nCopy\n```\n\nWhen the `async with` block ends, the crawler cleans up (closes the browser, etc.).\n### 2.2 Manual Start & Close\n```\ncrawler = AsyncWebCrawler(config=browser_cfg)\nawait crawler.start()\n\nresult1 = await crawler.arun(\"https://example.com\")\nresult2 = await crawler.arun(\"https://another.com\")\n\nawait crawler.close()\nCopy\n```\n\nUse this style if you have a **long-running** application or need full control of the crawlerâ€™s lifecycle.\n* * *\n## 3. Primary Method: `arun()`\n```\nasync def arun(\n    self,\n    url: str,\n    config: Optional[CrawlerRunConfig] = None,\n    # Legacy parameters for backward compatibility...\n) -> CrawlResult:\n    ...\nCopy\n```\n\n### 3.1 New Approach\nYou pass a `CrawlerRunConfig` object that sets up everything about a crawlâ€”content filtering, caching, session reuse, JS code, screenshots, etc.\n```\nimport asyncio\nfrom crawl4ai import CrawlerRunConfig, CacheMode\n\nrun_cfg = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    css_selector=\"main.article\",\n    word_count_threshold=10,\n    screenshot=True\n)\n\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com/news\", config=run_cfg)\n    print(\"Crawled HTML length:\", len(result.cleaned_html))\n    if result.screenshot:\n        print(\"Screenshot base64 length:\", len(result.screenshot))\nCopy\n```\n\n### 3.2 Legacy Parameters Still Accepted\nFor **backward** compatibility, `arun()` can still accept direct arguments like `css_selector=...`, `word_count_threshold=...`, etc., but we strongly advise migrating them into a **`CrawlerRunConfig`**.\n* * *\n## 4. Batch Processing: `arun_many()`\n```\nasync def arun_many(\n    self,\n    urls: List[str],\n    config: Optional[CrawlerRunConfig] = None,\n    # Legacy parameters maintained for backwards compatibility...\n) -> List[CrawlResult]:\n    \"\"\"\n    Process multiple URLs with intelligent rate limiting and resource monitoring.\n    \"\"\"\nCopy\n```\n\n### 4.1 Resource-Aware Crawling\nThe `arun_many()` method now uses an intelligent dispatcher that:\n  * Monitors system memory usage\n  * Implements adaptive rate limiting\n  * Provides detailed progress monitoring\n  * Manages concurrent crawls efficiently\n\n\n### 4.2 Example Usage\nCheck page [Multi-url Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/) for a detailed example of how to use `arun_many()`.\n```\n### 4.3 Key Features\n\n1.â€€**Rate Limiting**\n\n   - Automatic delay between requests\n   - Exponential backoff on rate limit detection\n   - Domain-specific rate limiting\n   - Configurable retry strategy\n\n2.â€€**Resource Monitoring**\n\n   - Memory usage tracking\n   - Adaptive concurrency based on system load\n   - Automatic pausing when resources are constrained\n\n3.â€€**Progress Monitoring**\n\n   - Detailed or aggregated progress display\n   - Real-time status updates\n   - Memory usage statistics\n\n4.â€€**Error Handling**\n\n   - Graceful handling of rate limits\n   - Automatic retries with backoff\n   - Detailed error reporting\n\n---\n\n## 5.â€€`CrawlResult` Output\n\nEach `arun()` returns a **`CrawlResult`** containing:\n\n- `url`: Final URL (if redirected).\n- `html`: Original HTML.\n- `cleaned_html`: Sanitized HTML.\n- `markdown_v2`: Deprecated. Instead just use regular `markdown`\n- `extracted_content`: If an extraction strategy was used (JSON for CSS/LLM strategies).\n- `screenshot`, `pdf`: If screenshots/PDF requested.\n- `media`, `links`: Information about discovered images/links.\n- `success`, `error_message`: Status info.\n\nFor details, see [CrawlResult doc](./crawl-result.md).\n\n---\n\n## 6.â€€Quick Example\n\nBelow is an example hooking it all together:\n\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    # 1.â€€Browser config\n    browser_cfg = BrowserConfig(\n        browser_type=\"firefox\",\n        headless=False,\n        verbose=True\n    )\n\n    # 2.â€€Run config\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\n                \"name\": \"title\", \n                \"selector\": \"h2\", \n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"url\", \n                \"selector\": \"a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        word_count_threshold=15,\n        remove_overlay_elements=True,\n        wait_for=\"css:.post\"  # Wait for posts to appear\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog\",\n            config=run_cfg\n        )\n\n        if result.success:\n            print(\"Cleaned HTML length:\", len(result.cleaned_html))\n            if result.extracted_content:\n                articles = json.loads(result.extracted_content)\n                print(\"Extracted articles:\", articles[:2])\n        else:\n            print(\"Error:\", result.error_message)\n\nasyncio.run(main())\nCopy\n```\n\n**Explanation** :\n  * We define a **`BrowserConfig`**with Firefox, no headless, and`verbose=True`. \n  * We define a **`CrawlerRunConfig`**that**bypasses cache** , uses a **CSS** extraction schema, has a `word_count_threshold=15`, etc. \n  * We pass them to `AsyncWebCrawler(config=...)` and `arun(url=..., config=...)`.\n\n\n* * *\n## 7. Best Practices & Migration Notes\n1. **Use** `BrowserConfig` for **global** settings about the browserâ€™s environment. 2. **Use** `CrawlerRunConfig` for **per-crawl** logic (caching, content filtering, extraction strategies, wait conditions). 3. **Avoid** legacy parameters like `css_selector` or `word_count_threshold` directly in `arun()`. Instead:\n```\nrun_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20)\nresult = await crawler.arun(url=\"...\", config=run_cfg)\nCopy\n```\n\n4. **Context Manager** usage is simplest unless you want a persistent crawler across many calls.\n* * *\n## 8. Summary\n**AsyncWebCrawler** is your entry point to asynchronous crawling:\n  * **Constructor** accepts **`BrowserConfig`**(or defaults).\n  * **`arun(url, config=CrawlerRunConfig)`**is the main method for single-page crawls.\n  * **`arun_many(urls, config=CrawlerRunConfig)`**handles concurrency across multiple URLs.\n  * For advanced lifecycle control, use `start()` and `close()` explicitly. \n\n\n**Migration** : \n  * If you used `AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\")`, move browser settings to `BrowserConfig(...)` and content/crawl logic to `CrawlerRunConfig(...)`.\n\n\nThis modular approach ensures your code is **clean** , **scalable** , and **easy to maintain**. For any advanced or rarely used parameters, see the [BrowserConfig docs](https://docs.crawl4ai.com/api/parameters/).\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/async-webcrawler/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/async-webcrawler/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/async-webcrawler/)\n\n\nESC to close\n#### On this page\n  * [1. Constructor Overview](https://docs.crawl4ai.com/api/async-webcrawler/#1-constructor-overview)\n  * [2. Lifecycle: Start/Close or Context Manager](https://docs.crawl4ai.com/api/async-webcrawler/#2-lifecycle-startclose-or-context-manager)\n  * [2.1 Context Manager (Recommended)](https://docs.crawl4ai.com/api/async-webcrawler/#21-context-manager-recommended)\n  * [2.2 Manual Start & Close](https://docs.crawl4ai.com/api/async-webcrawler/#22-manual-start-close)\n  * [3. Primary Method: arun()](https://docs.crawl4ai.com/api/async-webcrawler/#3-primary-method-arun)\n  * [3.1 New Approach](https://docs.crawl4ai.com/api/async-webcrawler/#31-new-approach)\n  * [3.2 Legacy Parameters Still Accepted](https://docs.crawl4ai.com/api/async-webcrawler/#32-legacy-parameters-still-accepted)\n  * [4. Batch Processing: arun_many()](https://docs.crawl4ai.com/api/async-webcrawler/#4-batch-processing-arun_many)\n  * [4.1 Resource-Aware Crawling](https://docs.crawl4ai.com/api/async-webcrawler/#41-resource-aware-crawling)\n  * [4.2 Example Usage](https://docs.crawl4ai.com/api/async-webcrawler/#42-example-usage)\n  * [7. Best Practices & Migration Notes](https://docs.crawl4ai.com/api/async-webcrawler/#7-best-practices-migration-notes)\n  * [8. Summary](https://docs.crawl4ai.com/api/async-webcrawler/#8-summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/arun",
    "depth": 1,
    "title": "arun() - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "7835274e37d9f31ada0c41f04a619ded",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/arun/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * arun()\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [arun() Parameter Guide (New Approach)](https://docs.crawl4ai.com/api/arun/#arun-parameter-guide-new-approach)\n  * [1. Core Usage](https://docs.crawl4ai.com/api/arun/#1-core-usage)\n  * [2. Cache Control](https://docs.crawl4ai.com/api/arun/#2-cache-control)\n  * [3. Content Processing & Selection](https://docs.crawl4ai.com/api/arun/#3-content-processing-selection)\n  * [4. Page Navigation & Timing](https://docs.crawl4ai.com/api/arun/#4-page-navigation-timing)\n  * [5. Session Management](https://docs.crawl4ai.com/api/arun/#5-session-management)\n  * [6. Screenshot, PDF & Media Options](https://docs.crawl4ai.com/api/arun/#6-screenshot-pdf-media-options)\n  * [7. Extraction Strategy](https://docs.crawl4ai.com/api/arun/#7-extraction-strategy)\n  * [8. Comprehensive Example](https://docs.crawl4ai.com/api/arun/#8-comprehensive-example)\n  * [9. Best Practices](https://docs.crawl4ai.com/api/arun/#9-best-practices)\n  * [10. Conclusion](https://docs.crawl4ai.com/api/arun/#10-conclusion)\n\n\n#  `arun()` Parameter Guide (New Approach)\nIn Crawl4AIâ€™s **latest** configuration model, nearly all parameters that once went directly to `arun()` are now part of **`CrawlerRunConfig`**. When calling`arun()` , you provide:\n```\nawait crawler.arun(\n    url=\"https://example.com\",  \n    config=my_run_config\n)\nCopy\n```\n\nBelow is an organized look at the parameters that can go inside `CrawlerRunConfig`, divided by their functional areas. For **Browser** settings (e.g., `headless`, `browser_type`), see [BrowserConfig](https://docs.crawl4ai.com/api/parameters/).\n* * *\n## 1. Core Usage\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    run_config = CrawlerRunConfig(\n        verbose=True,            # Detailed logging\n        cache_mode=CacheMode.ENABLED,  # Use normal read/write cache\n        check_robots_txt=True,   # Respect robots.txt rules\n        # ...â€€other parameters\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n\n        # Check if blocked by robots.txt\n        if not result.success and result.status_code == 403:\n            print(f\"Error: {result.error_message}\")\nCopy\n```\n\n**Key Fields** : - `verbose=True` logs each crawl step. - `cache_mode` decides how to read/write the local crawl cache.\n* * *\n## 2. Cache Control\n**`cache_mode`**(default:`CacheMode.ENABLED`)  \nUse a built-in enum from `CacheMode`:\n  * `ENABLED`: Normal cachingâ€”reads if available, writes if missing.\n  * `DISABLED`: No cachingâ€”always refetch pages.\n  * `READ_ONLY`: Reads from cache only; no new writes.\n  * `WRITE_ONLY`: Writes to cache but doesnâ€™t read existing data.\n  * `BYPASS`: Skips reading cache for this crawl (though it might still write if set up that way).\n\n\n```\nrun_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS\n)\nCopy\n```\n\n**Additional flags** :\n  * `bypass_cache=True` acts like `CacheMode.BYPASS`.\n  * `disable_cache=True` acts like `CacheMode.DISABLED`.\n  * `no_cache_read=True` acts like `CacheMode.WRITE_ONLY`.\n  * `no_cache_write=True` acts like `CacheMode.READ_ONLY`.\n\n\n* * *\n## 3. Content Processing & Selection\n### 3.1 Text Processing\n```\nrun_config = CrawlerRunConfig(\n    word_count_threshold=10,   # Ignore text blocks <10 words\n    only_text=False,           # If True, tries to remove non-text elements\n    keep_data_attributes=False # Keep or discard data-* attributes\n)\nCopy\n```\n\n### 3.2 Content Selection\n```\nrun_config = CrawlerRunConfig(\n    css_selector=\".main-content\",  # Focus on .main-content region only\n    excluded_tags=[\"form\", \"nav\"], # Remove entire tag blocks\n    remove_forms=True,             # Specifically strip <form> elements\n    remove_overlay_elements=True,  # Attempt to remove modals/popups\n)\nCopy\n```\n\n### 3.3 Link Handling\n```\nrun_config = CrawlerRunConfig(\n    exclude_external_links=True,         # Remove external links from final content\n    exclude_social_media_links=True,     # Remove links to known social sites\n    exclude_domains=[\"ads.example.com\"], # Exclude links to these domains\n    exclude_social_media_domains=[\"facebook.com\",\"twitter.com\"], # Extend the default list\n)\nCopy\n```\n\n### 3.4 Media Filtering\n```\nrun_config = CrawlerRunConfig(\n    exclude_external_images=True  # Strip images from other domains\n)\nCopy\n```\n\n* * *\n## 4. Page Navigation & Timing\n### 4.1 Basic Browser Flow\n```\nrun_config = CrawlerRunConfig(\n    wait_for=\"css:.dynamic-content\", # Wait for .dynamic-content\n    delay_before_return_html=2.0,    # Wait 2s before capturing final HTML\n    page_timeout=60000,             # Navigation & script timeout (ms)\n)\nCopy\n```\n\n**Key Fields** :\n  * `wait_for`: \n  * `\"css:selector\"` or \n  * `\"js:() => boolean\"`  \ne.g. `js:() => document.querySelectorAll('.item').length > 10`.\n  * `mean_delay` & `max_range`: define random delays for `arun_many()` calls. \n  * `semaphore_count`: concurrency limit when crawling multiple URLs.\n\n\n### 4.2 JavaScript Execution\n```\nrun_config = CrawlerRunConfig(\n    js_code=[\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        \"document.querySelector('.load-more')?.click();\"\n    ],\n    js_only=False\n)\nCopy\n```\n\n  * `js_code` can be a single string or a list of strings. \n  * `js_only=True` means â€œIâ€™m continuing in the same session with new JS steps, no new full navigation.â€\n\n\n### 4.3 Anti-Bot\n```\nrun_config = CrawlerRunConfig(\n    magic=True,\n    simulate_user=True,\n    override_navigator=True\n)\nCopy\n```\n\n- `magic=True` tries multiple stealth features. - `simulate_user=True` mimics mouse movements or random delays. - `override_navigator=True` fakes some navigator properties (like user agent checks).\n* * *\n## 5. Session Management\n**`session_id`**:\n```\nrun_config = CrawlerRunConfig(\n    session_id=\"my_session123\"\n)\nCopy\n```\n\nIf re-used in subsequent `arun()` calls, the same tab/page context is continued (helpful for multi-step tasks or stateful browsing).\n* * *\n## 6. Screenshot, PDF & Media Options\n```\nrun_config = CrawlerRunConfig(\n    screenshot=True,             # Grab a screenshot as base64\n    screenshot_wait_for=1.0,     # Wait 1s before capturing\n    pdf=True,                    # Also produce a PDF\n    image_description_min_word_threshold=5,  # If analyzing alt text\n    image_score_threshold=3,                # Filter out low-score images\n)\nCopy\n```\n\n**Where they appear** : - `result.screenshot` â†’ Base64 screenshot string. - `result.pdf` â†’ Byte array with PDF data.\n* * *\n## 7. Extraction Strategy\n**For advanced data extraction** (CSS/LLM-based), set `extraction_strategy`:\n```\nrun_config = CrawlerRunConfig(\n    extraction_strategy=my_css_or_llm_strategy\n)\nCopy\n```\n\nThe extracted data will appear in `result.extracted_content`.\n* * *\n## 8. Comprehensive Example\nBelow is a snippet combining many parameters:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    # Example schema\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\",  \"selector\": \"a\",  \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    run_config = CrawlerRunConfig(\n        # Core\n        verbose=True,\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,   # Respect robots.txt rules\n\n        # Content\n        word_count_threshold=10,\n        css_selector=\"main.content\",\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n\n        # Page & JS\n        js_code=\"document.querySelector('.show-more')?.click();\",\n        wait_for=\"css:.loaded-block\",\n        page_timeout=30000,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n\n        # Session\n        session_id=\"persistent_session\",\n\n        # Media\n        screenshot=True,\n        pdf=True,\n\n        # Anti-bot\n        simulate_user=True,\n        magic=True,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/posts\", config=run_config)\n        if result.success:\n            print(\"HTML length:\", len(result.cleaned_html))\n            print(\"Extraction JSON:\", result.extracted_content)\n            if result.screenshot:\n                print(\"Screenshot length:\", len(result.screenshot))\n            if result.pdf:\n                print(\"PDF bytes length:\", len(result.pdf))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**What we covered** :\n1. **Crawling** the main content region, ignoring external links. 2. Running **JavaScript** to click â€œ.show-moreâ€. 3. **Waiting** for â€œ.loaded-blockâ€ to appear. 4. Generating a **screenshot** & **PDF** of the final page. 5. Extracting repeated â€œarticle.postâ€ elements with a **CSS-based** extraction strategy.\n* * *\n## 9. Best Practices\n1. **Use`BrowserConfig` for global browser** settings (headless, user agent). 2. **Use`CrawlerRunConfig`** to handle the **specific** crawl needs: content filtering, caching, JS, screenshot, extraction, etc. 3. Keep your **parameters consistent** in run configsâ€”especially if youâ€™re part of a large codebase with multiple crawls. 4. **Limit** large concurrency (`semaphore_count`) if the site or your system canâ€™t handle it. 5. For dynamic pages, set `js_code` or `scan_full_page` so you load all content.\n* * *\n## 10. Conclusion\nAll parameters that used to be direct arguments to `arun()` now belong in **`CrawlerRunConfig`**. This approach:\n  * Makes code **clearer** and **more maintainable**. \n  * Minimizes confusion about which arguments affect global vs. per-crawl behavior. \n  * Allows you to create **reusable** config objects for different pages or tasks.\n\n\nFor a **full** reference, check out the [CrawlerRunConfig Docs](https://docs.crawl4ai.com/api/parameters/). \nHappy crawling with your **structured, flexible** config approach!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/arun/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/arun/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/arun/)\n\n\nESC to close\n#### On this page\n  * [1. Core Usage](https://docs.crawl4ai.com/api/arun/#1-core-usage)\n  * [2. Cache Control](https://docs.crawl4ai.com/api/arun/#2-cache-control)\n  * [3. Content Processing & Selection](https://docs.crawl4ai.com/api/arun/#3-content-processing-selection)\n  * [3.1 Text Processing](https://docs.crawl4ai.com/api/arun/#31-text-processing)\n  * [3.2 Content Selection](https://docs.crawl4ai.com/api/arun/#32-content-selection)\n  * [3.3 Link Handling](https://docs.crawl4ai.com/api/arun/#33-link-handling)\n  * [3.4 Media Filtering](https://docs.crawl4ai.com/api/arun/#34-media-filtering)\n  * [4. Page Navigation & Timing](https://docs.crawl4ai.com/api/arun/#4-page-navigation-timing)\n  * [4.1 Basic Browser Flow](https://docs.crawl4ai.com/api/arun/#41-basic-browser-flow)\n  * [4.2 JavaScript Execution](https://docs.crawl4ai.com/api/arun/#42-javascript-execution)\n  * [4.3 Anti-Bot](https://docs.crawl4ai.com/api/arun/#43-anti-bot)\n  * [5. Session Management](https://docs.crawl4ai.com/api/arun/#5-session-management)\n  * [6. Screenshot, PDF & Media Options](https://docs.crawl4ai.com/api/arun/#6-screenshot-pdf-media-options)\n  * [7. Extraction Strategy](https://docs.crawl4ai.com/api/arun/#7-extraction-strategy)\n  * [8. Comprehensive Example](https://docs.crawl4ai.com/api/arun/#8-comprehensive-example)\n  * [9. Best Practices](https://docs.crawl4ai.com/api/arun/#9-best-practices)\n  * [10. Conclusion](https://docs.crawl4ai.com/api/arun/#10-conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/arun_many",
    "depth": 1,
    "title": "arun_many() - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "a32ea3634c8a5e451228ad6db861c409",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/arun_many/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * arun_many()\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [arun_many(...) Reference](https://docs.crawl4ai.com/api/arun_many/#arun_many-reference)\n  * [Function Signature](https://docs.crawl4ai.com/api/arun_many/#function-signature)\n  * [Differences from arun()](https://docs.crawl4ai.com/api/arun_many/#differences-from-arun)\n  * [Dispatcher Reference](https://docs.crawl4ai.com/api/arun_many/#dispatcher-reference)\n  * [Common Pitfalls](https://docs.crawl4ai.com/api/arun_many/#common-pitfalls)\n  * [Conclusion](https://docs.crawl4ai.com/api/arun_many/#conclusion)\n\n\n#  `arun_many(...)` Reference\n> **Note** : This function is very similar to [`arun()`](https://docs.crawl4ai.com/api/arun/) but focused on **concurrent** or **batch** crawling. If youâ€™re unfamiliar with `arun()` usage, please read that doc first, then review this for differences.\n## Function Signature\n```\nasync def arun_many(\n    urls: Union[List[str], List[Any]],\n    config: Optional[Union[CrawlerRunConfig, List[CrawlerRunConfig]]] = None,\n    dispatcher: Optional[BaseDispatcher] = None,\n    ...\n) -> Union[List[CrawlResult], AsyncGenerator[CrawlResult, None]]:\n    \"\"\"\n    Crawl multiple URLs concurrently or in batches.\n\n    :param urls: A list of URLs (or tasks) to crawl.\n    :param config: (Optional) Either:\n        - A single `CrawlerRunConfig` applying to all URLs\n        - A list of `CrawlerRunConfig` objects with url_matcher patterns\n    :param dispatcher: (Optional) A concurrency controller (e.g.â€€MemoryAdaptiveDispatcher).\n    ...\n    :return: Either a list of `CrawlResult` objects, or an async generator if streaming is enabled.\n    \"\"\"\nCopy\n```\n\n## Differences from `arun()`\n1. **Multiple URLs** : \n  * Instead of crawling a single URL, you pass a list of them (strings or tasks). \n  * The function returns either a **list** of `CrawlResult` or an **async generator** if streaming is enabled.\n\n\n2. **Concurrency & Dispatchers**: \n  * **`dispatcher`**param allows advanced concurrency control.\n  * If omitted, a default dispatcher (like `MemoryAdaptiveDispatcher`) is used internally. \n  * Dispatchers handle concurrency, rate limiting, and memory-based adaptive throttling (see [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)).\n\n\n3. **Streaming Support** : \n  * Enable streaming by setting `stream=True` in your `CrawlerRunConfig`.\n  * When streaming, use `async for` to process results as they become available.\n  * Ideal for processing large numbers of URLs without waiting for all to complete.\n\n\n4. **Parallel** Execution**: \n  * `arun_many()` can run multiple requests concurrently under the hood. \n  * Each `CrawlResult` might also include a **`dispatch_result`**with concurrency details (like memory usage, start/end times).\n\n\n### Basic Example (Batch Mode)\n```\n# Minimal usage: The default dispatcher will be used\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\"],\n    config=CrawlerRunConfig(stream=False)  # Default behavior\n)\n\nfor res in results:\n    if res.success:\n        print(res.url, \"crawled OK!\")\n    else:\n        print(\"Failed:\", res.url, \"-\", res.error_message)\nCopy\n```\n\n### Streaming Example\n```\nconfig = CrawlerRunConfig(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\n# Process results as they complete\nasync for result in await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=config\n):\n    if result.success:\n        print(f\"Just completed: {result.url}\")\n        # Process each result immediately\n        process_result(result)\nCopy\n```\n\n### With a Custom Dispatcher\n```\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=70.0,\n    max_session_permit=10\n)\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=my_run_config,\n    dispatcher=dispatcher\n)\nCopy\n```\n\n### URL-Specific Configurations\nInstead of using one config for all URLs, provide a list of configs with `url_matcher` patterns:\n```\nfrom crawl4ai import CrawlerRunConfig, MatchMode\nfrom crawl4ai.processors.pdf import PDFContentScrapingStrategy\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\n# PDF files - specialized extraction\npdf_config = CrawlerRunConfig(\n    url_matcher=\"*.pdf\",\n    scraping_strategy=PDFContentScrapingStrategy()\n)\n\n# Blog/article pages - content filtering\nblog_config = CrawlerRunConfig(\n    url_matcher=[\"*/blog/*\", \"*/article/*\", \"*python.org*\"],\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.48)\n    )\n)\n\n# Dynamic pages - JavaScript execution\ngithub_config = CrawlerRunConfig(\n    url_matcher=lambda url: 'github.com' in url,\n    js_code=\"window.scrollTo(0, 500);\"\n)\n\n# API endpoints - JSON extraction\napi_config = CrawlerRunConfig(\n    url_matcher=lambda url: 'api' in url or url.endswith('.json'),\n    # Custome settings for JSON extraction\n)\n\n# Default fallback config\ndefault_config = CrawlerRunConfig()  # No url_matcher means it never matches except as fallback\n\n# Pass the list of configs - first match wins!\nresults = await crawler.arun_many(\n    urls=[\n        \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\",  # â†’ pdf_config\n        \"https://blog.python.org/\",  # â†’ blog_config\n        \"https://github.com/microsoft/playwright\",  # â†’ github_config\n        \"https://httpbin.org/json\",  # â†’ api_config\n        \"https://example.com/\"  # â†’ default_config\n    ],\n    config=[pdf_config, blog_config, github_config, api_config, default_config]\n)\nCopy\n```\n\n**URL Matching Features** : - **String patterns** : `\"*.pdf\"`, `\"*/blog/*\"`, `\"*python.org*\"` - **Function matchers** : `lambda url: 'api' in url` - **Mixed patterns** : Combine strings and functions with `MatchMode.OR` or `MatchMode.AND` - **First match wins** : Configs are evaluated in order\n**Key Points** : - Each URL is processed by the same or separate sessions, depending on the dispatcherâ€™s strategy. - `dispatch_result` in each `CrawlResult` (if using concurrency) can hold memory and timing info. - If you need to handle authentication or session IDs, pass them in each individual task or within your run config. - **Important** : Always include a default config (without `url_matcher`) as the last item if you want to handle all URLs. Otherwise, unmatched URLs will fail.\n### Return Value\nEither a **list** of [`CrawlResult`](https://docs.crawl4ai.com/api/crawl-result/) objects, or an **async generator** if streaming is enabled. You can iterate to check `result.success` or read each itemâ€™s `extracted_content`, `markdown`, or `dispatch_result`.\n* * *\n## Dispatcher Reference\n  * **`MemoryAdaptiveDispatcher`**: Dynamically manages concurrency based on system memory usage.\n  * **`SemaphoreDispatcher`**: Fixed concurrency limit, simpler but less adaptive.\n\n\nFor advanced usage or custom settings, see [Multi-URL Crawling with Dispatchers](https://docs.crawl4ai.com/advanced/multi-url-crawling/).\n* * *\n## Common Pitfalls\n1. **Large Lists** : If you pass thousands of URLs, be mindful of memory or rate-limits. A dispatcher can help. \n2. **Session Reuse** : If you need specialized logins or persistent contexts, ensure your dispatcher or tasks handle sessions accordingly. \n3. **Error Handling** : Each `CrawlResult` might fail for different reasonsâ€”always check `result.success` or the `error_message` before proceeding.\n* * *\n## Conclusion\nUse `arun_many()` when you want to **crawl multiple URLs** simultaneously or in controlled parallel tasks. If you need advanced concurrency features (like memory-based adaptive throttling or complex rate-limiting), provide a **dispatcher**. Each result is a standard `CrawlResult`, possibly augmented with concurrency stats (`dispatch_result`) for deeper inspection. For more details on concurrency logic and dispatchers, see the [Advanced Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/) docs.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/arun_many/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/arun_many/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/arun_many/)\n\n\nESC to close\n#### On this page\n  * [Function Signature](https://docs.crawl4ai.com/api/arun_many/#function-signature)\n  * [Differences from arun()](https://docs.crawl4ai.com/api/arun_many/#differences-from-arun)\n  * [Basic Example (Batch Mode)](https://docs.crawl4ai.com/api/arun_many/#basic-example-batch-mode)\n  * [Streaming Example](https://docs.crawl4ai.com/api/arun_many/#streaming-example)\n  * [With a Custom Dispatcher](https://docs.crawl4ai.com/api/arun_many/#with-a-custom-dispatcher)\n  * [URL-Specific Configurations](https://docs.crawl4ai.com/api/arun_many/#url-specific-configurations)\n  * [Return Value](https://docs.crawl4ai.com/api/arun_many/#return-value)\n  * [Dispatcher Reference](https://docs.crawl4ai.com/api/arun_many/#dispatcher-reference)\n  * [Common Pitfalls](https://docs.crawl4ai.com/api/arun_many/#common-pitfalls)\n  * [Conclusion](https://docs.crawl4ai.com/api/arun_many/#conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/ssl-certificate",
    "depth": 1,
    "title": "SSL Certificate - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "2f265102ef1145825b2026698f082ef5",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * SSL Certificate\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [SSLCertificate Reference](https://docs.crawl4ai.com/advanced/ssl-certificate/#sslcertificate-reference)\n  * [1. Overview](https://docs.crawl4ai.com/advanced/ssl-certificate/#1-overview)\n  * [2. Construction & Fetching](https://docs.crawl4ai.com/advanced/ssl-certificate/#2-construction-fetching)\n  * [3. Common Properties](https://docs.crawl4ai.com/advanced/ssl-certificate/#3-common-properties)\n  * [4. Export Methods](https://docs.crawl4ai.com/advanced/ssl-certificate/#4-export-methods)\n  * [5. Example Usage in Crawl4AI](https://docs.crawl4ai.com/advanced/ssl-certificate/#5-example-usage-in-crawl4ai)\n  * [6. Notes & Best Practices](https://docs.crawl4ai.com/advanced/ssl-certificate/#6-notes-best-practices)\n\n\n#  `SSLCertificate` Reference\nThe **`SSLCertificate`**class encapsulates an SSL certificateâ€™s data and allows exporting it in various formats (PEM, DER, JSON, or text). Itâ€™s used within**Crawl4AI** whenever you set **`fetch_ssl_certificate=True`**in your**`CrawlerRunConfig`**.\n## 1. Overview\n**Location** : `crawl4ai/ssl_certificate.py`\n```\nclass SSLCertificate:\n    \"\"\"\n    Represents an SSL certificate with methods to export in various formats.\n\n    Main Methods:\n    - from_url(url, timeout=10)\n    - from_file(file_path)\n    - from_binary(binary_data)\n    - to_json(filepath=None)\n    - to_pem(filepath=None)\n    - to_der(filepath=None)\n    ...\n\n    Common Properties:\n    - issuer\n    - subject\n    - valid_from\n    - valid_until\n    - fingerprint\n    \"\"\"\nCopy\n```\n\n### Typical Use Case\n  1. You **enable** certificate fetching in your crawl by: \n```\nCrawlerRunConfig(fetch_ssl_certificate=True, ...)\nCopy\n```\n\n  2. After `arun()`, if `result.ssl_certificate` is present, itâ€™s an instance of **`SSLCertificate`**.\n  3. You can **read** basic properties (issuer, subject, validity) or **export** them in multiple formats.\n\n\n* * *\n## 2. Construction & Fetching\n### 2.1 **`from_url(url, timeout=10)`**\nManually load an SSL certificate from a given URL (port 443). Typically used internally, but you can call it directly if you want:\n```\ncert = SSLCertificate.from_url(\"https://example.com\")\nif cert:\n    print(\"Fingerprint:\", cert.fingerprint)\nCopy\n```\n\n### 2.2 **`from_file(file_path)`**\nLoad from a file containing certificate data in ASN.1 or DER. Rarely needed unless you have local cert files:\n```\ncert = SSLCertificate.from_file(\"/path/to/cert.der\")\nCopy\n```\n\n### 2.3 **`from_binary(binary_data)`**\nInitialize from raw binary. E.g., if you captured it from a socket or another source:\n```\ncert = SSLCertificate.from_binary(raw_bytes)\nCopy\n```\n\n* * *\n## 3. Common Properties\nAfter obtaining a **`SSLCertificate`**instance (e.g.`result.ssl_certificate` from a crawl), you can read:\n1. **`issuer`**_(dict)_  \n- E.g. `{\"CN\": \"My Root CA\", \"O\": \"...\"}` 2. **`subject`**_(dict)_  \n- E.g. `{\"CN\": \"example.com\", \"O\": \"ExampleOrg\"}` 3. **`valid_from`**_(str)_  \n- NotBefore date/time. Often in ASN.1/UTC format. 4. **`valid_until`**_(str)_  \n- NotAfter date/time. 5. **`fingerprint`**_(str)_  \n- The SHA-256 digest (lowercase hex).  \n- E.g. `\"d14d2e...\"`\n* * *\n## 4. Export Methods\nOnce you have a **`SSLCertificate`**object, you can**export** or **inspect** it:\n### 4.1 **`to_json(filepath=None)`â†’`Optional[str]`**\n  * Returns a JSON string containing the parsed certificate fields. \n  * If `filepath` is provided, saves it to disk instead, returning `None`.\n\n\n**Usage** : \n```\njson_data = cert.to_json()  # returns JSON string\ncert.to_json(\"certificate.json\")  # writes file, returns None\nCopy\n```\n\n### 4.2 **`to_pem(filepath=None)`â†’`Optional[str]`**\n  * Returns a PEM-encoded string (common for web servers). \n  * If `filepath` is provided, saves it to disk instead.\n\n\n```\npem_str = cert.to_pem()              # in-memory PEM string\ncert.to_pem(\"/path/to/cert.pem\")     # saved to file\nCopy\n```\n\n### 4.3 **`to_der(filepath=None)`â†’`Optional[bytes]`**\n  * Returns the original DER (binary ASN.1) bytes. \n  * If `filepath` is specified, writes the bytes there instead.\n\n\n```\nder_bytes = cert.to_der()\ncert.to_der(\"certificate.der\")\nCopy\n```\n\n### 4.4 (Optional) **`export_as_text()`**\n  * If you see a method like `export_as_text()`, it typically returns an OpenSSL-style textual representation. \n  * Not always needed, but can help for debugging or manual inspection.\n\n\n* * *\n## 5. Example Usage in Crawl4AI\nBelow is a minimal sample showing how the crawler obtains an SSL cert from a site, then reads or exports it. The code snippet:\n```\nimport asyncio\nimport os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = \"tmp\"\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            # 1. Basic Info\n            print(\"Issuer CN:\", cert.issuer.get(\"CN\", \"\"))\n            print(\"Valid until:\", cert.valid_until)\n            print(\"Fingerprint:\", cert.fingerprint)\n\n            # 2. Export\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## 6. Notes & Best Practices\n1. **Timeout** : `SSLCertificate.from_url` internally uses a default **10s** socket connect and wraps SSL.  \n2. **Binary Form** : The certificate is loaded in ASN.1 (DER) form, then re-parsed by `OpenSSL.crypto`.  \n3. **Validation** : This does **not** validate the certificate chain or trust store. It only fetches and parses.  \n4. **Integration** : Within Crawl4AI, you typically just set `fetch_ssl_certificate=True` in `CrawlerRunConfig`; the final resultâ€™s `ssl_certificate` is automatically built.  \n5. **Export** : If you need to store or analyze a cert, the `to_json` and `to_pem` are quite universal.\n* * *\n### Summary\n  * **`SSLCertificate`**is a convenience class for capturing and exporting the**TLS certificate** from your crawled site(s). \n  * Common usage is in the **`CrawlResult.ssl_certificate`**field, accessible after setting`fetch_ssl_certificate=True`. \n  * Offers quick access to essential certificate details (`issuer`, `subject`, `fingerprint`) and is easy to export (PEM, DER, JSON) for further analysis or server usage.\n\n\nUse it whenever you need **insight** into a siteâ€™s certificate or require some form of cryptographic or compliance check.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n\n\nESC to close\n#### On this page\n  * [1. Overview](https://docs.crawl4ai.com/advanced/ssl-certificate/#1-overview)\n  * [Typical Use Case](https://docs.crawl4ai.com/advanced/ssl-certificate/#typical-use-case)\n  * [2. Construction & Fetching](https://docs.crawl4ai.com/advanced/ssl-certificate/#2-construction-fetching)\n  * [2.1 from_url(url, timeout=10)](https://docs.crawl4ai.com/advanced/ssl-certificate/#21-from_urlurl-timeout10)\n  * [2.2 from_file(file_path)](https://docs.crawl4ai.com/advanced/ssl-certificate/#22-from_filefile_path)\n  * [2.3 from_binary(binary_data)](https://docs.crawl4ai.com/advanced/ssl-certificate/#23-from_binarybinary_data)\n  * [3. Common Properties](https://docs.crawl4ai.com/advanced/ssl-certificate/#3-common-properties)\n  * [4. Export Methods](https://docs.crawl4ai.com/advanced/ssl-certificate/#4-export-methods)\n  * [4.1 to_json(filepath=None) â†’ Optional[str]](https://docs.crawl4ai.com/advanced/ssl-certificate/#41-to_jsonfilepathnone-optionalstr)\n  * [4.2 to_pem(filepath=None) â†’ Optional[str]](https://docs.crawl4ai.com/advanced/ssl-certificate/#42-to_pemfilepathnone-optionalstr)\n  * [4.3 to_der(filepath=None) â†’ Optional[bytes]](https://docs.crawl4ai.com/advanced/ssl-certificate/#43-to_derfilepathnone-optionalbytes)\n  * [4.4 (Optional) export_as_text()](https://docs.crawl4ai.com/advanced/ssl-certificate/#44-optional-export_as_text)\n  * [5. Example Usage in Crawl4AI](https://docs.crawl4ai.com/advanced/ssl-certificate/#5-example-usage-in-crawl4ai)\n  * [6. Notes & Best Practices](https://docs.crawl4ai.com/advanced/ssl-certificate/#6-notes-best-practices)\n  * [Summary](https://docs.crawl4ai.com/advanced/ssl-certificate/#summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/advanced/virtual-scroll",
    "depth": 1,
    "title": "Virtual Scroll - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "0cdde422d4c1fe0e132cbb89381febac",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * Virtual Scroll\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/#virtual-scroll)\n  * [Understanding Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/#understanding-virtual-scroll)\n  * [Basic Usage](https://docs.crawl4ai.com/advanced/virtual-scroll/#basic-usage)\n  * [Configuration Parameters](https://docs.crawl4ai.com/advanced/virtual-scroll/#configuration-parameters)\n  * [Real-World Examples](https://docs.crawl4ai.com/advanced/virtual-scroll/#real-world-examples)\n  * [Virtual Scroll vs scan_full_page](https://docs.crawl4ai.com/advanced/virtual-scroll/#virtual-scroll-vs-scan_full_page)\n  * [Combining with Extraction](https://docs.crawl4ai.com/advanced/virtual-scroll/#combining-with-extraction)\n  * [Performance Tips](https://docs.crawl4ai.com/advanced/virtual-scroll/#performance-tips)\n  * [How It Works Internally](https://docs.crawl4ai.com/advanced/virtual-scroll/#how-it-works-internally)\n  * [Error Handling](https://docs.crawl4ai.com/advanced/virtual-scroll/#error-handling)\n  * [Complete Example](https://docs.crawl4ai.com/advanced/virtual-scroll/#complete-example)\n\n\n# Virtual Scroll\nModern websites increasingly use **virtual scrolling** (also called windowed rendering or viewport rendering) to handle large datasets efficiently. This technique only renders visible items in the DOM, replacing content as users scroll. Popular examples include Twitter's timeline, Instagram's feed, and many data tables.\nCrawl4AI's Virtual Scroll feature automatically detects and handles these scenarios, ensuring you capture **all content** , not just what's initially visible.\n## Understanding Virtual Scroll\n### The Problem\nTraditional infinite scroll **appends** new content to existing content. Virtual scroll **replaces** content to maintain performance:\n```\nTraditional Scroll:          Virtual Scroll:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Item 1      â”‚             â”‚ Item 11     â”‚  <- Items 1-10 removed\nâ”‚ Item 2      â”‚             â”‚ Item 12     â”‚  <- Only visible items\nâ”‚ ...         â”‚             â”‚ Item 13     â”‚     in DOM\nâ”‚ Item 10     â”‚             â”‚ Item 14     â”‚\nâ”‚ Item 11 NEW â”‚             â”‚ Item 15     â”‚\nâ”‚ Item 12 NEW â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             \nDOM keeps growing           DOM size stays constant\nCopy\n```\n\nWithout proper handling, crawlers only capture the currently visible items, missing the rest of the content.\n### Three Scrolling Scenarios\nCrawl4AI's Virtual Scroll detects and handles three scenarios:\n  1. **No Change** - Content doesn't update on scroll (static page or end reached)\n  2. **Content Appended** - New items added to existing ones (traditional infinite scroll) \n  3. **Content Replaced** - Items replaced with new ones (true virtual scroll)\n\n\nOnly scenario 3 requires special handling, which Virtual Scroll automates.\n## Basic Usage\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, VirtualScrollConfig\n\n# Configure virtual scroll\nvirtual_config = VirtualScrollConfig(\n    container_selector=\"#feed\",      # CSS selector for scrollable container\n    scroll_count=20,                 # Number of scrolls to perform\n    scroll_by=\"container_height\",    # How much to scroll each time\n    wait_after_scroll=0.5           # Wait time (seconds) after each scroll\n)\n\n# Use in crawler configuration\nconfig = CrawlerRunConfig(\n    virtual_scroll_config=virtual_config\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=\"https://example.com\", config=config)\n    # result.html contains ALL items from the virtual scroll\nCopy\n```\n\n## Configuration Parameters\n### VirtualScrollConfig\nParameter | Type | Default | Description  \n---|---|---|---  \n`container_selector` | `str` | Required | CSS selector for the scrollable container  \n`scroll_count` | `int` | `10` | Maximum number of scrolls to perform  \n`scroll_by` |  `str` or `int` | `\"container_height\"` | Scroll amount per step  \n`wait_after_scroll` | `float` | `0.5` | Seconds to wait after each scroll  \n### Scroll By Options\n  * `\"container_height\"` - Scroll by the container's visible height\n  * `\"page_height\"` - Scroll by the viewport height\n  * `500` (integer) - Scroll by exact pixel amount\n\n\n## Real-World Examples\n### Twitter-like Timeline\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, VirtualScrollConfig, BrowserConfig\n\nasync def crawl_twitter_timeline():\n    # Twitter replaces tweets as you scroll\n    virtual_config = VirtualScrollConfig(\n        container_selector=\"[data-testid='primaryColumn']\",\n        scroll_count=30,\n        scroll_by=\"container_height\",\n        wait_after_scroll=1.0  # Twitter needs time to load\n    )\n\n    browser_config = BrowserConfig(headless=True)  # Set to False to watch it work\n    config = CrawlerRunConfig(\n        virtual_scroll_config=virtual_config\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://twitter.com/search?q=AI\",\n            config=config\n        )\n\n        # Extract tweet count\n        import re\n        tweets = re.findall(r'data-testid=\"tweet\"', result.html)\n        print(f\"Captured {len(tweets)} tweets\")\nCopy\n```\n\n### Instagram Grid\n```\nasync def crawl_instagram_grid():\n    # Instagram uses virtualized grid for performance\n    virtual_config = VirtualScrollConfig(\n        container_selector=\"article\",  # Main feed container\n        scroll_count=50,               # More scrolls for grid layout\n        scroll_by=800,                 # Fixed pixel scrolling\n        wait_after_scroll=0.8\n    )\n\n    config = CrawlerRunConfig(\n        virtual_scroll_config=virtual_config,\n        screenshot=True  # Capture final state\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.instagram.com/explore/tags/photography/\",\n            config=config\n        )\n\n        # Count posts\n        posts = result.html.count('class=\"post\"')\n        print(f\"Captured {posts} posts from virtualized grid\")\nCopy\n```\n\n### Mixed Content (News Feed)\nSome sites mix static and virtualized content:\n```\nasync def crawl_mixed_feed():\n    # Featured articles stay, regular articles virtualize\n    virtual_config = VirtualScrollConfig(\n        container_selector=\".main-feed\",\n        scroll_count=25,\n        scroll_by=\"container_height\",\n        wait_after_scroll=0.5\n    )\n\n    config = CrawlerRunConfig(\n        virtual_scroll_config=virtual_config\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.example.com\",\n            config=config\n        )\n\n        # Featured articles remain throughout\n        featured = result.html.count('class=\"featured-article\"')\n        regular = result.html.count('class=\"regular-article\"')\n\n        print(f\"Featured (static): {featured}\")\n        print(f\"Regular (virtualized): {regular}\")\nCopy\n```\n\n## Virtual Scroll vs scan_full_page\nBoth features handle dynamic content, but serve different purposes:\nFeature | Virtual Scroll | scan_full_page  \n---|---|---  \n**Purpose** | Capture content that's replaced during scroll | Load content that's appended during scroll  \n**Use Case** | Twitter, Instagram, virtual tables | Traditional infinite scroll, lazy-loaded images  \n**DOM Behavior** | Replaces elements | Adds elements  \n**Memory Usage** | Efficient (merges content) | Can grow large  \n**Configuration** | Requires container selector | Works on full page  \n### When to Use Which?\nUse **Virtual Scroll** when: - Content disappears as you scroll (Twitter timeline) - DOM element count stays relatively constant - You need ALL items from a virtualized list - Container-based scrolling (not full page)\nUse **scan_full_page** when: - Content accumulates as you scroll - Images load lazily - Simple \"load more\" behavior - Full page scrolling\n## Combining with Extraction\nVirtual Scroll works seamlessly with extraction strategies:\n```\nfrom crawl4ai import LLMExtractionStrategy, LLMConfig\n\n# Define extraction schema\nschema = {\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\", \n        \"properties\": {\n            \"author\": {\"type\": \"string\"},\n            \"content\": {\"type\": \"string\"},\n            \"timestamp\": {\"type\": \"string\"}\n        }\n    }\n}\n\n# Configure both virtual scroll and extraction\nconfig = CrawlerRunConfig(\n    virtual_scroll_config=VirtualScrollConfig(\n        container_selector=\"#timeline\",\n        scroll_count=20\n    ),\n    extraction_strategy=LLMExtractionStrategy(\n        llm_config=LLMConfig(provider=\"openai/gpt-4o-mini\"),\n        schema=schema\n    )\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=\"...\", config=config)\n\n    # Extracted data from ALL scrolled content\n    import json\n    posts = json.loads(result.extracted_content)\n    print(f\"Extracted {len(posts)} posts from virtual scroll\")\nCopy\n```\n\n## Performance Tips\n  1. **Container Selection** : Be specific with selectors. Using the correct container improves performance.\n  2. **Scroll Count** : Start conservative and increase as needed: \n```\n# Start with fewer scrolls\nvirtual_config = VirtualScrollConfig(\n    container_selector=\"#feed\",\n    scroll_count=10  # Test with 10, increase if needed\n)\nCopy\n```\n\n  3. **Wait Times** : Adjust based on site speed: \n```\n# Fast sites\nwait_after_scroll=0.2\n\n# Slower sites or heavy content\nwait_after_scroll=1.5\nCopy\n```\n\n  4. **Debug Mode** : Set `headless=False` to watch scrolling: \n```\nbrowser_config = BrowserConfig(headless=False)\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    # Watch the scrolling happen\nCopy\n```\n\n\n\n## How It Works Internally\n  1. **Detection Phase** : Scrolls and compares HTML to detect behavior\n  2. **Capture Phase** : For replaced content, stores HTML chunks at each position\n  3. **Merge Phase** : Combines all chunks, removing duplicates based on text content\n  4. **Result** : Complete HTML with all unique items\n\n\nThe deduplication uses normalized text (lowercase, no spaces/symbols) to ensure accurate merging without false positives.\n## Error Handling\nVirtual Scroll handles errors gracefully:\n```\n# If container not found or scrolling fails\nresult = await crawler.arun(url=\"...\", config=config)\n\nif result.success:\n    # Virtual scroll worked or wasn't needed\n    print(f\"Captured {len(result.html)} characters\")\nelse:\n    # Crawl failed entirely\n    print(f\"Error: {result.error_message}\")\nCopy\n```\n\nIf the container isn't found, crawling continues normally without virtual scroll.\n## Complete Example\nSee our [comprehensive example](https://docs.crawl4ai.com/docs/examples/virtual_scroll_example.py) that demonstrates: - Twitter-like feeds - Instagram grids  \n- Traditional infinite scroll - Mixed content scenarios - Performance comparisons\n```\n# Run the examples\ncd docs/examples\npython virtual_scroll_example.py\nCopy\n```\n\nThe example includes a local test server with different scrolling behaviors for experimentation.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n\n\nESC to close\n#### On this page\n  * [Understanding Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/#understanding-virtual-scroll)\n  * [The Problem](https://docs.crawl4ai.com/advanced/virtual-scroll/#the-problem)\n  * [Three Scrolling Scenarios](https://docs.crawl4ai.com/advanced/virtual-scroll/#three-scrolling-scenarios)\n  * [Basic Usage](https://docs.crawl4ai.com/advanced/virtual-scroll/#basic-usage)\n  * [Configuration Parameters](https://docs.crawl4ai.com/advanced/virtual-scroll/#configuration-parameters)\n  * [VirtualScrollConfig](https://docs.crawl4ai.com/advanced/virtual-scroll/#virtualscrollconfig)\n  * [Scroll By Options](https://docs.crawl4ai.com/advanced/virtual-scroll/#scroll-by-options)\n  * [Real-World Examples](https://docs.crawl4ai.com/advanced/virtual-scroll/#real-world-examples)\n  * [Twitter-like Timeline](https://docs.crawl4ai.com/advanced/virtual-scroll/#twitter-like-timeline)\n  * [Instagram Grid](https://docs.crawl4ai.com/advanced/virtual-scroll/#instagram-grid)\n  * [Mixed Content (News Feed)](https://docs.crawl4ai.com/advanced/virtual-scroll/#mixed-content-news-feed)\n  * [Virtual Scroll vs scan_full_page](https://docs.crawl4ai.com/advanced/virtual-scroll/#virtual-scroll-vs-scan_full_page)\n  * [When to Use Which?](https://docs.crawl4ai.com/advanced/virtual-scroll/#when-to-use-which)\n  * [Combining with Extraction](https://docs.crawl4ai.com/advanced/virtual-scroll/#combining-with-extraction)\n  * [Performance Tips](https://docs.crawl4ai.com/advanced/virtual-scroll/#performance-tips)\n  * [How It Works Internally](https://docs.crawl4ai.com/advanced/virtual-scroll/#how-it-works-internally)\n  * [Error Handling](https://docs.crawl4ai.com/advanced/virtual-scroll/#error-handling)\n  * [Complete Example](https://docs.crawl4ai.com/advanced/virtual-scroll/#complete-example)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/c4a-script-reference",
    "depth": 1,
    "title": "C4A-Script Reference - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "759793531ce0c2261f6ad0de83ca6000",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/c4a-script-reference/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * C4A-Script Reference\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [C4A-Script API Reference](https://docs.crawl4ai.com/api/c4a-script-reference/#c4a-script-api-reference)\n  * [Command Categories](https://docs.crawl4ai.com/api/c4a-script-reference/#command-categories)\n  * [Error Handling Best Practices](https://docs.crawl4ai.com/api/c4a-script-reference/#error-handling-best-practices)\n  * [Common Patterns](https://docs.crawl4ai.com/api/c4a-script-reference/#common-patterns)\n  * [Integration with Crawl4AI](https://docs.crawl4ai.com/api/c4a-script-reference/#integration-with-crawl4ai)\n\n\n# C4A-Script API Reference\nComplete reference for all C4A-Script commands, syntax, and advanced features.\n## Command Categories\n### ðŸ§­ Navigation Commands\nNavigate between pages and manage browser history.\n#### `GO <url>`\nNavigate to a specific URL.\n**Syntax:**\n```\nGO <url>\nCopy\n```\n\n**Parameters:** - `url` - Target URL (string)\n**Examples:**\n```\nGO https://example.com\nGO https://api.example.com/login\nGO /relative/path\nCopy\n```\n\n**Notes:** - Supports both absolute and relative URLs - Automatically handles protocol detection - Waits for page load to complete\n* * *\n#### `RELOAD`\nRefresh the current page.\n**Syntax:**\n```\nRELOAD\nCopy\n```\n\n**Examples:**\n```\nRELOAD\nCopy\n```\n\n**Notes:** - Equivalent to pressing F5 or clicking browser refresh - Waits for page reload to complete - Preserves current URL\n* * *\n#### `BACK`\nNavigate back in browser history.\n**Syntax:**\n```\nBACK\nCopy\n```\n\n**Examples:**\n```\nBACK\nCopy\n```\n\n**Notes:** - Equivalent to clicking browser back button - Does nothing if no previous page exists - Waits for navigation to complete\n* * *\n#### `FORWARD`\nNavigate forward in browser history.\n**Syntax:**\n```\nFORWARD\nCopy\n```\n\n**Examples:**\n```\nFORWARD\nCopy\n```\n\n**Notes:** - Equivalent to clicking browser forward button - Does nothing if no next page exists - Waits for navigation to complete\n### â±ï¸ Wait Commands\nControl timing and synchronization with page elements.\n#### `WAIT <time>`\nWait for a specified number of seconds.\n**Syntax:**\n```\nWAIT <seconds>\nCopy\n```\n\n**Parameters:** - `seconds` - Number of seconds to wait (number)\n**Examples:**\n```\nWAIT 3\nWAIT 1.5\nWAIT 10\nCopy\n```\n\n**Notes:** - Accepts decimal values - Useful for giving dynamic content time to load - Non-blocking for other browser operations\n* * *\n#### `WAIT <selector> <timeout>`\nWait for an element to appear on the page.\n**Syntax:**\n```\nWAIT `<selector>` <timeout>\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector for the element (string in backticks) - `timeout` - Maximum seconds to wait (number)\n**Examples:**\n```\nWAIT `#content` 10\nWAIT `.loading-spinner` 5\nWAIT `button[type=\"submit\"]` 15\nWAIT `.results .item:first-child` 8\nCopy\n```\n\n**Notes:** - Fails if element doesn't appear within timeout - More reliable than fixed time waits - Supports complex CSS selectors\n* * *\n#### `WAIT \"<text>\" <timeout>`\nWait for specific text to appear anywhere on the page.\n**Syntax:**\n```\nWAIT \"<text>\" <timeout>\nCopy\n```\n\n**Parameters:** - `text` - Text content to wait for (string in quotes) - `timeout` - Maximum seconds to wait (number)\n**Examples:**\n```\nWAIT \"Loading complete\" 10\nWAIT \"Welcome back\" 5\nWAIT \"Search results\" 15\nCopy\n```\n\n**Notes:** - Case-sensitive text matching - Searches entire page content - Useful for dynamic status messages\n### ðŸ–±ï¸ Mouse Commands\nSimulate mouse interactions and movements.\n#### `CLICK <selector>`\nClick on an element specified by CSS selector.\n**Syntax:**\n```\nCLICK `<selector>`\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector for the element (string in backticks)\n**Examples:**\n```\nCLICK `#submit-button`\nCLICK `.menu-item:first-child`\nCLICK `button[data-action=\"save\"]`\nCLICK `a[href=\"/dashboard\"]`\nCopy\n```\n\n**Notes:** - Waits for element to be clickable - Scrolls element into view if necessary - Handles overlapping elements intelligently\n* * *\n#### `CLICK <x> <y>`\nClick at specific coordinates on the page.\n**Syntax:**\n```\nCLICK <x> <y>\nCopy\n```\n\n**Parameters:** - `x` - X coordinate in pixels (number) - `y` - Y coordinate in pixels (number)\n**Examples:**\n```\nCLICK 100 200\nCLICK 500 300\nCLICK 0 0\nCopy\n```\n\n**Notes:** - Coordinates are relative to viewport - Useful when element selectors are unreliable - Consider responsive design implications\n* * *\n#### `DOUBLE_CLICK <selector>`\nDouble-click on an element.\n**Syntax:**\n```\nDOUBLE_CLICK `<selector>`\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector for the element (string in backticks)\n**Examples:**\n```\nDOUBLE_CLICK `.file-icon`\nDOUBLE_CLICK `#editable-cell`\nDOUBLE_CLICK `.expandable-item`\nCopy\n```\n\n**Notes:** - Triggers dblclick event - Common for opening files or editing inline content - Timing between clicks is automatically handled\n* * *\n#### `RIGHT_CLICK <selector>`\nRight-click on an element to open context menu.\n**Syntax:**\n```\nRIGHT_CLICK `<selector>`\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector for the element (string in backticks)\n**Examples:**\n```\nRIGHT_CLICK `#context-target`\nRIGHT_CLICK `.menu-trigger`\nRIGHT_CLICK `img.thumbnail`\nCopy\n```\n\n**Notes:** - Opens browser/application context menu - Useful for testing context menu interactions - May be blocked by some applications\n* * *\n#### `SCROLL <direction> <amount>`\nScroll the page in a specified direction.\n**Syntax:**\n```\nSCROLL <direction> <amount>\nCopy\n```\n\n**Parameters:** - `direction` - Direction to scroll: `UP`, `DOWN`, `LEFT`, `RIGHT` - `amount` - Number of pixels to scroll (number)\n**Examples:**\n```\nSCROLL DOWN 500\nSCROLL UP 200\nSCROLL LEFT 100\nSCROLL RIGHT 300\nCopy\n```\n\n**Notes:** - Smooth scrolling animation - Useful for infinite scroll pages - Amount can be larger than viewport\n* * *\n#### `MOVE <x> <y>`\nMove mouse cursor to specific coordinates.\n**Syntax:**\n```\nMOVE <x> <y>\nCopy\n```\n\n**Parameters:** - `x` - X coordinate in pixels (number) - `y` - Y coordinate in pixels (number)\n**Examples:**\n```\nMOVE 200 100\nMOVE 500 400\nCopy\n```\n\n**Notes:** - Triggers hover effects - Useful for testing mouseover interactions - Does not click, only moves cursor\n* * *\n#### `DRAG <x1> <y1> <x2> <y2>`\nDrag from one point to another.\n**Syntax:**\n```\nDRAG <x1> <y1> <x2> <y2>\nCopy\n```\n\n**Parameters:** - `x1`, `y1` - Starting coordinates (numbers) - `x2`, `y2` - Ending coordinates (numbers)\n**Examples:**\n```\nDRAG 100 100 500 300\nDRAG 0 200 400 200\nCopy\n```\n\n**Notes:** - Simulates click, drag, and release - Useful for sliders, resizing, reordering - Smooth drag animation\n### âŒ¨ï¸ Keyboard Commands\nSimulate keyboard input and key presses.\n#### `TYPE \"<text>\"`\nType text into the currently focused element.\n**Syntax:**\n```\nTYPE \"<text>\"\nCopy\n```\n\n**Parameters:** - `text` - Text to type (string in quotes)\n**Examples:**\n```\nTYPE \"Hello, World!\"\nTYPE \"user@example.com\"\nTYPE \"Password123!\"\nCopy\n```\n\n**Notes:** - Requires an input element to be focused - Types character by character with realistic timing - Supports special characters and Unicode\n* * *\n#### `TYPE $<variable>`\nType the value of a variable.\n**Syntax:**\n```\nTYPE $<variable>\nCopy\n```\n\n**Parameters:** - `variable` - Variable name (without quotes)\n**Examples:**\n```\nSETVAR email = \"user@example.com\"\nTYPE $email\nCopy\n```\n\n**Notes:** - Variable must be defined with SETVAR first - Variable values are strings - Useful for reusable credentials or data\n* * *\n#### `PRESS <key>`\nPress and release a special key.\n**Syntax:**\n```\nPRESS <key>\nCopy\n```\n\n**Parameters:** - `key` - Key name (see supported keys below)\n**Supported Keys:** - `Tab`, `Enter`, `Escape`, `Space` - `ArrowUp`, `ArrowDown`, `ArrowLeft`, `ArrowRight` - `Delete`, `Backspace` - `Home`, `End`, `PageUp`, `PageDown`\n**Examples:**\n```\nPRESS Tab\nPRESS Enter\nPRESS Escape\nPRESS ArrowDown\nCopy\n```\n\n**Notes:** - Simulates actual key press and release - Useful for form navigation and shortcuts - Case-sensitive key names\n* * *\n#### `KEY_DOWN <key>`\nHold down a modifier key.\n**Syntax:**\n```\nKEY_DOWN <key>\nCopy\n```\n\n**Parameters:** - `key` - Modifier key: `Shift`, `Control`, `Alt`, `Meta`\n**Examples:**\n```\nKEY_DOWN Shift\nKEY_DOWN Control\nCopy\n```\n\n**Notes:** - Must be paired with KEY_UP - Useful for key combinations - Meta key is Cmd on Mac, Windows key on PC\n* * *\n#### `KEY_UP <key>`\nRelease a modifier key.\n**Syntax:**\n```\nKEY_UP <key>\nCopy\n```\n\n**Parameters:** - `key` - Modifier key: `Shift`, `Control`, `Alt`, `Meta`\n**Examples:**\n```\nKEY_UP Shift\nKEY_UP Control\nCopy\n```\n\n**Notes:** - Must be paired with KEY_DOWN - Releases the specified modifier key - Good practice to always release held keys\n* * *\n#### `CLEAR <selector>`\nClear the content of an input field.\n**Syntax:**\n```\nCLEAR `<selector>`\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector for input element (string in backticks)\n**Examples:**\n```\nCLEAR `#search-box`\nCLEAR `input[name=\"email\"]`\nCLEAR `.form-input:first-child`\nCopy\n```\n\n**Notes:** - Works with input, textarea elements - Faster than selecting all and deleting - Triggers appropriate change events\n* * *\n#### `SET <selector> \"<value>\"`\nSet the value of an input field directly.\n**Syntax:**\n```\nSET `<selector>` \"<value>\"\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector for input element (string in backticks) - `value` - Value to set (string in quotes)\n**Examples:**\n```\nSET `#email` \"user@example.com\"\nSET `#age` \"25\"\nSET `textarea#message` \"Hello, this is a test message.\"\nCopy\n```\n\n**Notes:** - Directly sets value without typing animation - Faster than TYPE for long text - Triggers change and input events\n### ðŸ”€ Control Flow Commands\nAdd conditional logic and loops to your scripts.\n#### `IF (EXISTS <selector>) THEN <command>`\nExecute command if element exists.\n**Syntax:**\n```\nIF (EXISTS `<selector>`) THEN <command>\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector to check (string in backticks) - `command` - Command to execute if condition is true\n**Examples:**\n```\nIF (EXISTS `.cookie-banner`) THEN CLICK `.accept-cookies`\nIF (EXISTS `#popup-modal`) THEN CLICK `.close-button`\nIF (EXISTS `.error-message`) THEN RELOAD\nCopy\n```\n\n**Notes:** - Checks for element existence at time of execution - Does not wait for element to appear - Can be combined with ELSE\n* * *\n#### `IF (EXISTS <selector>) THEN <command> ELSE <command>`\nExecute command based on element existence.\n**Syntax:**\n```\nIF (EXISTS `<selector>`) THEN <command> ELSE <command>\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector to check (string in backticks) - First `command` - Execute if condition is true - Second `command` - Execute if condition is false\n**Examples:**\n```\nIF (EXISTS `.user-menu`) THEN CLICK `.logout` ELSE CLICK `.login`\nIF (EXISTS `.loading`) THEN WAIT 5 ELSE CLICK `#continue`\nCopy\n```\n\n**Notes:** - Exactly one command will be executed - Useful for handling different page states - Commands must be on same line\n* * *\n#### `IF (NOT EXISTS <selector>) THEN <command>`\nExecute command if element does not exist.\n**Syntax:**\n```\nIF (NOT EXISTS `<selector>`) THEN <command>\nCopy\n```\n\n**Parameters:** - `selector` - CSS selector to check (string in backticks) - `command` - Command to execute if element doesn't exist\n**Examples:**\n```\nIF (NOT EXISTS `.logged-in`) THEN GO /login\nIF (NOT EXISTS `.results`) THEN CLICK `#search-button`\nCopy\n```\n\n**Notes:** - Inverse of EXISTS condition - Useful for error handling - Can check for missing required elements\n* * *\n#### `IF (<javascript>) THEN <command>`\nExecute command based on JavaScript condition.\n**Syntax:**\n```\nIF (`<javascript>`) THEN <command>\nCopy\n```\n\n**Parameters:** - `javascript` - JavaScript expression that returns boolean (string in backticks) - `command` - Command to execute if condition is true\n**Examples:**\n```\nIF (`window.innerWidth < 768`) THEN CLICK `.mobile-menu`\nIF (`document.readyState === \"complete\"`) THEN CLICK `#start`\nIF (`localStorage.getItem(\"user\")`) THEN GO /dashboard\nCopy\n```\n\n**Notes:** - JavaScript executes in browser context - Must return boolean value - Access to all browser APIs and globals\n* * *\n#### `REPEAT (<command>, <count>)`\nRepeat a command a specific number of times.\n**Syntax:**\n```\nREPEAT (<command>, <count>)\nCopy\n```\n\n**Parameters:** - `command` - Command to repeat - `count` - Number of times to repeat (number)\n**Examples:**\n```\nREPEAT (SCROLL DOWN 300, 5)\nREPEAT (PRESS Tab, 3)\nREPEAT (CLICK `.load-more`, 10)\nCopy\n```\n\n**Notes:** - Executes command exactly count times - Useful for pagination, scrolling, navigation - No delay between repetitions (add WAIT if needed)\n* * *\n#### `REPEAT (<command>, <condition>)`\nRepeat a command while condition is true.\n**Syntax:**\n```\nREPEAT (<command>, `<condition>`)\nCopy\n```\n\n**Parameters:** - `command` - Command to repeat - `condition` - JavaScript condition to check (string in backticks)\n**Examples:**\n```\nREPEAT (SCROLL DOWN 500, `document.querySelector(\".load-more\")`)\nREPEAT (PRESS ArrowDown, `window.scrollY < document.body.scrollHeight`)\nCopy\n```\n\n**Notes:** - Condition checked before each iteration - JavaScript condition must return boolean - Be careful to avoid infinite loops\n### ðŸ’¾ Variables and Data\nStore and manipulate data within scripts.\n#### `SETVAR <name> = \"<value>\"`\nCreate or update a variable.\n**Syntax:**\n```\nSETVAR <name> = \"<value>\"\nCopy\n```\n\n**Parameters:** - `name` - Variable name (alphanumeric, underscore) - `value` - Variable value (string in quotes)\n**Examples:**\n```\nSETVAR username = \"john@example.com\"\nSETVAR password = \"secret123\"\nSETVAR base_url = \"https://api.example.com\"\nSETVAR counter = \"0\"\nCopy\n```\n\n**Notes:** - Variables are global within script scope - Values are always strings - Can be used with TYPE command using $variable syntax\n* * *\n#### `EVAL <javascript>`\nExecute arbitrary JavaScript code.\n**Syntax:**\n```\nEVAL `<javascript>`\nCopy\n```\n\n**Parameters:** - `javascript` - JavaScript code to execute (string in backticks)\n**Examples:**\n```\nEVAL `console.log(\"Script started\")`\nEVAL `window.scrollTo(0, 0)`\nEVAL `localStorage.setItem(\"test\", \"value\")`\nEVAL `document.title = \"Automated Test\"`\nCopy\n```\n\n**Notes:** - Full access to browser JavaScript APIs - Useful for custom logic and debugging - Return values are not captured - Be careful with security implications\n### ðŸ“ Comments and Documentation\n#### `# <comment>`\nAdd comments to scripts for documentation.\n**Syntax:**\n```\n# <comment text>\nCopy\n```\n\n**Examples:**\n```\n# This script logs into the application\n# Step 1: Navigate to login page\nGO /login\n\n# Step 2: Fill credentials\nTYPE \"user@example.com\"\nCopy\n```\n\n**Notes:** - Comments are ignored during execution - Useful for documentation and debugging - Can appear anywhere in script - Supports multi-line documentation blocks\n### ðŸ”§ Procedures (Advanced)\nDefine reusable command sequences.\n#### `PROC <name> ... ENDPROC`\nDefine a reusable procedure.\n**Syntax:**\n```\nPROC <name>\n  <commands>\nENDPROC\nCopy\n```\n\n**Parameters:** - `name` - Procedure name (alphanumeric, underscore) - `commands` - Commands to include in procedure\n**Examples:**\n```\nPROC login\n  CLICK `#email`\n  TYPE $email\n  CLICK `#password`\n  TYPE $password\n  CLICK `#submit`\nENDPROC\n\nPROC handle_popups\n  IF (EXISTS `.cookie-banner`) THEN CLICK `.accept`\n  IF (EXISTS `.newsletter-modal`) THEN CLICK `.close`\nENDPROC\nCopy\n```\n\n**Notes:** - Procedures must be defined before use - Support nested command structures - Variables are shared with main script scope\n* * *\n#### `<procedure_name>`\nCall a defined procedure.\n**Syntax:**\n```\n<procedure_name>\nCopy\n```\n\n**Examples:**\n```\n# Define procedure first\nPROC setup\n  GO /login\n  WAIT `#form` 5\nENDPROC\n\n# Call procedure\nsetup\nlogin\nCopy\n```\n\n**Notes:** - Procedure must be defined before calling - Can be called multiple times - No parameters supported (use variables instead)\n## Error Handling Best Practices\n### 1. Always Use Waits\n```\n# Bad - element might not be ready\nCLICK `#button`\n\n# Good - wait for element first\nWAIT `#button` 5\nCLICK `#button`\nCopy\n```\n\n### 2. Handle Optional Elements\n```\n# Check before interacting\nIF (EXISTS `.popup`) THEN CLICK `.close`\nIF (EXISTS `.cookie-banner`) THEN CLICK `.accept`\n\n# Then proceed with main flow\nCLICK `#main-action`\nCopy\n```\n\n### 3. Use Descriptive Variables\n```\n# Set up reusable data\nSETVAR admin_email = \"admin@company.com\"\nSETVAR test_password = \"TestPass123!\"\nSETVAR staging_url = \"https://staging.example.com\"\n\n# Use throughout script\nGO $staging_url\nTYPE $admin_email\nCopy\n```\n\n### 4. Add Debugging Information\n```\n# Log progress\nEVAL `console.log(\"Starting login process\")`\nGO /login\n\n# Verify page state\nIF (`document.title.includes(\"Login\")`) THEN EVAL `console.log(\"On login page\")`\n\n# Continue with login\nTYPE $username\nCopy\n```\n\n## Common Patterns\n### Login Flow\n```\n# Complete login automation\nSETVAR email = \"user@example.com\"\nSETVAR password = \"mypassword\"\n\nGO /login\nWAIT `#login-form` 5\n\n# Handle optional cookie banner\nIF (EXISTS `.cookie-banner`) THEN CLICK `.accept-cookies`\n\n# Fill and submit form\nCLICK `#email`\nTYPE $email\nPRESS Tab\nTYPE $password\nCLICK `button[type=\"submit\"]`\n\n# Wait for redirect\nWAIT `.dashboard` 10\nCopy\n```\n\n### Infinite Scroll\n```\n# Load all content with infinite scroll\nGO /products\n\n# Scroll and load more content\nREPEAT (SCROLL DOWN 500, `document.querySelector(\".load-more\")`)\n\n# Alternative: Fixed number of scrolls\nREPEAT (SCROLL DOWN 800, 10)\nWAIT 2\nCopy\n```\n\n### Form Validation\n```\n# Handle form with validation\nSET `#email` \"invalid-email\"\nCLICK `#submit`\n\n# Check for validation error\nIF (EXISTS `.error-email`) THEN SET `#email` \"valid@example.com\"\n\n# Retry submission\nCLICK `#submit`\nWAIT `.success-message` 5\nCopy\n```\n\n### Multi-step Process\n```\n# Complex multi-step workflow\nPROC navigate_to_step\n  CLICK `.next-button`\n  WAIT `.step-content` 5\nENDPROC\n\n# Step 1\nWAIT `.step-1` 5\nSET `#name` \"John Doe\"\nnavigate_to_step\n\n# Step 2\nSET `#email` \"john@example.com\"\nnavigate_to_step\n\n# Step 3\nCLICK `#submit-final`\nWAIT `.confirmation` 10\nCopy\n```\n\n## Integration with Crawl4AI\nUse C4A-Script with Crawl4AI for dynamic content interaction:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Define interaction script\nscript = \"\"\"\n# Handle dynamic content loading\nWAIT `.content` 5\nIF (EXISTS `.load-more-button`) THEN CLICK `.load-more-button`\nWAIT `.additional-content` 5\n\n# Accept cookies if needed\nIF (EXISTS `.cookie-banner`) THEN CLICK `.accept-all`\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    c4a_script=script,\n    wait_for=\".content\",\n    screenshot=True\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://example.com\", config=config)\n    print(result.markdown)\nCopy\n```\n\nThis reference covers all available C4A-Script commands and patterns. For interactive learning, try the [tutorial](https://docs.crawl4ai.com/api/examples/c4a_script/tutorial/) or [live demo](https://docs.crawl4ai.com/c4a-script/demo).\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/c4a-script-reference/)\n\n\nESC to close\n#### On this page\n  * [Command Categories](https://docs.crawl4ai.com/api/c4a-script-reference/#command-categories)\n  * [ðŸ§­ Navigation Commands](https://docs.crawl4ai.com/api/c4a-script-reference/#navigation-commands)\n  * [GO <url>](https://docs.crawl4ai.com/api/c4a-script-reference/#go-url)\n  * [RELOAD](https://docs.crawl4ai.com/api/c4a-script-reference/#reload)\n  * [BACK](https://docs.crawl4ai.com/api/c4a-script-reference/#back)\n  * [FORWARD](https://docs.crawl4ai.com/api/c4a-script-reference/#forward)\n  * [â±ï¸ Wait Commands](https://docs.crawl4ai.com/api/c4a-script-reference/#wait-commands)\n  * [WAIT <time>](https://docs.crawl4ai.com/api/c4a-script-reference/#wait-time)\n  * [WAIT <selector> <timeout>](https://docs.crawl4ai.com/api/c4a-script-reference/#wait-selector-timeout)\n  * [WAIT \"<text>\" <timeout>](https://docs.crawl4ai.com/api/c4a-script-reference/#wait-text-timeout)\n  * [ðŸ–±ï¸ Mouse Commands](https://docs.crawl4ai.com/api/c4a-script-reference/#mouse-commands)\n  * [CLICK <selector>](https://docs.crawl4ai.com/api/c4a-script-reference/#click-selector)\n  * [CLICK <x> <y>](https://docs.crawl4ai.com/api/c4a-script-reference/#click-x-y)\n  * [DOUBLE_CLICK <selector>](https://docs.crawl4ai.com/api/c4a-script-reference/#double_click-selector)\n  * [RIGHT_CLICK <selector>](https://docs.crawl4ai.com/api/c4a-script-reference/#right_click-selector)\n  * [SCROLL <direction> <amount>](https://docs.crawl4ai.com/api/c4a-script-reference/#scroll-direction-amount)\n  * [MOVE <x> <y>](https://docs.crawl4ai.com/api/c4a-script-reference/#move-x-y)\n  * [DRAG <x1> <y1> <x2> <y2>](https://docs.crawl4ai.com/api/c4a-script-reference/#drag-x1-y1-x2-y2)\n  * [âŒ¨ï¸ Keyboard Commands](https://docs.crawl4ai.com/api/c4a-script-reference/#keyboard-commands)\n  * [TYPE \"<text>\"](https://docs.crawl4ai.com/api/c4a-script-reference/#type-text)\n  * [TYPE $<variable>](https://docs.crawl4ai.com/api/c4a-script-reference/#type-variable)\n  * [PRESS <key>](https://docs.crawl4ai.com/api/c4a-script-reference/#press-key)\n  * [KEY_DOWN <key>](https://docs.crawl4ai.com/api/c4a-script-reference/#key_down-key)\n  * [KEY_UP <key>](https://docs.crawl4ai.com/api/c4a-script-reference/#key_up-key)\n  * [CLEAR <selector>](https://docs.crawl4ai.com/api/c4a-script-reference/#clear-selector)\n  * [SET <selector> \"<value>\"](https://docs.crawl4ai.com/api/c4a-script-reference/#set-selector-value)\n  * [ðŸ”€ Control Flow Commands](https://docs.crawl4ai.com/api/c4a-script-reference/#control-flow-commands)\n  * [IF (EXISTS <selector>) THEN <command>](https://docs.crawl4ai.com/api/c4a-script-reference/#if-exists-selector-then-command)\n  * [IF (EXISTS <selector>) THEN <command> ELSE <command>](https://docs.crawl4ai.com/api/c4a-script-reference/#if-exists-selector-then-command-else-command)\n  * [IF (NOT EXISTS <selector>) THEN <command>](https://docs.crawl4ai.com/api/c4a-script-reference/#if-not-exists-selector-then-command)\n  * [IF (<javascript>) THEN <command>](https://docs.crawl4ai.com/api/c4a-script-reference/#if-javascript-then-command)\n  * [REPEAT (<command>, <count>)](https://docs.crawl4ai.com/api/c4a-script-reference/#repeat-command-count)\n  * [REPEAT (<command>, <condition>)](https://docs.crawl4ai.com/api/c4a-script-reference/#repeat-command-condition)\n  * [ðŸ’¾ Variables and Data](https://docs.crawl4ai.com/api/c4a-script-reference/#variables-and-data)\n  * [SETVAR <name> = \"<value>\"](https://docs.crawl4ai.com/api/c4a-script-reference/#setvar-name-value)\n  * [EVAL <javascript>](https://docs.crawl4ai.com/api/c4a-script-reference/#eval-javascript)\n  * [ðŸ“ Comments and Documentation](https://docs.crawl4ai.com/api/c4a-script-reference/#comments-and-documentation)\n  * [# <comment>](https://docs.crawl4ai.com/api/c4a-script-reference/#comment)\n  * [ðŸ”§ Procedures (Advanced)](https://docs.crawl4ai.com/api/c4a-script-reference/#procedures-advanced)\n  * [PROC <name> ... ENDPROC](https://docs.crawl4ai.com/api/c4a-script-reference/#proc-name-endproc)\n  * [<procedure_name>](https://docs.crawl4ai.com/api/c4a-script-reference/#procedure_name)\n  * [Error Handling Best Practices](https://docs.crawl4ai.com/api/c4a-script-reference/#error-handling-best-practices)\n  * [1. Always Use Waits](https://docs.crawl4ai.com/api/c4a-script-reference/#1-always-use-waits)\n  * [2. Handle Optional Elements](https://docs.crawl4ai.com/api/c4a-script-reference/#2-handle-optional-elements)\n  * [3. Use Descriptive Variables](https://docs.crawl4ai.com/api/c4a-script-reference/#3-use-descriptive-variables)\n  * [4. Add Debugging Information](https://docs.crawl4ai.com/api/c4a-script-reference/#4-add-debugging-information)\n  * [Common Patterns](https://docs.crawl4ai.com/api/c4a-script-reference/#common-patterns)\n  * [Login Flow](https://docs.crawl4ai.com/api/c4a-script-reference/#login-flow)\n  * [Infinite Scroll](https://docs.crawl4ai.com/api/c4a-script-reference/#infinite-scroll)\n  * [Form Validation](https://docs.crawl4ai.com/api/c4a-script-reference/#form-validation)\n  * [Multi-step Process](https://docs.crawl4ai.com/api/c4a-script-reference/#multi-step-process)\n  * [Integration with Crawl4AI](https://docs.crawl4ai.com/api/c4a-script-reference/#integration-with-crawl4ai)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/crawl-result",
    "depth": 1,
    "title": "CrawlResult - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "a12d9013787e5909a36911b293a53006",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/crawl-result/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * CrawlResult\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [CrawlResult Reference](https://docs.crawl4ai.com/api/crawl-result/#crawlresult-reference)\n  * [1. Basic Crawl Info](https://docs.crawl4ai.com/api/crawl-result/#1-basic-crawl-info)\n  * [2. Raw / Cleaned Content](https://docs.crawl4ai.com/api/crawl-result/#2-raw-cleaned-content)\n  * [3. Markdown Fields](https://docs.crawl4ai.com/api/crawl-result/#3-markdown-fields)\n  * [4. Media & Links](https://docs.crawl4ai.com/api/crawl-result/#4-media-links)\n  * [5. Additional Fields](https://docs.crawl4ai.com/api/crawl-result/#5-additional-fields)\n  * [6. dispatch_result (optional)](https://docs.crawl4ai.com/api/crawl-result/#6-dispatch_result-optional)\n  * [7. Network Requests & Console Messages](https://docs.crawl4ai.com/api/crawl-result/#7-network-requests-console-messages)\n  * [8. Example: Accessing Everything](https://docs.crawl4ai.com/api/crawl-result/#8-example-accessing-everything)\n  * [9. Key Points & Future](https://docs.crawl4ai.com/api/crawl-result/#9-key-points-future)\n\n\n#  `CrawlResult` Reference\nThe **`CrawlResult`**class encapsulates everything returned after a single crawl operation. It provides the**raw or processed content** , details on links and media, plus optional metadata (like screenshots, PDFs, or extracted JSON).\n**Location** : `crawl4ai/crawler/models.py` (for reference)\n```\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    fit_html: Optional[str] = None  # Preprocessed HTML optimized for extraction\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    mhtml: Optional[str] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    dispatch_result: Optional[DispatchResult] = None\n    ...\nCopy\n```\n\nBelow is a **field-by-field** explanation and possible usage patterns.\n* * *\n## 1. Basic Crawl Info\n### 1.1 **`url`**_(str)_\n**What** : The final crawled URL (after any redirects).  \n**Usage** : \n```\nprint(result.url)  # e.g., \"https://example.com/\"\nCopy\n```\n\n### 1.2 **`success`**_(bool)_\n**What** : `True` if the crawl pipeline ended without major errors; `False` otherwise.  \n**Usage** : \n```\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\nCopy\n```\n\n### 1.3 **`status_code`**_(Optional[int])_\n**What** : The page's HTTP status code (e.g., 200, 404).  \n**Usage** : \n```\nif result.status_code == 404:\n    print(\"Page not found!\")\nCopy\n```\n\n### 1.4 **`error_message`**_(Optional[str])_\n**What** : If `success=False`, a textual description of the failure.  \n**Usage** : \n```\nif not result.success:\n    print(\"Error:\", result.error_message)\nCopy\n```\n\n### 1.5 **`session_id`**_(Optional[str])_\n**What** : The ID used for reusing a browser context across multiple calls.  \n**Usage** : \n```\n# If you used session_id=\"login_session\" in CrawlerRunConfig, see it here:\nprint(\"Session:\", result.session_id)\nCopy\n```\n\n### 1.6 **`response_headers`**_(Optional[dict])_\n**What** : Final HTTP response headers.  \n**Usage** : \n```\nif result.response_headers:\n    print(\"Server:\", result.response_headers.get(\"Server\", \"Unknown\"))\nCopy\n```\n\n### 1.7 **`ssl_certificate`**_(Optional[SSLCertificate])_\n**What** : If `fetch_ssl_certificate=True` in your CrawlerRunConfig, **`result.ssl_certificate`**contains a[**`SSLCertificate`**](https://docs.crawl4ai.com/advanced/ssl-certificate/)object describing the site's certificate. You can export the cert in multiple formats (PEM/DER/JSON) or access its properties like`issuer`, `subject`, `valid_from`, `valid_until`, etc. **Usage** : \n```\nif result.ssl_certificate:\n    print(\"Issuer:\", result.ssl_certificate.issuer)\nCopy\n```\n\n* * *\n## 2. Raw / Cleaned Content\n### 2.1 **`html`**_(str)_\n**What** : The **original** unmodified HTML from the final page load.  \n**Usage** : \n```\n# Possibly large\nprint(len(result.html))\nCopy\n```\n\n### 2.2 **`cleaned_html`**_(Optional[str])_\n**What** : A sanitized HTML versionâ€”scripts, styles, or excluded tags are removed based on your `CrawlerRunConfig`.  \n**Usage** : \n```\nprint(result.cleaned_html[:500])  # Show a snippet\nCopy\n```\n\n* * *\n## 3. Markdown Fields\n### 3.1 The Markdown Generation Approach\nCrawl4AI can convert HTMLâ†’Markdown, optionally including:\n  * **Raw** markdown \n  * **Links as citations** (with a references section) \n  * **Fit** markdown if a **content filter** is used (like Pruning or BM25)\n\n\n**`MarkdownGenerationResult`**includes: -**`raw_markdown`**_(str)_ : The full HTMLâ†’Markdown conversion.  \n- **`markdown_with_citations`**_(str)_ : Same markdown, but with link references as academic-style citations.  \n- **`references_markdown`**_(str)_ : The reference list or footnotes at the end.  \n- **`fit_markdown`**_(Optional[str])_ : If content filtering (Pruning/BM25) was applied, the filtered \"fit\" text.  \n- **`fit_html`**_(Optional[str])_ : The HTML that led to `fit_markdown`.\n**Usage** : \n```\nif result.markdown:\n    md_res = result.markdown\n    print(\"Raw MD:\", md_res.raw_markdown[:300])\n    print(\"Citations MD:\", md_res.markdown_with_citations[:300])\n    print(\"References:\", md_res.references_markdown)\n    if md_res.fit_markdown:\n        print(\"Pruned text:\", md_res.fit_markdown[:300])\nCopy\n```\n\n### 3.2 **`markdown`**_(Optional[Union[str, MarkdownGenerationResult]])_\n**What** : Holds the `MarkdownGenerationResult`.  \n**Usage** : \n```\nprint(result.markdown.raw_markdown[:200])\nprint(result.markdown.fit_markdown)\nprint(result.markdown.fit_html)\nCopy\n```\n\n**Important** : \"Fit\" content (in `fit_markdown`/`fit_html`) exists in result.markdown, only if you used a **filter** (like **PruningContentFilter** or **BM25ContentFilter**) within a `MarkdownGenerationStrategy`.\n* * *\n## 4. Media & Links\n### 4.1 **`media`**_(Dict[str, List[Dict]])_\n**What** : Contains info about discovered images, videos, or audio. Typically keys: `\"images\"`, `\"videos\"`, `\"audios\"`.  \n**Common Fields** in each item:\n  * `src` _(str)_ : Media URL \n  * `alt` or `title` _(str)_ : Descriptive text \n  * `score` _(float)_ : Relevance score if the crawler's heuristic found it \"important\" \n  * `desc` or `description` _(Optional[str])_ : Additional context extracted from surrounding text \n\n\n**Usage** : \n```\nimages = result.media.get(\"images\", [])\nfor img in images:\n    if img.get(\"score\", 0) > 5:\n        print(\"High-value image:\", img[\"src\"])\nCopy\n```\n\n### 4.2 **`links`**_(Dict[str, List[Dict]])_\n**What** : Holds internal and external link data. Usually two keys: `\"internal\"` and `\"external\"`.  \n**Common Fields** :\n  * `href` _(str)_ : The link target \n  * `text` _(str)_ : Link text \n  * `title` _(str)_ : Title attribute \n  * `context` _(str)_ : Surrounding text snippet \n  * `domain` _(str)_ : If external, the domain\n\n\n**Usage** : \n```\nfor link in result.links[\"internal\"]:\n    print(f\"Internal link to {link['href']} with text {link['text']}\")\nCopy\n```\n\n* * *\n## 5. Additional Fields\n### 5.1 **`extracted_content`**_(Optional[str])_\n**What** : If you used **`extraction_strategy`**(CSS, LLM, etc.), the structured output (JSON).  \n**Usage** : \n```\nif result.extracted_content:\n    data = json.loads(result.extracted_content)\n    print(data)\nCopy\n```\n\n### 5.2 **`downloaded_files`**_(Optional[List[str]])_\n**What** : If `accept_downloads=True` in your `BrowserConfig` + `downloads_path`, lists local file paths for downloaded items.  \n**Usage** : \n```\nif result.downloaded_files:\n    for file_path in result.downloaded_files:\n        print(\"Downloaded:\", file_path)\nCopy\n```\n\n### 5.3 **`screenshot`**_(Optional[str])_\n**What** : Base64-encoded screenshot if `screenshot=True` in `CrawlerRunConfig`.  \n**Usage** : \n```\nimport base64\nif result.screenshot:\n    with open(\"page.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\nCopy\n```\n\n### 5.4 **`pdf`**_(Optional[bytes])_\n**What** : Raw PDF bytes if `pdf=True` in `CrawlerRunConfig`.  \n**Usage** : \n```\nif result.pdf:\n    with open(\"page.pdf\", \"wb\") as f:\n        f.write(result.pdf)\nCopy\n```\n\n### 5.5 **`mhtml`**_(Optional[str])_\n**What** : MHTML snapshot of the page if `capture_mhtml=True` in `CrawlerRunConfig`. MHTML (MIME HTML) format preserves the entire web page with all its resources (CSS, images, scripts, etc.) in a single file.  \n**Usage** : \n```\nif result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)\nCopy\n```\n\n### 5.6 **`metadata`**_(Optional[dict])_\n**What** : Page-level metadata if discovered (title, description, OG data, etc.).  \n**Usage** : \n```\nif result.metadata:\n    print(\"Title:\", result.metadata.get(\"title\"))\n    print(\"Author:\", result.metadata.get(\"author\"))\nCopy\n```\n\n* * *\n## 6. `dispatch_result` (optional)\nA `DispatchResult` object providing additional concurrency and resource usage information when crawling URLs in parallel (e.g., via `arun_many()` with custom dispatchers). It contains:\n  * **`task_id`**: A unique identifier for the parallel task.\n  * **`memory_usage`**(float): The memory (in MB) used at the time of completion.\n  * **`peak_memory`**(float): The peak memory usage (in MB) recorded during the task's execution.\n  * **`start_time`**/**`end_time`**(datetime): Time range for this crawling task.\n  * **`error_message`**(str): Any dispatcher- or concurrency-related error encountered.\n\n\n```\n# Example usage:\nfor result in results:\n    if result.success and result.dispatch_result:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}, Task ID: {dr.task_id}\")\n        print(f\"Memory: {dr.memory_usage:.1f} MB (Peak: {dr.peak_memory:.1f} MB)\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")\nCopy\n```\n\n> **Note** : This field is typically populated when using `arun_many(...)` alongside a **dispatcher** (e.g., `MemoryAdaptiveDispatcher` or `SemaphoreDispatcher`). If no concurrency or dispatcher is used, `dispatch_result` may remain `None`. \n* * *\n## 7. Network Requests & Console Messages\nWhen you enable network and console message capturing in `CrawlerRunConfig` using `capture_network_requests=True` and `capture_console_messages=True`, the `CrawlResult` will include these fields:\n### 7.1 **`network_requests`**_(Optional[List[Dict[str, Any]]])_\n**What** : A list of dictionaries containing information about all network requests, responses, and failures captured during the crawl. **Structure** : - Each item has an `event_type` field that can be `\"request\"`, `\"response\"`, or `\"request_failed\"`. - Request events include `url`, `method`, `headers`, `post_data`, `resource_type`, and `is_navigation_request`. - Response events include `url`, `status`, `status_text`, `headers`, and `request_timing`. - Failed request events include `url`, `method`, `resource_type`, and `failure_text`. - All events include a `timestamp` field.\n**Usage** : \n```\nif result.network_requests:\n    # Count different types of events\n    requests = [r for r in result.network_requests if r.get(\"event_type\") == \"request\"]\n    responses = [r for r in result.network_requests if r.get(\"event_type\") == \"response\"]\n    failures = [r for r in result.network_requests if r.get(\"event_type\") == \"request_failed\"]\n\n    print(f\"Captured {len(requests)} requests, {len(responses)} responses, and {len(failures)} failures\")\n\n    # Analyze API calls\n    api_calls = [r for r in requests if \"api\" in r.get(\"url\", \"\")]\n\n    # Identify failed resources\n    for failure in failures:\n        print(f\"Failed to load: {failure.get('url')} - {failure.get('failure_text')}\")\nCopy\n```\n\n### 7.2 **`console_messages`**_(Optional[List[Dict[str, Any]]])_\n**What** : A list of dictionaries containing all browser console messages captured during the crawl. **Structure** : - Each item has a `type` field indicating the message type (e.g., `\"log\"`, `\"error\"`, `\"warning\"`, etc.). - The `text` field contains the actual message text. - Some messages include `location` information (URL, line, column). - All messages include a `timestamp` field.\n**Usage** : \n```\nif result.console_messages:\n    # Count messages by type\n    message_types = {}\n    for msg in result.console_messages:\n        msg_type = msg.get(\"type\", \"unknown\")\n        message_types[msg_type] = message_types.get(msg_type, 0) + 1\n\n    print(f\"Message type counts: {message_types}\")\n\n    # Display errors (which are usually most important)\n    for msg in result.console_messages:\n        if msg.get(\"type\") == \"error\":\n            print(f\"Error: {msg.get('text')}\")\nCopy\n```\n\nThese fields provide deep visibility into the page's network activity and browser console, which is invaluable for debugging, security analysis, and understanding complex web applications.\nFor more details on network and console capturing, see the [Network & Console Capture documentation](https://docs.crawl4ai.com/advanced/network-console-capture/).\n* * *\n## 8. Example: Accessing Everything\n```\nasync def handle_result(result: CrawlResult):\n    if not result.success:\n        print(\"Crawl error:\", result.error_message)\n        return\n\n    # Basic info\n    print(\"Crawled URL:\", result.url)\n    print(\"Status code:\", result.status_code)\n\n    # HTML\n    print(\"Original HTML size:\", len(result.html))\n    print(\"Cleaned HTML size:\", len(result.cleaned_html or \"\"))\n\n    # Markdown output\n    if result.markdown:\n        print(\"Raw Markdown:\", result.markdown.raw_markdown[:300])\n        print(\"Citations Markdown:\", result.markdown.markdown_with_citations[:300])\n        if result.markdown.fit_markdown:\n            print(\"Fit Markdown:\", result.markdown.fit_markdown[:200])\n\n    # Media & Links\n    if \"images\" in result.media:\n        print(\"Image count:\", len(result.media[\"images\"]))\n    if \"internal\" in result.links:\n        print(\"Internal link count:\", len(result.links[\"internal\"]))\n\n    # Extraction strategy result\n    if result.extracted_content:\n        print(\"Structured data:\", result.extracted_content)\n\n    # Screenshot/PDF/MHTML\n    if result.screenshot:\n        print(\"Screenshot length:\", len(result.screenshot))\n    if result.pdf:\n        print(\"PDF bytes length:\", len(result.pdf))\n    if result.mhtml:\n        print(\"MHTML length:\", len(result.mhtml))\n\n    # Network and console capturing\n    if result.network_requests:\n        print(f\"Network requests captured: {len(result.network_requests)}\")\n        # Analyze request types\n        req_types = {}\n        for req in result.network_requests:\n            if \"resource_type\" in req:\n                req_types[req[\"resource_type\"]] = req_types.get(req[\"resource_type\"], 0) + 1\n        print(f\"Resource types: {req_types}\")\n\n    if result.console_messages:\n        print(f\"Console messages captured: {len(result.console_messages)}\")\n        # Count by message type\n        msg_types = {}\n        for msg in result.console_messages:\n            msg_types[msg.get(\"type\", \"unknown\")] = msg_types.get(msg.get(\"type\", \"unknown\"), 0) + 1\n        print(f\"Message types: {msg_types}\")\nCopy\n```\n\n* * *\n## 9. Key Points & Future\n1. **Deprecated legacy properties of CrawlResult**  \n- `markdown_v2` - Deprecated in v0.5. Just use `markdown`. It holds the `MarkdownGenerationResult` now! - `fit_markdown` and `fit_html` - Deprecated in v0.5. They can now be accessed via `MarkdownGenerationResult` in `result.markdown`. eg: `result.markdown.fit_markdown` and `result.markdown.fit_html`\n2. **Fit Content**  \n- **`fit_markdown`**and**`fit_html`**appear in MarkdownGenerationResult, only if you used a content filter (like**PruningContentFilter** or **BM25ContentFilter**) inside your **MarkdownGenerationStrategy** or set them directly.  \n- If no filter is used, they remain `None`.\n3. **References & Citations**  \n- If you enable link citations in your `DefaultMarkdownGenerator` (`options={\"citations\": True}`), youâ€™ll see `markdown_with_citations` plus a **`references_markdown`**block. This helps large language models or academic-like referencing.\n4. **Links & Media**  \n- `links[\"internal\"]` and `links[\"external\"]` group discovered anchors by domain.  \n- `media[\"images\"]` / `[\"videos\"]` / `[\"audios\"]` store extracted media elements with optional scoring or context.\n5. **Error Cases**  \n- If `success=False`, check `error_message` (e.g., timeouts, invalid URLs).  \n- `status_code` might be `None` if we failed before an HTTP response.\nUse **`CrawlResult`**to glean all final outputs and feed them into your data pipelines, AI models, or archives. With the synergy of a properly configured**BrowserConfig** and **CrawlerRunConfig** , the crawler can produce robust, structured results here in **`CrawlResult`**.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/crawl-result/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/crawl-result/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/crawl-result/)\n\n\nESC to close\n#### On this page\n  * [1. Basic Crawl Info](https://docs.crawl4ai.com/api/crawl-result/#1-basic-crawl-info)\n  * [1.1 url (str)](https://docs.crawl4ai.com/api/crawl-result/#11-url-str)\n  * [1.2 success (bool)](https://docs.crawl4ai.com/api/crawl-result/#12-success-bool)\n  * [1.3 status_code (Optional[int])](https://docs.crawl4ai.com/api/crawl-result/#13-status_code-optionalint)\n  * [1.4 error_message (Optional[str])](https://docs.crawl4ai.com/api/crawl-result/#14-error_message-optionalstr)\n  * [1.5 session_id (Optional[str])](https://docs.crawl4ai.com/api/crawl-result/#15-session_id-optionalstr)\n  * [1.6 response_headers (Optional[dict])](https://docs.crawl4ai.com/api/crawl-result/#16-response_headers-optionaldict)\n  * [1.7 ssl_certificate (Optional[SSLCertificate])](https://docs.crawl4ai.com/api/crawl-result/#17-ssl_certificate-optionalsslcertificate)\n  * [2. Raw / Cleaned Content](https://docs.crawl4ai.com/api/crawl-result/#2-raw-cleaned-content)\n  * [2.1 html (str)](https://docs.crawl4ai.com/api/crawl-result/#21-html-str)\n  * [2.2 cleaned_html (Optional[str])](https://docs.crawl4ai.com/api/crawl-result/#22-cleaned_html-optionalstr)\n  * [3. Markdown Fields](https://docs.crawl4ai.com/api/crawl-result/#3-markdown-fields)\n  * [3.1 The Markdown Generation Approach](https://docs.crawl4ai.com/api/crawl-result/#31-the-markdown-generation-approach)\n  * [3.2 markdown (Optional[Union[str, MarkdownGenerationResult]])](https://docs.crawl4ai.com/api/crawl-result/#32-markdown-optionalunionstr-markdowngenerationresult)\n  * [4. Media & Links](https://docs.crawl4ai.com/api/crawl-result/#4-media-links)\n  * [4.1 media (Dict[str, List[Dict]])](https://docs.crawl4ai.com/api/crawl-result/#41-media-dictstr-listdict)\n  * [4.2 links (Dict[str, List[Dict]])](https://docs.crawl4ai.com/api/crawl-result/#42-links-dictstr-listdict)\n  * [5. Additional Fields](https://docs.crawl4ai.com/api/crawl-result/#5-additional-fields)\n  * [5.1 extracted_content (Optional[str])](https://docs.crawl4ai.com/api/crawl-result/#51-extracted_content-optionalstr)\n  * [5.2 downloaded_files (Optional[List[str]])](https://docs.crawl4ai.com/api/crawl-result/#52-downloaded_files-optionalliststr)\n  * [5.3 screenshot (Optional[str])](https://docs.crawl4ai.com/api/crawl-result/#53-screenshot-optionalstr)\n  * [5.4 pdf (Optional[bytes])](https://docs.crawl4ai.com/api/crawl-result/#54-pdf-optionalbytes)\n  * [5.5 mhtml (Optional[str])](https://docs.crawl4ai.com/api/crawl-result/#55-mhtml-optionalstr)\n  * [5.6 metadata (Optional[dict])](https://docs.crawl4ai.com/api/crawl-result/#56-metadata-optionaldict)\n  * [6. dispatch_result (optional)](https://docs.crawl4ai.com/api/crawl-result/#6-dispatch_result-optional)\n  * [7. Network Requests & Console Messages](https://docs.crawl4ai.com/api/crawl-result/#7-network-requests-console-messages)\n  * [7.1 network_requests (Optional[List[Dict[str, Any]]])](https://docs.crawl4ai.com/api/crawl-result/#71-network_requests-optionallistdictstr-any)\n  * [7.2 console_messages (Optional[List[Dict[str, Any]]])](https://docs.crawl4ai.com/api/crawl-result/#72-console_messages-optionallistdictstr-any)\n  * [8. Example: Accessing Everything](https://docs.crawl4ai.com/api/crawl-result/#8-example-accessing-everything)\n  * [9. Key Points & Future](https://docs.crawl4ai.com/api/crawl-result/#9-key-points-future)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/parameters",
    "depth": 1,
    "title": "Browser, Crawler & LLM Config - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "ca4941f88ec392e6b46b156d45923a37",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/parameters/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * Browser, Crawler & LLM Config\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [1. BrowserConfig â€“ Controlling the Browser](https://docs.crawl4ai.com/api/parameters/#1-browserconfig-controlling-the-browser)\n  * [1.1 Parameter Highlights](https://docs.crawl4ai.com/api/parameters/#11-parameter-highlights)\n  * [2. CrawlerRunConfig â€“ Controlling Each Crawl](https://docs.crawl4ai.com/api/parameters/#2-crawlerrunconfig-controlling-each-crawl)\n  * [2.1 Parameter Highlights](https://docs.crawl4ai.com/api/parameters/#21-parameter-highlights)\n  * [2.2 Helper Methods](https://docs.crawl4ai.com/api/parameters/#22-helper-methods)\n  * [2.3 Example Usage](https://docs.crawl4ai.com/api/parameters/#23-example-usage)\n  * [2.4 Compliance & Ethics](https://docs.crawl4ai.com/api/parameters/#24-compliance-ethics)\n  * [3. LLMConfig - Setting up LLM providers](https://docs.crawl4ai.com/api/parameters/#3-llmconfig-setting-up-llm-providers)\n  * [3.1 Parameters](https://docs.crawl4ai.com/api/parameters/#31-parameters)\n  * [3.2 Example Usage](https://docs.crawl4ai.com/api/parameters/#32-example-usage)\n  * [4. Putting It All Together](https://docs.crawl4ai.com/api/parameters/#4-putting-it-all-together)\n\n\n# 1. **BrowserConfig** â€“ Controlling the Browser\n`BrowserConfig` focuses on **how** the browser is launched and behaves. This includes headless mode, proxies, user agents, and other environment tweaks.\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    viewport_width=1280,\n    viewport_height=720,\n    proxy=\"http://user:pass@proxy:8080\",\n    user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\",\n)\nCopy\n```\n\n## 1.1 Parameter Highlights\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`browser_type`**|  `\"chromium\"`, `\"firefox\"`, `\"webkit\"`  \n_(default:`\"chromium\"`)_ | Which browser engine to use. `\"chromium\"` is typical for many sites, `\"firefox\"` or `\"webkit\"` for specialized tests.  \n**`headless`**|  `bool` (default: `True`) | Headless means no visible UI. `False` is handy for debugging.  \n**`browser_mode`**|  `str` (default: `\"dedicated\"`) | How browser is initialized: `\"dedicated\"` (new instance), `\"builtin\"` (CDP background), `\"custom\"` (explicit CDP), `\"docker\"` (container).  \n**`use_managed_browser`**|  `bool` (default: `False`) | Launch browser via CDP for advanced control. Set automatically based on `browser_mode`.  \n**`cdp_url`**|  `str` (default: `None`) | Chrome DevTools Protocol endpoint URL (e.g., `\"ws://localhost:9222/devtools/browser/\"`). Set automatically based on `browser_mode`.  \n**`debugging_port`**|  `int` (default: `9222`) | Port for browser debugging protocol.  \n**`host`**|  `str` (default: `\"localhost\"`) | Host for browser connection.  \n**`viewport_width`**|  `int` (default: `1080`) | Initial page width (in px). Useful for testing responsive layouts.  \n**`viewport_height`**|  `int` (default: `600`) | Initial page height (in px).  \n**`viewport`**|  `dict` (default: `None`) | Viewport dimensions dict. If set, overrides `viewport_width` and `viewport_height`.  \n**`proxy`**|  `str` (deprecated) | Deprecated. Use `proxy_config` instead. If set, it will be auto-converted internally.  \n**`proxy_config`**|  `ProxyConfig or dict` (default: `None`) | For advanced or multi-proxy needs, specify `ProxyConfig` object or dict like `{\"server\": \"...\", \"username\": \"...\", \"password\": \"...\"}`.  \n**`use_persistent_context`**|  `bool` (default: `False`) | If `True`, uses a **persistent** browser context (keep cookies, sessions across runs). Also sets `use_managed_browser=True`.  \n**`user_data_dir`**|  `str or None` (default: `None`) | Directory to store user data (profiles, cookies). Must be set if you want permanent sessions.  \n**`chrome_channel`**|  `str` (default: `\"chromium\"`) | Chrome channel to launch (e.g., \"chrome\", \"msedge\"). Only for `browser_type=\"chromium\"`. Auto-set to empty for Firefox/WebKit.  \n**`channel`**|  `str` (default: `\"chromium\"`) | Alias for `chrome_channel`.  \n**`accept_downloads`**|  `bool` (default: `False`) | Whether to allow file downloads. Requires `downloads_path` if `True`.  \n**`downloads_path`**|  `str or None` (default: `None`) | Directory to store downloaded files.  \n**`storage_state`**|  `str or dict or None` (default: `None`) | In-memory storage state (cookies, localStorage) to restore browser state.  \n**`ignore_https_errors`**|  `bool` (default: `True`) | If `True`, continues despite invalid certificates (common in dev/staging).  \n**`java_script_enabled`**|  `bool` (default: `True`) | Disable if you want no JS overhead, or if only static content is needed.  \n**`sleep_on_close`**|  `bool` (default: `False`) | Add a small delay when closing browser (can help with cleanup issues).  \n**`cookies`**|  `list` (default: `[]`) | Pre-set cookies, each a dict like `{\"name\": \"session\", \"value\": \"...\", \"url\": \"...\"}`.  \n**`headers`**|  `dict` (default: `{}`) | Extra HTTP headers for every request, e.g. `{\"Accept-Language\": \"en-US\"}`.  \n**`user_agent`**|  `str` (default: Chrome-based UA) | Your custom user agent string.  \n**`user_agent_mode`**|  `str` (default: `\"\"`) | Set to `\"random\"` to randomize user agent from a pool (helps with bot detection).  \n**`user_agent_generator_config`**|  `dict` (default: `{}`) | Configuration dict for user agent generation when `user_agent_mode=\"random\"`.  \n**`text_mode`**|  `bool` (default: `False`) | If `True`, tries to disable images/other heavy content for speed.  \n**`light_mode`**|  `bool` (default: `False`) | Disables some background features for performance gains.  \n**`extra_args`**|  `list` (default: `[]`) | Additional flags for the underlying browser process, e.g. `[\"--disable-extensions\"]`.  \n**`enable_stealth`**|  `bool` (default: `False`) | Enable playwright-stealth mode to bypass bot detection. Cannot be used with `browser_mode=\"builtin\"`.  \n**Tips** : - Set `headless=False` to visually **debug** how pages load or how interactions proceed.  \n- If you need **authentication** storage or repeated sessions, consider `use_persistent_context=True` and specify `user_data_dir`.  \n- For large pages, you might need a bigger `viewport_width` and `viewport_height` to handle dynamic content.\n* * *\n# 2. **CrawlerRunConfig** â€“ Controlling Each Crawl\nWhile `BrowserConfig` sets up the **environment** , `CrawlerRunConfig` details **how** each **crawl operation** should behave: caching, content filtering, link or domain blocking, timeouts, JavaScript code, etc.\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nrun_cfg = CrawlerRunConfig(\n    wait_for=\"css:.main-content\",\n    word_count_threshold=15,\n    excluded_tags=[\"nav\", \"footer\"],\n    exclude_external_links=True,\n    stream=True,  # Enable streaming for arun_many()\n)\nCopy\n```\n\n## 2.1 Parameter Highlights\nWe group them by category. \n### A) **Content Processing**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`word_count_threshold`**|  `int` (default: ~200) | Skips text blocks below X words. Helps ignore trivial sections.  \n**`extraction_strategy`**|  `ExtractionStrategy` (default: None) | If set, extracts structured data (CSS-based, LLM-based, etc.).  \n**`chunking_strategy`**|  `ChunkingStrategy` (default: RegexChunking()) | Strategy to chunk content before extraction. Can be customized for different chunking approaches.  \n**`markdown_generator`**|  `MarkdownGenerationStrategy` (None) | If you want specialized markdown output (citations, filtering, chunking, etc.). Can be customized with options such as `content_source` parameter to select the HTML input source ('cleaned_html', 'raw_html', or 'fit_html').  \n**`css_selector`**|  `str` (None) | Retains only the part of the page matching this selector. Affects the entire extraction process.  \n**`target_elements`**|  `List[str]` (None) | List of CSS selectors for elements to focus on for markdown generation and data extraction, while still processing the entire page for links, media, etc. Provides more flexibility than `css_selector`.  \n**`excluded_tags`**|  `list` (None) | Removes entire tags (e.g. `[\"script\", \"style\"]`).  \n**`excluded_selector`**|  `str` (None) | Like `css_selector` but to exclude. E.g. `\"#ads, .tracker\"`.  \n**`only_text`**|  `bool` (False) | If `True`, tries to extract text-only content.  \n**`prettiify`**|  `bool` (False) | If `True`, beautifies final HTML (slower, purely cosmetic).  \n**`keep_data_attributes`**|  `bool` (False) | If `True`, preserve `data-*` attributes in cleaned HTML.  \n**`keep_attrs`**|  `list` (default: []) | List of HTML attributes to keep during processing (e.g., `[\"id\", \"class\", \"data-value\"]`).  \n**`remove_forms`**|  `bool` (False) | If `True`, remove all `<form>` elements.  \n**`parser_type`**|  `str` (default: \"lxml\") | HTML parser to use (e.g., \"lxml\", \"html.parser\").  \n**`scraping_strategy`**|  `ContentScrapingStrategy` (default: LXMLWebScrapingStrategy()) | Strategy to use for content scraping. Can be customized for different scraping needs (e.g., PDF extraction).  \n* * *\n### B) **Browser Location and Identity**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`locale`**|  `str or None` (None) | Browser's locale (e.g., \"en-US\", \"fr-FR\") for language preferences.  \n**`timezone_id`**|  `str or None` (None) | Browser's timezone (e.g., \"America/New_York\", \"Europe/Paris\").  \n**`geolocation`**|  `GeolocationConfig or None` (None) | GPS coordinates configuration. Use `GeolocationConfig(latitude=..., longitude=..., accuracy=...)`.  \n**`fetch_ssl_certificate`**|  `bool` (False) | If `True`, fetches and includes SSL certificate information in the result.  \n**`proxy_config`**|  `ProxyConfig or dict or None` (None) | Proxy configuration for this specific crawl. Can override browser-level proxy settings.  \n**`proxy_rotation_strategy`**|  `ProxyRotationStrategy` (None) | Strategy for rotating proxies during crawl operations.  \n* * *\n### C) **Caching & Session**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`cache_mode`**| `CacheMode or None` | Controls how caching is handled (`ENABLED`, `BYPASS`, `DISABLED`, etc.). If `None`, typically defaults to `ENABLED`.  \n**`session_id`**| `str or None` | Assign a unique ID to reuse a single browser session across multiple `arun()` calls.  \n**`bypass_cache`**|  `bool` (False) |  **Deprecated.** If `True`, acts like `CacheMode.BYPASS`. Use `cache_mode` instead.  \n**`disable_cache`**|  `bool` (False) |  **Deprecated.** If `True`, acts like `CacheMode.DISABLED`. Use `cache_mode` instead.  \n**`no_cache_read`**|  `bool` (False) |  **Deprecated.** If `True`, acts like `CacheMode.WRITE_ONLY` (writes cache but never reads). Use `cache_mode` instead.  \n**`no_cache_write`**|  `bool` (False) |  **Deprecated.** If `True`, acts like `CacheMode.READ_ONLY` (reads cache but never writes). Use `cache_mode` instead.  \n**`shared_data`**|  `dict or None` (None) | Shared data to be passed between hooks and accessible across crawl operations.  \nUse these for controlling whether you read or write from a local content cache. Handy for large batch crawls or repeated site visits.\n* * *\n### D) **Page Navigation & Timing**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`wait_until`**|  `str` (domcontentloaded) | Condition for navigation to \"complete\". Often `\"networkidle\"` or `\"domcontentloaded\"`.  \n**`page_timeout`**|  `int` (60000 ms) | Timeout for page navigation or JS steps. Increase for slow sites.  \n**`wait_for`**| `str or None` | Wait for a CSS (`\"css:selector\"`) or JS (`\"js:() => bool\"`) condition before content extraction.  \n**`wait_for_timeout`**|  `int or None` (None) | Specific timeout in ms for the `wait_for` condition. If None, uses `page_timeout`.  \n**`wait_for_images`**|  `bool` (False) | Wait for images to load before finishing. Slows down if you only want text.  \n**`delay_before_return_html`**|  `float` (0.1) | Additional pause (seconds) before final HTML is captured. Good for last-second updates.  \n**`check_robots_txt`**|  `bool` (False) | Whether to check and respect robots.txt rules before crawling. If True, caches robots.txt for efficiency.  \n**`mean_delay`**and**`max_range`** |  `float` (0.1, 0.3) | If you call `arun_many()`, these define random delay intervals between crawls, helping avoid detection or rate limits.  \n**`semaphore_count`**|  `int` (5) | Max concurrency for `arun_many()`. Increase if you have resources for parallel crawls.  \n* * *\n### E) **Page Interaction**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`js_code`**|  `str or list[str]` (None) | JavaScript to run after load. E.g. `\"document.querySelector('button')?.click();\"`.  \n**`c4a_script`**|  `str or list[str]` (None) | C4A script that compiles to JavaScript. Alternative to writing raw JS.  \n**`js_only`**|  `bool` (False) | If `True`, indicates we're reusing an existing session and only applying JS. No full reload.  \n**`ignore_body_visibility`**|  `bool` (True) | Skip checking if `<body>` is visible. Usually best to keep `True`.  \n**`scan_full_page`**|  `bool` (False) | If `True`, auto-scroll the page to load dynamic content (infinite scroll).  \n**`scroll_delay`**|  `float` (0.2) | Delay between scroll steps if `scan_full_page=True`.  \n**`max_scroll_steps`**|  `int or None` (None) | Maximum number of scroll steps during full page scan. If None, scrolls until entire page is loaded.  \n**`process_iframes`**|  `bool` (False) | Inlines iframe content for single-page extraction.  \n**`remove_overlay_elements`**|  `bool` (False) | Removes potential modals/popups blocking the main content.  \n**`simulate_user`**|  `bool` (False) | Simulate user interactions (mouse movements) to avoid bot detection.  \n**`override_navigator`**|  `bool` (False) | Override `navigator` properties in JS for stealth.  \n**`magic`**|  `bool` (False) | Automatic handling of popups/consent banners. Experimental.  \n**`adjust_viewport_to_content`**|  `bool` (False) | Resizes viewport to match page content height.  \nIf your page is a single-page app with repeated JS updates, set `js_only=True` in subsequent calls, plus a `session_id` for reusing the same tab.\n* * *\n### F) **Media Handling**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`screenshot`**|  `bool` (False) | Capture a screenshot (base64) in `result.screenshot`.  \n**`screenshot_wait_for`**| `float or None` | Extra wait time before the screenshot.  \n**`screenshot_height_threshold`**|  `int` (~20000) | If the page is taller than this, alternate screenshot strategies are used.  \n**`pdf`**|  `bool` (False) | If `True`, returns a PDF in `result.pdf`.  \n**`capture_mhtml`**|  `bool` (False) | If `True`, captures an MHTML snapshot of the page in `result.mhtml`. MHTML includes all page resources (CSS, images, etc.) in a single file.  \n**`image_description_min_word_threshold`**|  `int` (~50) | Minimum words for an image's alt text or description to be considered valid.  \n**`image_score_threshold`**|  `int` (~3) | Filter out low-scoring images. The crawler scores images by relevance (size, context, etc.).  \n**`exclude_external_images`**|  `bool` (False) | Exclude images from other domains.  \n**`exclude_all_images`**|  `bool` (False) | If `True`, excludes all images from processing (both internal and external).  \n**`table_score_threshold`**|  `int` (7) | Minimum score threshold for processing a table. Lower values include more tables.  \n**`table_extraction`**|  `TableExtractionStrategy` (DefaultTableExtraction) | Strategy for table extraction. Defaults to DefaultTableExtraction with configured threshold.  \n* * *\n### G) **Link/Domain Handling**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`exclude_social_media_domains`**|  `list` (e.g. Facebook/Twitter) | A default list can be extended. Any link to these domains is removed from final output.  \n**`exclude_external_links`**|  `bool` (False) | Removes all links pointing outside the current domain.  \n**`exclude_social_media_links`**|  `bool` (False) | Strips links specifically to social sites (like Facebook or Twitter).  \n**`exclude_domains`**|  `list` ([]) | Provide a custom list of domains to exclude (like `[\"ads.com\", \"trackers.io\"]`).  \n**`exclude_internal_links`**|  `bool` (False) | If `True`, excludes internal links from the results.  \n**`score_links`**|  `bool` (False) | If `True`, calculates intrinsic quality scores for all links using URL structure, text quality, and contextual metrics.  \n**`preserve_https_for_internal_links`**|  `bool` (False) | If `True`, preserves HTTPS scheme for internal links even when the server redirects to HTTP. Useful for security-conscious crawling.  \nUse these for link-level content filtering (often to keep crawls â€œinternalâ€ or to remove spammy domains).\n* * *\n### H) **Debug, Logging & Network Monitoring**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`verbose`**|  `bool` (True) | Prints logs detailing each step of crawling, interactions, or errors.  \n**`log_console`**|  `bool` (False) | Logs the page's JavaScript console output if you want deeper JS debugging.  \n**`capture_network_requests`**|  `bool` (False) | If `True`, captures network requests made by the page in `result.captured_requests`.  \n**`capture_console_messages`**|  `bool` (False) | If `True`, captures console messages from the page in `result.console_messages`.  \n* * *\n### I) **Connection & HTTP Parameters**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`method`**|  `str` (\"GET\") | HTTP method to use when using AsyncHTTPCrawlerStrategy (e.g., \"GET\", \"POST\").  \n**`stream`**|  `bool` (False) | If `True`, enables streaming mode for `arun_many()` to process URLs as they complete rather than waiting for all.  \n**`url`**|  `str or None` (None) | URL for this specific config. Not typically set directly but used internally for URL-specific configurations.  \n**`user_agent`**|  `str or None` (None) | Custom User-Agent string for this crawl. Can override browser-level user agent.  \n**`user_agent_mode`**|  `str or None` (None) | Set to `\"random\"` to randomize user agent. Can override browser-level setting.  \n**`user_agent_generator_config`**|  `dict` ({}) | Configuration for user agent generation when `user_agent_mode=\"random\"`.  \n* * *\n### J) **Virtual Scroll Configuration**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`virtual_scroll_config`**|  `VirtualScrollConfig or dict` (None) | Configuration for handling virtualized scrolling on sites like Twitter/Instagram where content is replaced rather than appended.  \nWhen sites use virtual scrolling (content replaced as you scroll), use `VirtualScrollConfig`:\n```\nfrom crawl4ai import VirtualScrollConfig\n\nvirtual_config = VirtualScrollConfig(\n    container_selector=\"#timeline\",    # CSS selector for scrollable container\n    scroll_count=30,                   # Number of times to scroll\n    scroll_by=\"container_height\",      # How much to scroll: \"container_height\", \"page_height\", or pixels (e.g. 500)\n    wait_after_scroll=0.5             # Seconds to wait after each scroll for content to load\n)\n\nconfig = CrawlerRunConfig(\n    virtual_scroll_config=virtual_config\n)\nCopy\n```\n\n**VirtualScrollConfig Parameters:**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`container_selector`**|  `str` (required) | CSS selector for the scrollable container (e.g., `\"#feed\"`, `\".timeline\"`)  \n**`scroll_count`**|  `int` (10) | Maximum number of scrolls to perform  \n**`scroll_by`**|  `str or int` (\"container_height\") | Scroll amount: `\"container_height\"`, `\"page_height\"`, or pixels (e.g., `500`)  \n**`wait_after_scroll`**|  `float` (0.5) | Time in seconds to wait after each scroll for new content to load  \n**When to use Virtual Scroll vs scan_full_page:** - Use `virtual_scroll_config` when content is **replaced** during scroll (Twitter, Instagram) - Use `scan_full_page` when content is **appended** during scroll (traditional infinite scroll)\nSee [Virtual Scroll documentation](https://docs.crawl4ai.com/advanced/virtual-scroll.md) for detailed examples.\n* * *\n### K) **URL Matching Configuration**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`url_matcher`**|  `UrlMatcher` (None) | Pattern(s) to match URLs against. Can be: string (glob), function, or list of mixed types. **None means match ALL URLs**  \n**`match_mode`**|  `MatchMode` (MatchMode.OR) | How to combine multiple matchers in a list: `MatchMode.OR` (any match) or `MatchMode.AND` (all must match)  \nThe `url_matcher` parameter enables URL-specific configurations when used with `arun_many()`:\n```\nfrom crawl4ai import CrawlerRunConfig, MatchMode\nfrom crawl4ai.processors.pdf import PDFContentScrapingStrategy\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\n# Simple string pattern (glob-style)\npdf_config = CrawlerRunConfig(\n    url_matcher=\"*.pdf\",\n    scraping_strategy=PDFContentScrapingStrategy()\n)\n\n# Multiple patterns with OR logic (default)\nblog_config = CrawlerRunConfig(\n    url_matcher=[\"*/blog/*\", \"*/article/*\", \"*/news/*\"],\n    match_mode=MatchMode.OR  # Any pattern matches\n)\n\n# Function matcher\napi_config = CrawlerRunConfig(\n    url_matcher=lambda url: 'api' in url or url.endswith('.json'),\n    # Other settings like extraction_strategy\n)\n\n# Mixed: String + Function with AND logic\ncomplex_config = CrawlerRunConfig(\n    url_matcher=[\n        lambda url: url.startswith('https://'),  # Must be HTTPS\n        \"*.org/*\",                               # Must be .org domain\n        lambda url: 'docs' in url                # Must contain 'docs'\n    ],\n    match_mode=MatchMode.AND  # ALL conditions must match\n)\n\n# Combined patterns and functions with AND logic\nsecure_docs = CrawlerRunConfig(\n    url_matcher=[\"https://*\", lambda url: '.doc' in url],\n    match_mode=MatchMode.AND  # Must be HTTPS AND contain .doc\n)\n\n# Default config - matches ALL URLs\ndefault_config = CrawlerRunConfig()  # No url_matcher = matches everything\nCopy\n```\n\n**UrlMatcher Types:** - **None (default)** : When `url_matcher` is None or not set, the config matches ALL URLs - **String patterns** : Glob-style patterns like `\"*.pdf\"`, `\"*/api/*\"`, `\"https://*.example.com/*\"` - **Functions** : `lambda url: bool` - Custom logic for complex matching - **Lists** : Mix strings and functions, combined with `MatchMode.OR` or `MatchMode.AND`\n**Important Behavior:** - When passing a list of configs to `arun_many()`, URLs are matched against each config's `url_matcher` in order. First match wins! - If no config matches a URL and there's no default config (one without `url_matcher`), the URL will fail with \"No matching configuration found\" - Always include a default config as the last item if you want to handle all URLs\n* * *\n### L) **Advanced Crawling Features**\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`deep_crawl_strategy`**|  `DeepCrawlStrategy or None` (None) | Strategy for deep/recursive crawling. Enables automatic link following and multi-level site crawling.  \n**`link_preview_config`**|  `LinkPreviewConfig or dict or None` (None) | Configuration for link head extraction and scoring. Fetches and scores link metadata without full page loads.  \n**`experimental`**|  `dict or None` (None) | Dictionary for experimental/beta features not yet integrated into main parameters. Use with caution.  \n**Deep Crawl Strategy** enables automatic site exploration by following links according to defined rules. Useful for sitemap generation or comprehensive site archiving.\n**Link Preview Config** allows efficient link discovery and scoring by fetching only the `<head>` section of linked pages, enabling smart crawl prioritization without the overhead of full page loads.\n**Experimental** parameters are features in beta testing. They may change or be removed in future versions. Check documentation for currently available experimental features.\n* * *\n## 2.2 Helper Methods\nBoth `BrowserConfig` and `CrawlerRunConfig` provide a `clone()` method to create modified copies:\n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200\n)\n\n# Create variations using clone()\nstream_config = base_config.clone(stream=True)\nno_cache_config = base_config.clone(\n    cache_mode=CacheMode.BYPASS,\n    stream=True\n)\nCopy\n```\n\nThe `clone()` method is particularly useful when you need slightly different configurations for different use cases, without modifying the original config.\n## 2.3 Example Usage\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Configure the browser\n    browser_cfg = BrowserConfig(\n        headless=False,\n        viewport_width=1280,\n        viewport_height=720,\n        proxy=\"http://user:pass@myproxy:8080\",\n        text_mode=True\n    )\n\n    # Configure the run\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        session_id=\"my_session\",\n        css_selector=\"main.article\",\n        excluded_tags=[\"script\", \"style\"],\n        exclude_external_links=True,\n        wait_for=\"css:.article-loaded\",\n        screenshot=True,\n        stream=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/news\",\n            config=run_cfg\n        )\n        if result.success:\n            print(\"Final cleaned_html length:\", len(result.cleaned_html))\n            if result.screenshot:\n                print(\"Screenshot captured (base64, length):\", len(result.screenshot))\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 2.4 Compliance & Ethics\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`check_robots_txt`**|  `bool` (False) | When True, checks and respects robots.txt rules before crawling. Uses efficient caching with SQLite backend.  \n**`user_agent`**|  `str` (None) | User agent string to identify your crawler. Used for robots.txt checking when enabled.  \n```\nrun_config = CrawlerRunConfig(\n    check_robots_txt=True,  # Enable robots.txt compliance\n    user_agent=\"MyBot/1.0\"  # Identify your crawler\n)\nCopy\n```\n\n# 3. **LLMConfig** - Setting up LLM providers\nLLMConfig is useful to pass LLM provider config to strategies and functions that rely on LLMs to do extraction, filtering, schema generation etc. Currently it can be used in the following -\n  1. LLMExtractionStrategy\n  2. LLMContentFilter\n  3. JsonCssExtractionStrategy.generate_schema\n  4. JsonXPathExtractionStrategy.generate_schema\n\n\n## 3.1 Parameters\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`provider`**|  `\"ollama/llama3\",\"groq/llama3-70b-8192\",\"groq/llama3-8b-8192\", \"openai/gpt-4o-mini\" ,\"openai/gpt-4o\",\"openai/o1-mini\",\"openai/o1-preview\",\"openai/o3-mini\",\"openai/o3-mini-high\",\"anthropic/claude-3-haiku-20240307\",\"anthropic/claude-3-opus-20240229\",\"anthropic/claude-3-sonnet-20240229\",\"anthropic/claude-3-5-sonnet-20240620\",\"gemini/gemini-pro\",\"gemini/gemini-1.5-pro\",\"gemini/gemini-2.0-flash\",\"gemini/gemini-2.0-flash-exp\",\"gemini/gemini-2.0-flash-lite-preview-02-05\",\"deepseek/deepseek-chat\"`  \n_(default:`\"openai/gpt-4o-mini\"`)_ | Which LLM provider to use.  \n**`api_token`**|  1.Optional. When not provided explicitly, api_token will be read from environment variables based on provider. For example: If a gemini model is passed as provider then,`\"GEMINI_API_KEY\"` will be read from environment variables   \n2. API token of LLM provider   \neg: `api_token = \"gsk_1ClHGGJ7Lpn4WGybR7vNWGdyb3FY7zXEw3SCiy0BAVM9lL8CQv\"`   \n3. Environment variable - use with prefix \"env:\"   \neg:`api_token = \"env: GROQ_API_KEY\"` | API token to use for the given provider  \n**`base_url`**|  Optional. Custom API endpoint | If your provider has a custom endpoint  \n**`backoff_base_delay`**|  Optional. `int` _(default:`2`)_ | Seconds to wait before the first retry when the provider throttles a request.  \n**`backoff_max_attempts`**|  Optional. `int` _(default:`3`)_ | Total tries (initial call + retries) before surfacing an error.  \n**`backoff_exponential_factor`**|  Optional. `int` _(default:`2`)_ | Multiplier that increases the wait time for each retry (`delay = base_delay * factor^attempt`).  \n## 3.2 Example Usage\n```\nllm_config = LLMConfig(\n    provider=\"openai/gpt-4o-mini\",\n    api_token=os.getenv(\"OPENAI_API_KEY\"),\n    backoff_base_delay=1, # optional\n    backoff_max_attempts=5, # optional\n    backoff_exponential_factor=3, # optional\n)\nCopy\n```\n\n## 4. Putting It All Together\n  * **Use** `BrowserConfig` for **global** browser settings: engine, headless, proxy, user agent. \n  * **Use** `CrawlerRunConfig` for each crawlâ€™s **context** : how to filter content, handle caching, wait for dynamic elements, or run JS. \n  * **Pass** both configs to `AsyncWebCrawler` (the `BrowserConfig`) and then to `arun()` (the `CrawlerRunConfig`). \n  * **Use** `LLMConfig` for LLM provider configurations that can be used across all extraction, filtering, schema generation tasks. Can be used in - `LLMExtractionStrategy`, `LLMContentFilter`, `JsonCssExtractionStrategy.generate_schema` & `JsonXPathExtractionStrategy.generate_schema`\n\n\n```\n# Create a modified copy with the clone() method\nstream_cfg = run_cfg.clone(\n    stream=True,\n    cache_mode=CacheMode.BYPASS\n)\nCopy\n```\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/parameters/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/parameters/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/parameters/)\n\n\nESC to close\n#### On this page\n  * [1.1 Parameter Highlights](https://docs.crawl4ai.com/api/parameters/#11-parameter-highlights)\n  * [2.1 Parameter Highlights](https://docs.crawl4ai.com/api/parameters/#21-parameter-highlights)\n  * [A) Content Processing](https://docs.crawl4ai.com/api/parameters/#a-content-processing)\n  * [B) Browser Location and Identity](https://docs.crawl4ai.com/api/parameters/#b-browser-location-and-identity)\n  * [C) Caching & Session](https://docs.crawl4ai.com/api/parameters/#c-caching-session)\n  * [D) Page Navigation & Timing](https://docs.crawl4ai.com/api/parameters/#d-page-navigation-timing)\n  * [E) Page Interaction](https://docs.crawl4ai.com/api/parameters/#e-page-interaction)\n  * [F) Media Handling](https://docs.crawl4ai.com/api/parameters/#f-media-handling)\n  * [G) Link/Domain Handling](https://docs.crawl4ai.com/api/parameters/#g-linkdomain-handling)\n  * [H) Debug, Logging & Network Monitoring](https://docs.crawl4ai.com/api/parameters/#h-debug-logging-network-monitoring)\n  * [I) Connection & HTTP Parameters](https://docs.crawl4ai.com/api/parameters/#i-connection-http-parameters)\n  * [J) Virtual Scroll Configuration](https://docs.crawl4ai.com/api/parameters/#j-virtual-scroll-configuration)\n  * [K) URL Matching Configuration](https://docs.crawl4ai.com/api/parameters/#k-url-matching-configuration)\n  * [L) Advanced Crawling Features](https://docs.crawl4ai.com/api/parameters/#l-advanced-crawling-features)\n  * [2.2 Helper Methods](https://docs.crawl4ai.com/api/parameters/#22-helper-methods)\n  * [2.3 Example Usage](https://docs.crawl4ai.com/api/parameters/#23-example-usage)\n  * [2.4 Compliance & Ethics](https://docs.crawl4ai.com/api/parameters/#24-compliance-ethics)\n  * [3.1 Parameters](https://docs.crawl4ai.com/api/parameters/#31-parameters)\n  * [3.2 Example Usage](https://docs.crawl4ai.com/api/parameters/#32-example-usage)\n  * [4. Putting It All Together](https://docs.crawl4ai.com/api/parameters/#4-putting-it-all-together)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/apps/c4a-script",
    "depth": 1,
    "title": "C4A-Script Interactive Tutorial | Crawl4AI",
    "content_hash": "e82571b7dcb5d72c53225864ebc4a853",
    "content": "## Welcome to C4A-Script Tutorial!\nC4A-Script is a simple language for web automation. This interactive tutorial will teach you:\n  * How to handle popups and banners\n  * Form filling and navigation\n  * Advanced automation techniques\n\n\nStart Tutorial Skip\n#### Edit Event\nCommand Type CLICK DOUBLE_CLICK RIGHT_CLICK TYPE SET SCROLL WAIT\nSelector\nValue\nDirection UP DOWN LEFT RIGHT\nCancel Save\n## C4A-Script Editor\nðŸ“š ðŸ“‹ ðŸ—‘ ðŸ§© â–¶Run  âºRecord  ðŸ“Š\n```\nxxxxxxxxxx\n```\n\n1\n1\n```\nâ€‹\n```\n\n### Recording Timeline\nâ† Back Select All Clear Generate Script\nNavigation\nWait\nMouse Actions\nKeyboard\nControl Flow\nVariables\nProcedures\nComments\nConsole Generated JS\n$ Ready to run C4A scripts...\nðŸ“‹ âœï¸\n```\n// JavaScript will appear here...\n```\n\n## Playground\nðŸ”„ â›¶\nðŸª We use cookies to enhance your experience. By continuing, you agree to our cookie policy.\nAccept All Decline\nÃ—\n## ðŸ“¬ Subscribe to Our Newsletter\nGet the latest updates on web automation!\nSubscribe\n[Home](https://docs.crawl4ai.com/apps/c4a-script/#home) [Products](https://docs.crawl4ai.com/apps/c4a-script/#catalog) [Forms](https://docs.crawl4ai.com/apps/c4a-script/#forms) [Data Tables](https://docs.crawl4ai.com/apps/c4a-script/#data-tables)\n[More â–¼](https://docs.crawl4ai.com/apps/c4a-script/)\n[Tabs Demo](https://docs.crawl4ai.com/apps/c4a-script/#tabs) [FAQ](https://docs.crawl4ai.com/apps/c4a-script/#accordion) [Gallery](https://docs.crawl4ai.com/apps/c4a-script/#gallery)\nðŸ” Login\nðŸ‘¤ Welcome, User! Logout\n# Welcome to C4A-Script Playground\nThis is an interactive demo for testing C4A-Script commands. Each section contains different challenges for web automation.\nStart Tutorial\n### ðŸ” Authentication\nTest login forms and user sessions\n### ðŸ“œ Dynamic Content\nInfinite scroll and pagination\n### ðŸ“ Forms\nComplex form interactions\n### ðŸ“Š Data Tables\nSortable and filterable data\nÃ—\n## Login\nEmail\nPassword\nRemember me \nLogin\n# Product Catalog\nInfinite Scroll Pagination\n### Filters\n#### Category â–¼\nElectronics Clothing Books\n#### Price Range â–¼\n$0 - $500\nðŸ“¦\nProduct 1\n$27.07\nQuick View\nðŸ“¦\nProduct 2\n$55.64\nQuick View\nðŸ“¦\nProduct 3\n$84.59\nQuick View\nðŸ“¦\nProduct 4\n$42.92\nQuick View\nðŸ“¦\nProduct 5\n$20.21\nQuick View\nðŸ“¦\nProduct 6\n$52.16\nQuick View\nðŸ“¦\nProduct 7\n$11.94\nQuick View\nðŸ“¦\nProduct 8\n$100.39\nQuick View\nðŸ“¦\nProduct 9\n$92.13\nQuick View\nðŸ“¦\nProduct 10\n$53.60\nQuick View\nLoading more products...\nLoad More\n1 2 3\n# Form Examples\n## Contact Form\nName\nEmail\nSubject Select a subject Support Sales Feedback\nDepartment Select department\nMessage\nSend Message\n## Multi-step Survey\n### Step 1: Basic Information\nFull Name\nEmail\nNext\n### Step 2: Preferences\nInterests (select multiple) Technology Sports Music Travel\nPrevious Next\n### Step 3: Confirmation\nPlease review your information and submit.\nPrevious Submit Survey\nâœ… Survey submitted successfully! \n# Tabs Demo\nDescription Reviews Specifications\n### Product Description\nThis is a detailed description of the product...\nLorem ipsum dolor sit amet, consectetur adipiscing elit...\nShow More\nThis is the hidden text that appears when you click \"Show More\". It contains additional details about the product that weren't visible initially.\n### Customer Reviews\nLoad Comments\n### Technical Specifications\nModel | XYZ-2000  \n---|---  \nWeight | 2.5 kg  \nDimensions | 30 x 20 x 10 cm  \n# Data Tables\nExport\nName â†• | Email â†• | Date â†• | Actions  \n---|---|---|---  \nUser 11 | user11@example.com | 1/25/2026 | Edit  \nUser 12 | user12@example.com | 1/25/2026 | Edit  \nUser 13 | user13@example.com | 1/25/2026 | Edit  \nUser 14 | user14@example.com | 1/25/2026 | Edit  \nUser 15 | user15@example.com | 1/25/2026 | Edit  \nUser 16 | user16@example.com | 1/25/2026 | Edit  \nUser 17 | user17@example.com | 1/25/2026 | Edit  \nUser 18 | user18@example.com | 1/25/2026 | Edit  \nUser 19 | user19@example.com | 1/25/2026 | Edit  \nUser 20 | user20@example.com | 1/25/2026 | Edit  \nLoad More Rows\nStep 1 of 9 Welcome\nLet's start by waiting for the page to load.\nâ† Previous Next â†’\nÃ—\n"
  },
  {
    "url": "https://docs.crawl4ai.com/api/strategies",
    "depth": 1,
    "title": "Strategies - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "e473b0bd3257652ca3cea171be47b89d",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/api/strategies/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * Strategies\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Extraction & Chunking Strategies API](https://docs.crawl4ai.com/api/strategies/#extraction-chunking-strategies-api)\n  * [Extraction Strategies](https://docs.crawl4ai.com/api/strategies/#extraction-strategies)\n  * [Chunking Strategies](https://docs.crawl4ai.com/api/strategies/#chunking-strategies)\n  * [Usage Examples](https://docs.crawl4ai.com/api/strategies/#usage-examples)\n  * [Best Practices](https://docs.crawl4ai.com/api/strategies/#best-practices)\n\n\n# Extraction & Chunking Strategies API\nThis documentation covers the API reference for extraction and chunking strategies in Crawl4AI.\n## Extraction Strategies\nAll extraction strategies inherit from the base `ExtractionStrategy` class and implement two key methods: - `extract(url: str, html: str) -> List[Dict[str, Any]]` - `run(url: str, sections: List[str]) -> List[Dict[str, Any]]`\n### LLMExtractionStrategy\nUsed for extracting structured data using Language Models.\n```\nLLMExtractionStrategy(\n    # Required Parameters\n    provider: str = DEFAULT_PROVIDER,     # LLM provider (e.g., \"ollama/llama2\")\n    api_token: Optional[str] = None,      # API token\n\n    # Extraction Configuration\n    instruction: str = None,              # Custom extraction instruction\n    schema: Dict = None,                  # Pydantic model schema for structured data\n    extraction_type: str = \"block\",       # \"block\" or \"schema\"\n\n    # Chunking Parameters\n    chunk_token_threshold: int = 4000,    # Maximum tokens per chunk\n    overlap_rate: float = 0.1,           # Overlap between chunks\n    word_token_rate: float = 0.75,       # Word to token conversion rate\n    apply_chunking: bool = True,         # Enable/disable chunking\n\n    # API Configuration\n    base_url: str = None,                # Base URL for API\n    extra_args: Dict = {},               # Additional provider arguments\n    verbose: bool = False                # Enable verbose logging\n)\nCopy\n```\n\n### RegexExtractionStrategy\nUsed for fast pattern-based extraction of common entities using regular expressions.\n```\nRegexExtractionStrategy(\n    # Pattern Configuration\n    pattern: IntFlag = RegexExtractionStrategy.Nothing,  # Bit flags of built-in patterns to use\n    custom: Optional[Dict[str, str]] = None,           # Custom pattern dictionary {label: regex}\n\n    # Input Format\n    input_format: str = \"fit_html\",                    # \"html\", \"markdown\", \"text\" or \"fit_html\"\n)\n\n# Built-in Patterns as Bit Flags\nRegexExtractionStrategy.Email           # Email addresses\nRegexExtractionStrategy.PhoneIntl       # International phone numbers \nRegexExtractionStrategy.PhoneUS         # US-format phone numbers\nRegexExtractionStrategy.Url             # HTTP/HTTPS URLs\nRegexExtractionStrategy.IPv4            # IPv4 addresses\nRegexExtractionStrategy.IPv6            # IPv6 addresses\nRegexExtractionStrategy.Uuid            # UUIDs\nRegexExtractionStrategy.Currency        # Currency values (USD, EUR, etc)\nRegexExtractionStrategy.Percentage      # Percentage values\nRegexExtractionStrategy.Number          # Numeric values\nRegexExtractionStrategy.DateIso         # ISO format dates\nRegexExtractionStrategy.DateUS          # US format dates\nRegexExtractionStrategy.Time24h         # 24-hour format times\nRegexExtractionStrategy.PostalUS        # US postal codes\nRegexExtractionStrategy.PostalUK        # UK postal codes\nRegexExtractionStrategy.HexColor        # HTML hex color codes\nRegexExtractionStrategy.TwitterHandle   # Twitter handles\nRegexExtractionStrategy.Hashtag         # Hashtags\nRegexExtractionStrategy.MacAddr         # MAC addresses\nRegexExtractionStrategy.Iban            # International bank account numbers\nRegexExtractionStrategy.CreditCard      # Credit card numbers\nRegexExtractionStrategy.All             # All available patterns\nCopy\n```\n\n### CosineStrategy\nUsed for content similarity-based extraction and clustering.\n```\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,        # Topic/keyword filter\n    word_count_threshold: int = 10,     # Minimum words per cluster\n    sim_threshold: float = 0.3,         # Similarity threshold\n\n    # Clustering Parameters\n    max_dist: float = 0.2,             # Maximum cluster distance\n    linkage_method: str = 'ward',       # Clustering method\n    top_k: int = 3,                    # Top clusters to return\n\n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n\n    verbose: bool = False              # Enable verbose logging\n)\nCopy\n```\n\n### JsonCssExtractionStrategy\nUsed for CSS selector-based structured data extraction.\n```\nJsonCssExtractionStrategy(\n    schema: Dict[str, Any],    # Extraction schema\n    verbose: bool = False      # Enable verbose logging\n)\n\n# Schema Structure\nschema = {\n    \"name\": str,              # Schema name\n    \"baseSelector\": str,      # Base CSS selector\n    \"fields\": [               # List of fields to extract\n        {\n            \"name\": str,      # Field name\n            \"selector\": str,  # CSS selector\n            \"type\": str,     # Field type: \"text\", \"attribute\", \"html\", \"regex\"\n            \"attribute\": str, # For type=\"attribute\"\n            \"pattern\": str,  # For type=\"regex\"\n            \"transform\": str, # Optional: \"lowercase\", \"uppercase\", \"strip\"\n            \"default\": Any    # Default value if extraction fails\n        }\n    ]\n}\nCopy\n```\n\n## Chunking Strategies\nAll chunking strategies inherit from `ChunkingStrategy` and implement the `chunk(text: str) -> list` method.\n### RegexChunking\nSplits text based on regex patterns.\n```\nRegexChunking(\n    patterns: List[str] = None  # Regex patterns for splitting\n                               # Default: [r'\\n\\n']\n)\nCopy\n```\n\n### SlidingWindowChunking\nCreates overlapping chunks with a sliding window approach.\n```\nSlidingWindowChunking(\n    window_size: int = 100,    # Window size in words\n    step: int = 50             # Step size between windows\n)\nCopy\n```\n\n### OverlappingWindowChunking\nCreates chunks with specified overlap.\n```\nOverlappingWindowChunking(\n    window_size: int = 1000,   # Chunk size in words\n    overlap: int = 100         # Overlap size in words\n)\nCopy\n```\n\n## Usage Examples\n### LLM Extraction\n```\nfrom pydantic import BaseModel\nfrom crawl4ai import LLMExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Define schema\nclass Article(BaseModel):\n    title: str\n    content: str\n    author: str\n\n# Create strategy\nstrategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"ollama/llama2\"),\n    schema=Article.schema(),\n    instruction=\"Extract article details\"\n)\n\n# Use with crawler\nresult = await crawler.arun(\n    url=\"https://example.com/article\",\n    extraction_strategy=strategy\n)\n\n# Access extracted data\ndata = json.loads(result.extracted_content)\nCopy\n```\n\n### Regex Extraction\n```\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, RegexExtractionStrategy\n\n# Method 1: Use built-in patterns\nstrategy = RegexExtractionStrategy(\n    pattern = RegexExtractionStrategy.Email | RegexExtractionStrategy.Url\n)\n\n# Method 2: Use custom patterns\nprice_pattern = {\"usd_price\": r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\"}\nstrategy = RegexExtractionStrategy(custom=price_pattern)\n\n# Method 3: Generate pattern with LLM assistance (one-time)\nfrom crawl4ai import LLMConfig\n\nasync with AsyncWebCrawler() as crawler:\n    # Get sample HTML first\n    sample_result = await crawler.arun(\"https://example.com/products\")\n    html = sample_result.fit_html\n\n    # Generate regex pattern once\n    pattern = RegexExtractionStrategy.generate_pattern(\n        label=\"price\",\n        html=html,\n        query=\"Product prices in USD format\",\n        llm_config=LLMConfig(provider=\"openai/gpt-4o-mini\")\n    )\n\n    # Save pattern for reuse\n    import json\n    with open(\"price_pattern.json\", \"w\") as f:\n        json.dump(pattern, f)\n\n    # Use pattern for extraction (no LLM calls)\n    strategy = RegexExtractionStrategy(custom=pattern)\n    result = await crawler.arun(\n        url=\"https://example.com/products\",\n        config=CrawlerRunConfig(extraction_strategy=strategy)\n    )\n\n    # Process results\n    data = json.loads(result.extracted_content)\n    for item in data:\n        print(f\"{item['label']}: {item['value']}\")\nCopy\n```\n\n### CSS Extraction\n```\nfrom crawl4ai import JsonCssExtractionStrategy\n\n# Define schema\nschema = {\n    \"name\": \"Product List\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\n            \"name\": \"title\",\n            \"selector\": \"h2.title\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \".price\",\n            \"type\": \"text\",\n            \"transform\": \"strip\"\n        },\n        {\n            \"name\": \"image\",\n            \"selector\": \"img\",\n            \"type\": \"attribute\",\n            \"attribute\": \"src\"\n        }\n    ]\n}\n\n# Create and use strategy\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)\nCopy\n```\n\n### Content Chunking\n```\nfrom crawl4ai.chunking_strategy import OverlappingWindowChunking\nfrom crawl4ai import LLMConfig\n\n# Create chunking strategy\nchunker = OverlappingWindowChunking(\n    window_size=500,  # 500 words per chunk\n    overlap=50        # 50 words overlap\n)\n\n# Use with extraction strategy\nstrategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"ollama/llama2\"),\n    chunking_strategy=chunker\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/long-article\",\n    extraction_strategy=strategy\n)\nCopy\n```\n\n## Best Practices\n  1. **Choose the Right Strategy**\n  2. Use `RegexExtractionStrategy` for common data types like emails, phones, URLs, dates\n  3. Use `JsonCssExtractionStrategy` for well-structured HTML with consistent patterns\n  4. Use `LLMExtractionStrategy` for complex, unstructured content requiring reasoning\n  5. Use `CosineStrategy` for content similarity and clustering\n  6. **Strategy Selection Guide**\n```\nIs the target data a common type (email/phone/date/URL)? \nâ†’ RegexExtractionStrategy\n\nDoes the page have consistent HTML structure?\nâ†’ JsonCssExtractionStrategy or JsonXPathExtractionStrategy\n\nIs the data semantically complex or unstructured?\nâ†’ LLMExtractionStrategy\n\nNeed to find content similar to a specific topic?\nâ†’ CosineStrategy\nCopy\n```\n\n  7. **Optimize Chunking**\n```\n# For long documents\nstrategy = LLMExtractionStrategy(\n    chunk_token_threshold=2000,  # Smaller chunks\n    overlap_rate=0.1           # 10% overlap\n)\nCopy\n```\n\n  8. **Combine Strategies for Best Performance**\n```\n# First pass: Extract structure with CSS\ncss_strategy = JsonCssExtractionStrategy(product_schema)\ncss_result = await crawler.arun(url, config=CrawlerRunConfig(extraction_strategy=css_strategy))\nproduct_data = json.loads(css_result.extracted_content)\n\n# Second pass: Extract specific fields with regex\ndescriptions = [product[\"description\"] for product in product_data]\nregex_strategy = RegexExtractionStrategy(\n    pattern=RegexExtractionStrategy.Email | RegexExtractionStrategy.PhoneUS,\n    custom={\"dimension\": r\"\\d+x\\d+x\\d+ (?:cm|in)\"}\n)\n\n# Process descriptions with regex\nfor text in descriptions:\n    matches = regex_strategy.extract(\"\", text)  # Direct extraction\nCopy\n```\n\n  9. **Handle Errors**\n```\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    if result.success:\n        content = json.loads(result.extracted_content)\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")\nCopy\n```\n\n  10. **Monitor Performance**\n```\nstrategy = CosineStrategy(\n    verbose=True,  # Enable logging\n    word_count_threshold=20,  # Filter short content\n    top_k=5  # Limit results\n)\nCopy\n```\n\n  11. **Cache Generated Patterns**\n```\n# For RegexExtractionStrategy pattern generation\nimport json\nfrom pathlib import Path\n\ncache_dir = Path(\"./pattern_cache\")\ncache_dir.mkdir(exist_ok=True)\npattern_file = cache_dir / \"product_pattern.json\"\n\nif pattern_file.exists():\n    with open(pattern_file) as f:\n        pattern = json.load(f)\nelse:\n    # Generate once with LLM\n    pattern = RegexExtractionStrategy.generate_pattern(...)\n    with open(pattern_file, \"w\") as f:\n        json.dump(pattern, f)\nCopy\n```\n\n\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/api/strategies/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/api/strategies/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/api/strategies/)\n\n\nESC to close\n#### On this page\n  * [Extraction Strategies](https://docs.crawl4ai.com/api/strategies/#extraction-strategies)\n  * [LLMExtractionStrategy](https://docs.crawl4ai.com/api/strategies/#llmextractionstrategy)\n  * [RegexExtractionStrategy](https://docs.crawl4ai.com/api/strategies/#regexextractionstrategy)\n  * [CosineStrategy](https://docs.crawl4ai.com/api/strategies/#cosinestrategy)\n  * [JsonCssExtractionStrategy](https://docs.crawl4ai.com/api/strategies/#jsoncssextractionstrategy)\n  * [Chunking Strategies](https://docs.crawl4ai.com/api/strategies/#chunking-strategies)\n  * [RegexChunking](https://docs.crawl4ai.com/api/strategies/#regexchunking)\n  * [SlidingWindowChunking](https://docs.crawl4ai.com/api/strategies/#slidingwindowchunking)\n  * [OverlappingWindowChunking](https://docs.crawl4ai.com/api/strategies/#overlappingwindowchunking)\n  * [Usage Examples](https://docs.crawl4ai.com/api/strategies/#usage-examples)\n  * [LLM Extraction](https://docs.crawl4ai.com/api/strategies/#llm-extraction)\n  * [Regex Extraction](https://docs.crawl4ai.com/api/strategies/#regex-extraction)\n  * [CSS Extraction](https://docs.crawl4ai.com/api/strategies/#css-extraction)\n  * [Content Chunking](https://docs.crawl4ai.com/api/strategies/#content-chunking)\n  * [Best Practices](https://docs.crawl4ai.com/api/strategies/#best-practices)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/apps",
    "depth": 1,
    "title": "Demo Apps - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "4a735c2b32bd7e01267669233a85bca8",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/apps/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * Demo Apps\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [ðŸš€ Crawl4AI Interactive Apps](https://docs.crawl4ai.com/apps/#crawl4ai-interactive-apps)\n  * [ðŸŽ¯ Available Apps](https://docs.crawl4ai.com/apps/#available-apps)\n  * [ðŸš€ Why Use These Apps?](https://docs.crawl4ai.com/apps/#why-use-these-apps)\n  * [ðŸ“¢ Stay Updated](https://docs.crawl4ai.com/apps/#stay-updated)\n\n\n# ðŸš€ Crawl4AI Interactive Apps\nWelcome to the Crawl4AI Apps Hub - your gateway to interactive tools and demos that make web scraping more intuitive and powerful.\n## ðŸ› ï¸ Interactive Tools for Modern Web Scraping\nOur apps are designed to make Crawl4AI more accessible and powerful. Whether you're learning browser automation, designing extraction strategies, or building complex scrapers, these tools provide visual, interactive ways to work with Crawl4AI's features. \n## ðŸŽ¯ Available Apps\nAvailable\n### ðŸŽ¨ C4A-Script Interactive Editor\nA visual, block-based programming environment for creating browser automation scripts. Perfect for beginners and experts alike! \n  * Drag-and-drop visual programming\n  * Real-time JavaScript generation\n  * Interactive tutorials\n  * Export to C4A-Script or JavaScript\n  * Live preview capabilities\n\n\n[Launch Editor â†’](https://docs.crawl4ai.com/apps/c4a-script/)\nAvailable\n### ðŸ§  LLM Context Builder\nGenerate optimized context files for your favorite LLM when working with Crawl4AI. Get focused, relevant documentation based on your needs. \n  * Modular context generation\n  * Memory, reasoning & examples perspectives\n  * Component-based selection\n  * Vibe coding preset\n  * Download custom contexts\n\n\n[Launch Builder â†’](https://docs.crawl4ai.com/apps/llmtxt/)\nComing Soon\n### ðŸ•¸ï¸ Web Scraping Playground\nTest your scraping strategies on real websites with instant feedback. See how different configurations affect your results. \n  * Live website testing\n  * Side-by-side result comparison\n  * Performance metrics\n  * Export configurations\n\n\n[Coming Soon](https://docs.crawl4ai.com/apps/)\nAvailable\n### ðŸ” Crawl4AI Assistant (Chrome Extension)\nVisual schema builder Chrome extension - click on webpage elements to generate extraction schemas and Python code! \n  * Visual element selection\n  * Container & field selection modes\n  * Smart selector generation\n  * Complete Python code generation\n  * One-click installation\n\n\n[Install Extension â†’](https://docs.crawl4ai.com/apps/crawl4ai-assistant/)\nComing Soon\n### ðŸ§ª Extraction Lab\nExperiment with different extraction strategies and see how they perform on your content. Compare LLM vs CSS vs XPath approaches. \n  * Strategy comparison tools\n  * Performance benchmarks\n  * Cost estimation for LLM strategies\n  * Best practice recommendations\n\n\n[Coming Soon](https://docs.crawl4ai.com/apps/)\nComing Soon\n### ðŸ¤– AI Prompt Designer\nCraft and test prompts for LLM-based extraction. See how different prompts affect extraction quality and costs. \n  * Prompt templates library\n  * A/B testing interface\n  * Token usage calculator\n  * Quality metrics\n\n\n[Coming Soon](https://docs.crawl4ai.com/apps/)\nComing Soon\n### ðŸ“Š Crawl Monitor\nReal-time monitoring dashboard for your crawling operations. Track performance, debug issues, and optimize your scrapers. \n  * Real-time crawl statistics\n  * Error tracking and debugging\n  * Resource usage monitoring\n  * Historical analytics\n\n\n[Coming Soon](https://docs.crawl4ai.com/apps/)\n## ðŸš€ Why Use These Apps?\n### ðŸŽ¯ **Accelerate Learning**\nVisual tools help you understand Crawl4AI's concepts faster than reading documentation alone.\n### ðŸ’¡ **Reduce Development Time**\nGenerate working code instantly instead of writing everything from scratch.\n### ðŸ” **Improve Quality**\nTest and refine your approach before deploying to production.\n### ðŸ¤ **Community Driven**\nThese tools are built based on user feedback. Have an idea? [Let us know](https://github.com/unclecode/crawl4ai/issues)!\n## ðŸ“¢ Stay Updated\nWant to know when new apps are released? \n  * â­ [Star us on GitHub](https://github.com/unclecode/crawl4ai) to get notifications\n  * ðŸ¦ Follow [@unclecode](https://twitter.com/unclecode) for announcements\n  * ðŸ’¬ Join our [Discord community](https://discord.gg/crawl4ai) for early access\n\n\n* * *\nDeveloper Resources\nBuilding your own tools with Crawl4AI? Check out our [API Reference](https://docs.crawl4ai.com/api/async-webcrawler/) and [Integration Guide](https://docs.crawl4ai.com/advanced/advanced-features/) for comprehensive documentation.\n#### On this page\n  * [ðŸ› ï¸ Interactive Tools for Modern Web Scraping](https://docs.crawl4ai.com/apps/#toc-heading-0--interactive-tools-for-modern-web-scraping)\n  * [ðŸŽ¯ Available Apps](https://docs.crawl4ai.com/apps/#available-apps)\n  * [ðŸŽ¨ C4A-Script Interactive Editor](https://docs.crawl4ai.com/apps/#toc-heading-2--c4a-script-interactive-editor)\n  * [ðŸ§  LLM Context Builder](https://docs.crawl4ai.com/apps/#toc-heading-3--llm-context-builder)\n  * [ðŸ•¸ï¸ Web Scraping Playground](https://docs.crawl4ai.com/apps/#toc-heading-4--web-scraping-playground)\n  * [ðŸ” Crawl4AI Assistant (Chrome Extension)](https://docs.crawl4ai.com/apps/#toc-heading-5--crawl4ai-assistant-chrome-extension)\n  * [ðŸ§ª Extraction Lab](https://docs.crawl4ai.com/apps/#toc-heading-6--extraction-lab)\n  * [ðŸ¤– AI Prompt Designer](https://docs.crawl4ai.com/apps/#toc-heading-7--ai-prompt-designer)\n  * [ðŸ“Š Crawl Monitor](https://docs.crawl4ai.com/apps/#toc-heading-8--crawl-monitor)\n  * [ðŸš€ Why Use These Apps?](https://docs.crawl4ai.com/apps/#why-use-these-apps)\n  * [ðŸŽ¯ Accelerate Learning](https://docs.crawl4ai.com/apps/#accelerate-learning)\n  * [ðŸ’¡ Reduce Development Time](https://docs.crawl4ai.com/apps/#reduce-development-time)\n  * [ðŸ” Improve Quality](https://docs.crawl4ai.com/apps/#improve-quality)\n  * [ðŸ¤ Community Driven](https://docs.crawl4ai.com/apps/#community-driven)\n  * [ðŸ“¢ Stay Updated](https://docs.crawl4ai.com/apps/#stay-updated)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/blog",
    "depth": 1,
    "title": "Blog Home - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "0e7cf5d8fa214e7318853d37dc1a9937",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/blog/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * Blog Home\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Crawl4AI Blog](https://docs.crawl4ai.com/blog/#crawl4ai-blog)\n  * [Featured Articles](https://docs.crawl4ai.com/blog/#featured-articles)\n  * [Latest Release](https://docs.crawl4ai.com/blog/#latest-release)\n  * [Recent Releases](https://docs.crawl4ai.com/blog/#recent-releases)\n  * [Older Releases](https://docs.crawl4ai.com/blog/#older-releases)\n  * [Project History](https://docs.crawl4ai.com/blog/#project-history)\n  * [Stay Updated](https://docs.crawl4ai.com/blog/#stay-updated)\n\n\n# Crawl4AI Blog\nWelcome to the Crawl4AI blog! Here you'll find detailed release notes, technical insights, and updates about the project. Whether you're looking for the latest improvements or want to dive deep into web crawling techniques, this is the place.\n## Featured Articles\n### [When to Stop Crawling: The Art of Knowing \"Enough\"](https://docs.crawl4ai.com/blog/articles/adaptive-crawling-revolution/)\n_January 29, 2025_\nTraditional crawlers are like tourists with unlimited timeâ€”they'll visit every street, every alley, every dead end. But what if your crawler could think like a researcher with a deadline? Discover how Adaptive Crawling revolutionizes web scraping by knowing when to stop. Learn about the three-layer intelligence system that evaluates coverage, consistency, and saturation to build focused knowledge bases instead of endless page collections.\n[Read the full article â†’](https://docs.crawl4ai.com/blog/articles/adaptive-crawling-revolution/)\n### [The LLM Context Protocol: Why Your AI Assistant Needs Memory, Reasoning, and Examples](https://docs.crawl4ai.com/blog/articles/llm-context-revolution/)\n_January 24, 2025_\nEver wondered why your AI coding assistant struggles with your library despite comprehensive documentation? This article introduces the three-dimensional context protocol that transforms how AI understands code. Learn why memory, reasoning, and examples together create wisdomâ€”not just information.\n[Read the full article â†’](https://docs.crawl4ai.com/blog/articles/llm-context-revolution/)\n## Latest Release\n### [Crawl4AI v0.7.8 â€“ Stability & Bug Fix Release](https://docs.crawl4ai.com/blog/release-v0.7.8.md)\n_December 2025_\nCrawl4AI v0.7.8 is a focused stability release addressing 11 bugs reported by the community. While there are no new features, these fixes resolve important issues affecting Docker deployments, LLM extraction, URL handling, and dependency compatibility.\nKey highlights: - **ðŸ³ Docker API Fixes** : ContentRelevanceFilter deserialization, ProxyConfig serialization, cache folder permissions - **ðŸ¤– LLM Improvements** : Configurable rate limiter backoff, HTML input format support, raw HTML URL handling - **ðŸ”— URL Handling** : Correct relative URL resolution after JavaScript redirects - **ðŸ“¦ Dependencies** : Replaced deprecated PyPDF2 with pypdf, Pydantic v2 ConfigDict compatibility - **ðŸ§  AdaptiveCrawler** : Fixed query expansion to actually use LLM instead of mock data\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/release-v0.7.8.md)\n## Recent Releases\n### [Crawl4AI v0.7.7 â€“ The Self-Hosting & Monitoring Update](https://docs.crawl4ai.com/blog/release-v0.7.7.md)\n_November 14, 2025_\nCrawl4AI v0.7.7 transforms Docker into a complete self-hosting platform with enterprise-grade real-time monitoring, comprehensive observability, and full operational control.\nKey highlights: - **ðŸ“Š Real-time Monitoring Dashboard** : Interactive web UI with live system metrics - **ðŸ”Œ Comprehensive Monitor API** : Complete REST API for programmatic access - **âš¡ WebSocket Streaming** : Real-time updates every 2 seconds - **ðŸ”¥ Smart Browser Pool** : 3-tier architecture with automatic promotion and cleanup\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/release-v0.7.7.md)\n### [Crawl4AI v0.7.6 â€“ The Webhook Infrastructure Update](https://docs.crawl4ai.com/blog/release-v0.7.6.md)\n_October 22, 2025_\nCrawl4AI v0.7.6 introduces comprehensive webhook support for the Docker job queue API, bringing real-time notifications to both crawling and LLM extraction workflows. No more polling!\nKey highlights: - **ðŸª Complete Webhook Support** : Real-time notifications for both `/crawl/job` and `/llm/job` endpoints - **ðŸ”„ Reliable Delivery** : Exponential backoff retry mechanism (5 attempts: 1s â†’ 2s â†’ 4s â†’ 8s â†’ 16s) - **ðŸ” Custom Authentication** : Add custom headers for webhook authentication - **ðŸ“Š Flexible Delivery** : Choose notification-only or include full data in payload - **âš™ï¸ Global Configuration** : Set default webhook URL in config.yml for all jobs\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/release-v0.7.6.md)\n### [Crawl4AI v0.7.5 â€“ The Docker Hooks & Security Update](https://docs.crawl4ai.com/blog/release-v0.7.5.md)\n_September 29, 2025_\nCrawl4AI v0.7.5 introduces the powerful Docker Hooks System for complete pipeline customization, enhanced LLM integration with custom providers, HTTPS preservation for modern web security, and resolves multiple community-reported issues.\nKey highlights: - **ðŸ”§ Docker Hooks System** : Custom Python functions at 8 key pipeline points for unprecedented customization - **ðŸ¤– Enhanced LLM Integration** : Custom providers with temperature control and base_url configuration - **ðŸ”’ HTTPS Preservation** : Secure internal link handling for modern web applications - **ðŸ Python 3.10+ Support** : Modern language features and enhanced performance\n[Read full release notes â†’](https://docs.crawl4ai.com/blog/release-v0.7.5.md)\n* * *\n## Older Releases\nVersion | Date | Highlights  \n---|---|---  \n[v0.7.4](https://docs.crawl4ai.com/blog/release-v0.7.4.md) | August 2025 | LLM-powered table extraction, performance improvements  \n[v0.7.3](https://docs.crawl4ai.com/blog/release-v0.7.3.md) | July 2025 | Undetected browser, multi-URL config, memory monitoring  \n[v0.7.1](https://docs.crawl4ai.com/blog/release-v0.7.1.md) | June 2025 | Bug fixes and stability improvements  \n[v0.7.0](https://docs.crawl4ai.com/blog/release-v0.7.0.md) | May 2025 | Adaptive crawling, virtual scroll, link analysis  \n## Project History\nCurious about how Crawl4AI has evolved? Check out our [complete changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md) for a detailed history of all versions and updates.\n## Stay Updated\n  * Star us on [GitHub](https://github.com/unclecode/crawl4ai)\n  * Follow [@unclecode](https://twitter.com/unclecode) on Twitter\n  * Join our community discussions on GitHub\n\n\n#### On this page\n  * [Featured Articles](https://docs.crawl4ai.com/blog/#featured-articles)\n  * [When to Stop Crawling: The Art of Knowing \"Enough\"](https://docs.crawl4ai.com/blog/#when-to-stop-crawling-the-art-of-knowing-enough)\n  * [The LLM Context Protocol: Why Your AI Assistant Needs Memory, Reasoning, and Examples](https://docs.crawl4ai.com/blog/#the-llm-context-protocol-why-your-ai-assistant-needs-memory-reasoning-and-examples)\n  * [Latest Release](https://docs.crawl4ai.com/blog/#latest-release)\n  * [Crawl4AI v0.7.8 â€“ Stability & Bug Fix Release](https://docs.crawl4ai.com/blog/#crawl4ai-v078-stability-bug-fix-release)\n  * [Recent Releases](https://docs.crawl4ai.com/blog/#recent-releases)\n  * [Crawl4AI v0.7.7 â€“ The Self-Hosting & Monitoring Update](https://docs.crawl4ai.com/blog/#crawl4ai-v077-the-self-hosting-monitoring-update)\n  * [Crawl4AI v0.7.6 â€“ The Webhook Infrastructure Update](https://docs.crawl4ai.com/blog/#crawl4ai-v076-the-webhook-infrastructure-update)\n  * [Crawl4AI v0.7.5 â€“ The Docker Hooks & Security Update](https://docs.crawl4ai.com/blog/#crawl4ai-v075-the-docker-hooks-security-update)\n  * [Older Releases](https://docs.crawl4ai.com/blog/#older-releases)\n  * [Project History](https://docs.crawl4ai.com/blog/#project-history)\n  * [Stay Updated](https://docs.crawl4ai.com/blog/#stay-updated)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/apps/llmtxt",
    "depth": 1,
    "title": "Crawl4AI LLM Context Builder",
    "content_hash": "8c21aba2338fa5428605456b7ef64d29",
    "content": "![Crawl4AI Logo](https://docs.crawl4ai.com/img/favicon-32x32.png)\n# Crawl4AI LLM Context Builder\nMulti-Dimensional Context for AI Assistants\n[â† Back to Docs](https://docs.crawl4ai.com/) [All Apps](https://docs.crawl4ai.com/apps/) [GitHub](https://github.com/unclecode/crawl4ai)\n## ðŸ§  A New Approach to LLM Context\nTraditional `llm.txt` files often fail with complex libraries like Crawl4AI. They dump massive amounts of API documentation, causing **information overload** and **lost focus**. They provide the \"what\" but miss the crucial \"how\" and \"why\" that makes AI assistants truly helpful. \n### ðŸ’¡ The Solution: Multi-Dimensional, Modular Contexts\nInspired by modular libraries like Lodash, I've redesigned how we provide context to AI assistants. Instead of one monolithic file, Crawl4AI's documentation is organized by **components** and **perspectives**. \nMemory\n#### The \"What\"\nPrecise API facts, parameters, signatures, and configuration objects. Your unambiguous reference.\nReasoning\n#### The \"How\" & \"Why\"\nDesign principles, best practices, trade-offs, and workflows. Teaches AI to think like an expert.\nExamples\n#### The \"Show Me\"\nRunnable code snippets demonstrating patterns in action. Pure practical implementation.\n**Why this matters:** You can now give your AI assistant exactly what it needs - whether that's quick API lookups, help designing solutions, or seeing practical implementations. No more information overload, just focused, relevant context. \n[ðŸ“– Read the full story behind this approach â†’](https://docs.crawl4ai.com/blog/articles/llm-context-revolution)\n## Select Components & Context Types\nSelect All Deselect All\n| Component | Memory  \nFull Content | Reasoning  \nDiagrams | Examples  \nCode  \n---|---|---|---|---  \n| Installation |  1,458 tokens |  2,658 tokens |   \n| Simple Crawling |  2,390 tokens |  3,133 tokens |   \n| Configuration Objects |  7,868 tokens |  9,795 tokens |   \n| Data Extraction Using LLM |  6,775 tokens |  3,543 tokens |   \n| Data Extraction Without LLM |  6,068 tokens |  3,543 tokens |   \n| Multi URLs Crawling |  2,230 tokens |  2,853 tokens |   \n| Deep Crawling |  2,208 tokens |  3,455 tokens |   \n| Docker |  5,155 tokens |  4,308 tokens |   \n| CLI |  2,373 tokens |  3,350 tokens |   \n| HTTP-based Crawler |  2,390 tokens |  3,413 tokens |   \n| URL Seeder |  4,745 tokens |  3,080 tokens |   \n| Advanced Filters & Scorers |  2,713 tokens |  3,030 tokens |   \nEstimated Tokens: 4,116\nâ¬‡ Generate & Download Context \n## Available Context Files\nComponent | Memory | Reasoning | Examples | Full  \n---|---|---|---|---  \n**Installation** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/installation.txt) 1,458 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/installation.txt) 2,658 tokens |  -  | -  \n**Simple Crawling** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/simple_crawling.txt) 2,390 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/simple_crawling.txt) 3,133 tokens |  -  | -  \n**Configuration Objects** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/config_objects.txt) 7,868 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/config_objects.txt) 9,795 tokens |  -  | -  \n**Data Extraction Using LLM** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/extraction-llm.txt) 6,775 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/extraction-llm.txt) 3,543 tokens |  -  | -  \n**Data Extraction Without LLM** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/extraction-no-llm.txt) 6,068 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/extraction-no-llm.txt) 3,543 tokens |  -  | -  \n**Multi URLs Crawling** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/multi_urls_crawling.txt) 2,230 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/multi_urls_crawling.txt) 2,853 tokens |  -  | -  \n**Deep Crawling** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/deep_crawling.txt) 2,208 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/deep_crawling.txt) 3,455 tokens |  -  | -  \n**Docker** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/docker.txt) 5,155 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/docker.txt) 4,308 tokens |  -  | -  \n**CLI** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/cli.txt) 2,373 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/cli.txt) 3,350 tokens |  -  | -  \n**HTTP-based Crawler** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/http_based_crawler_strategy.txt) 2,390 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/http_based_crawler_strategy.txt) 3,413 tokens |  -  | -  \n**URL Seeder** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/url_seeder.txt) 4,745 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/url_seeder.txt) 3,080 tokens |  -  | -  \n**Advanced Filters & Scorers** |  [Memory](https://docs.crawl4ai.com/assets/llm.txt/txt/deep_crawl_advanced_filters_scorers.txt) 2,713 tokens |  [Reasoning](https://docs.crawl4ai.com/assets/llm.txt/diagrams/deep_crawl_advanced_filters_scorers.txt) 3,030 tokens |  -  | -\n"
  },
  {
    "url": "https://docs.crawl4ai.com/branding",
    "depth": 1,
    "title": "Brand Book - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "b17670e03157948550994089cf383b56",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/branding/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * Brand Book\n\n\n* * *\n  * [ðŸŽ¨ Crawl4AI Brand Book](https://docs.crawl4ai.com/branding/#crawl4ai-brand-book)\n  * [ðŸ“– About This Guide](https://docs.crawl4ai.com/branding/#about-this-guide)\n\n\n# ðŸŽ¨ Crawl4AI Brand Book\n# Crawl4AI Brand Guidelines\nA comprehensive design system for building consistent, terminal-inspired experiences\n## ðŸ“– About This Guide\nThis brand book documents the complete visual language of Crawl4AI. Whether you're building documentation pages, interactive apps, or Chrome extensions, these guidelines ensure consistency while maintaining the unique terminal-aesthetic that defines our brand.\n* * *\nðŸŽ¨\n## Color System\nOur color palette is built around a terminal-dark aesthetic with vibrant cyan and pink accents. Every color serves a purpose and maintains accessibility standards.\n### Primary Colors\nPrimary Cyan\n`#50ffff`\nMain brand color, links, highlights, CTAs\nPrimary Teal\n`#09b5a5`\nHover states, dimmed accents, progress bars\nPrimary Green\n`#0fbbaa`\nAlternative primary, buttons, nav links\nAccent Pink\n`#f380f5`\nSecondary accents, keywords, highlights\n### Background Colors\nDeep Black\n`#070708`\nMain background, code blocks, deep containers\nSecondary Dark\n`#1a1a1a`\nHeaders, sidebars, secondary containers\nTertiary Gray\n`#3f3f44`\nCards, borders, code backgrounds, modals\nBlock Background\n`#202020`\nBlock elements, alternate rows\n### Text Colors\nPrimary Text\n`#e8e9ed`\nHeadings, body text, primary content\nSecondary Text\n`#d5cec0`\nBody text, descriptions, warm tone\nTertiary Text\n`#a3abba`\nCaptions, labels, metadata, cool tone\nDimmed Text\n`#8b857a`\nDisabled states, comments, subtle text\n### Semantic Colors\nSuccess Green\n`#50ff50`\nSuccess messages, completed states, valid\nError Red\n`#ff3c74`\nErrors, warnings, destructive actions\nWarning Orange\n`#f59e0b`\nWarnings, beta status, caution\nInfo Blue\n`#4a9eff`\nInfo messages, external links\n* * *\nâœï¸\n## Typography\nOur typography system is built around **Dank Mono** , a monospace font that reinforces the terminal aesthetic while maintaining excellent readability.\n### Font Family\n```\n--font-primary: 'Dank Mono', dm, Monaco, Courier New, monospace;\n--font-code: 'Dank Mono', 'Monaco', 'Menlo', 'Consolas', monospace;\nCopy\n```\n\n**Font Weights:** - Regular: 400 - Bold: 700 - Italic: 400 (italic variant)\n### Type Scale\nH1 / Hero\nSize: 2.5rem (40px) Weight: 700 Line-height: 1.2\n# The Quick Brown Fox Jumps Over\nH2 / Section\nSize: 1.75rem (28px) Weight: 700 Line-height: 1.3\n## Advanced Web Scraping Features\nH3 / Subsection\nSize: 1.3rem (20.8px) Weight: 600 Line-height: 1.4\n### Installation and Setup Guide\nH4 / Component\nSize: 1.1rem (17.6px) Weight: 600 Line-height: 1.4\n#### Quick Start Instructions\nBody / Regular\nSize: 14px Weight: 400 Line-height: 1.6\nCrawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models and data pipelines.\nCode / Monospace\nSize: 13px Weight: 400 Line-height: 1.5\n`async with AsyncWebCrawler() as crawler:`\nSmall / Caption\nSize: 12px Weight: 400 Line-height: 1.5\nUpdated 2 hours ago â€¢ v0.7.2\n* * *\nðŸ§©\n## Components\n### Buttons\n### Primary Button\nLaunch Editor â†’ Processing...\nHTML + CSS Copy\n```\n<button class=\"brand-btn brand-btn-primary\">\n  Launch Editor â†’\n</button>Copy\n```\n\n### Secondary Button\nView Documentation Loading...\nHTML + CSS Copy\n```\n<button class=\"brand-btn brand-btn-secondary\">\n  View Documentation\n</button>Copy\n```\n\n### Accent Button\nTry Beta Features Unavailable\nHTML + CSS Copy\n```\n<button class=\"brand-btn brand-btn-accent\">\n  Try Beta Features\n</button>Copy\n```\n\n### Ghost Button\nLearn More Coming Soon\nHTML + CSS Copy\n```\n<button class=\"brand-btn brand-btn-ghost\">\n  Learn More\n</button>Copy\n```\n\n### Badges & Status Indicators\n### Status Badges\nAvailable Beta Alpha New! Coming Soon\nHTML + CSS Copy\n```\n<span class=\"brand-badge badge-available\">Available</span>\n<span class=\"brand-badge badge-beta\">Beta</span>\n<span class=\"brand-badge badge-alpha\">Alpha</span>\n<span class=\"brand-badge badge-new\">New!</span>Copy\n```\n\n### Cards\n### ðŸŽ¨ C4A-Script Editor\nA visual, block-based programming environment for creating browser automation scripts. Perfect for beginners and experts alike!\nLaunch Editor â†’\n### ðŸ§  LLM Context Builder\nGenerate optimized context files for your favorite LLM when working with Crawl4AI. Get focused, relevant documentation based on your needs.\nLaunch Builder â†’\nHTML + CSS Copy\n```\n<div class=\"brand-card\">\n  <h3 class=\"brand-card-title\">Card Title</h3>\n  <p class=\"brand-card-description\">Card description...</p>\n</div>Copy\n```\n\n### Terminal Window\ncrawl4ai@terminal ~ %\n$ pip install crawl4ai\nSuccessfully installed crawl4ai-0.7.2\nHTML + CSS Copy\n```\n<div class=\"terminal-window\">\n  <div class=\"terminal-header\">\n    <div class=\"terminal-dots\">\n      <span class=\"terminal-dot red\"></span>\n      <span class=\"terminal-dot yellow\"></span>\n      <span class=\"terminal-dot green\"></span>\n    </div>\n    <span class=\"terminal-title\">Terminal Title</span>\n  </div>\n  <div class=\"terminal-content\">\n    Your content here\n  </div>\n</div>Copy\n```\n\n* * *\nðŸ“\n## Spacing & Layout\n### Spacing System\nOur spacing system is based on multiples of **10px** for consistency and ease of calculation.\n10px - Extra Small (xs)\n20px - Small (sm)\n30px - Medium (md)\n40px - Large (lg)\n60px - Extra Large (xl)\n80px - 2XL\n### Layout Patterns\n#### Terminal Container\nFull-height, flex-column layout with sticky header\n```\n.terminal-container {\n    min-height: 100vh;\n    display: flex;\n    flex-direction: column;\n}\nCopy\n```\n\n#### Content Grid\nAuto-fit responsive grid for cards and components\n```\n.component-grid {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n    gap: 2rem;\n}\nCopy\n```\n\n#### Centered Content\nMaximum width with auto margins for centered layouts\n```\n.content {\n    max-width: 1200px;\n    margin: 0 auto;\n    padding: 2rem;\n}\nCopy\n```\n\n* * *\nâœ…\n## Usage Guidelines\n### When to Use Each Style\n**Documentation Pages (`docs/md_v2/core` , `/advanced`, etc.)** - Use main documentation styles from `styles.css` and `layout.css` - Terminal theme with sidebar navigation - Dense, informative content - ToC on the right side - Focus on readability and technical accuracy\n**Landing Pages (`docs/md_v2/apps/crawl4ai-assistant` , etc.)** - Use `assistant.css` style approach - Hero sections with gradients - Feature cards with hover effects - Video/demo sections - Sticky header with navigation - Marketing-focused, visually engaging\n**App Home (`docs/md_v2/apps/index.md`)** - Grid-based card layouts - Status badges - Call-to-action buttons - Feature highlights - Mix of informational and promotional\n**Interactive Apps (`docs/md_v2/apps/llmtxt` , `/c4a-script`)** - Full-screen application layouts - Interactive controls - Real-time feedback - Tool-specific UI patterns - Functional over decorative\n**Chrome Extension (`popup.css`)** - Compact, fixed-width design (380px) - Clear mode selection - Session indicators - Minimal but effective - Fast loading, no heavy assets\n### Do's and Don'ts\nâœ… DO\nLaunch App â†’\nUse primary cyan for main CTAs and important actions \nâŒ DON'T\nLaunch App â†’\nDon't use arbitrary colors not in the brand palette \nâœ… DO\n`async with AsyncWebCrawler():`\nUse Dank Mono for all text to maintain terminal aesthetic \nâŒ DON'T\nasync with AsyncWebCrawler(): \nDon't use non-monospace fonts (breaks terminal feel) \nâœ… DO\nBeta\n#### New Feature\nUse status badges to indicate feature maturity \nâŒ DON'T\n#### New Feature (Beta)\nDon't put status indicators in plain text \n* * *\nðŸŽ¯\n## Accessibility\n### Color Contrast\nAll color combinations meet WCAG AA standards:\n  * **Primary Cyan (#50ffff) on Dark (#070708)** : 12.4:1 âœ…\n  * **Primary Text (#e8e9ed) on Dark (#070708)** : 11.8:1 âœ…\n  * **Secondary Text (#d5cec0) on Dark (#070708)** : 9.2:1 âœ…\n  * **Tertiary Text (#a3abba) on Dark (#070708)** : 6.8:1 âœ…\n\n\n### Focus States\nAll interactive elements must have visible focus indicators:\n```\nbutton:focus,\na:focus {\n    outline: 2px solid #50ffff;\n    outline-offset: 2px;\n}\nCopy\n```\n\n### Motion\nRespect `prefers-reduced-motion` for users who need it:\n```\n@media (prefers-reduced-motion: reduce) {\n    * {\n        animation-duration: 0.01ms !important;\n        transition-duration: 0.01ms !important;\n    }\n}\nCopy\n```\n\n* * *\nðŸ’¾\n## CSS Variables\nUse these CSS variables for consistency across all styles:\n```\n:root {\n    /* Colors */\n    --primary-color: #50ffff;\n    --primary-dimmed: #09b5a5;\n    --primary-green: #0fbbaa;\n    --accent-color: #f380f5;\n\n    /* Backgrounds */\n    --background-color: #070708;\n    --bg-secondary: #1a1a1a;\n    --code-bg-color: #3f3f44;\n    --border-color: #3f3f44;\n\n    /* Text */\n    --font-color: #e8e9ed;\n    --secondary-color: #d5cec0;\n    --tertiary-color: #a3abba;\n\n    /* Semantic */\n    --success-color: #50ff50;\n    --error-color: #ff3c74;\n    --warning-color: #f59e0b;\n\n    /* Typography */\n    --font-primary: 'Dank Mono', dm, Monaco, Courier New, monospace;\n    --global-font-size: 14px;\n    --global-line-height: 1.6;\n\n    /* Spacing */\n    --global-space: 10px;\n\n    /* Layout */\n    --header-height: 65px;\n    --sidebar-width: 280px;\n    --toc-width: 340px;\n    --content-max-width: 90em;\n}\nCopy\n```\n\n* * *\nðŸ“š\n## Resources\n### Download Assets\n  * [Dank Mono Font Files](https://docs.crawl4ai.com/docs/md_v2/assets/) (Regular, Bold, Italic)\n  * [Brand CSS Template](https://docs.crawl4ai.com/docs/md_v2/branding/assets/brand-examples.css)\n  * [Component Library](https://docs.crawl4ai.com/docs/md_v2/apps/)\n\n\n### Reference Files\n  * Main Documentation Styles: `docs/md_v2/assets/styles.css`\n  * Layout System: `docs/md_v2/assets/layout.css`\n  * Landing Page Style: `docs/md_v2/apps/crawl4ai-assistant/assistant.css`\n  * App Home Style: `docs/md_v2/apps/index.md`\n  * Extension Style: `docs/md_v2/apps/crawl4ai-assistant/popup/popup.css`\n\n\n### Questions?\nIf you're unsure about which style to use or need help implementing these guidelines:\n  * Check existing examples in the relevant section\n  * Review the \"When to Use Each Style\" guidelines above\n  * Ask in our [Discord community](https://discord.gg/crawl4ai)\n  * Open an issue on [GitHub](https://github.com/unclecode/crawl4ai)\n\n\n* * *\n### ðŸŽ¨ Keep It Terminal\nWhen in doubt, ask yourself: \"Does this feel like a terminal?\" If yes, you're on brand. \n#### On this page\n  * [ðŸ“– About This Guide](https://docs.crawl4ai.com/branding/#about-this-guide)\n  * [Color System](https://docs.crawl4ai.com/branding/#toc-heading-1-color-system)\n  * [Primary Colors](https://docs.crawl4ai.com/branding/#primary-colors)\n  * [Background Colors](https://docs.crawl4ai.com/branding/#background-colors)\n  * [Text Colors](https://docs.crawl4ai.com/branding/#text-colors)\n  * [Semantic Colors](https://docs.crawl4ai.com/branding/#semantic-colors)\n  * [Typography](https://docs.crawl4ai.com/branding/#toc-heading-6-typography)\n  * [Font Family](https://docs.crawl4ai.com/branding/#font-family)\n  * [Type Scale](https://docs.crawl4ai.com/branding/#type-scale)\n  * [Advanced Web Scraping Features](https://docs.crawl4ai.com/branding/#toc-heading-9-advanced-web-scraping-features)\n  * [Installation and Setup Guide](https://docs.crawl4ai.com/branding/#toc-heading-10-installation-and-setup-guide)\n  * [Quick Start Instructions](https://docs.crawl4ai.com/branding/#toc-heading-11-quick-start-instructions)\n  * [Components](https://docs.crawl4ai.com/branding/#toc-heading-12-components)\n  * [Buttons](https://docs.crawl4ai.com/branding/#buttons)\n  * [Primary Button](https://docs.crawl4ai.com/branding/#toc-heading-14-primary-button)\n  * [Secondary Button](https://docs.crawl4ai.com/branding/#toc-heading-15-secondary-button)\n  * [Accent Button](https://docs.crawl4ai.com/branding/#toc-heading-16-accent-button)\n  * [Ghost Button](https://docs.crawl4ai.com/branding/#toc-heading-17-ghost-button)\n  * [Badges & Status Indicators](https://docs.crawl4ai.com/branding/#badges-status-indicators)\n  * [Status Badges](https://docs.crawl4ai.com/branding/#toc-heading-19-status-badges)\n  * [Cards](https://docs.crawl4ai.com/branding/#cards)\n  * [ðŸŽ¨ C4A-Script Editor](https://docs.crawl4ai.com/branding/#toc-heading-21--c4a-script-editor)\n  * [ðŸ§  LLM Context Builder](https://docs.crawl4ai.com/branding/#toc-heading-22--llm-context-builder)\n  * [Terminal Window](https://docs.crawl4ai.com/branding/#terminal-window)\n  * [Spacing & Layout](https://docs.crawl4ai.com/branding/#toc-heading-24-spacing--layout)\n  * [Spacing System](https://docs.crawl4ai.com/branding/#spacing-system)\n  * [Layout Patterns](https://docs.crawl4ai.com/branding/#layout-patterns)\n  * [Terminal Container](https://docs.crawl4ai.com/branding/#terminal-container)\n  * [Content Grid](https://docs.crawl4ai.com/branding/#content-grid)\n  * [Centered Content](https://docs.crawl4ai.com/branding/#centered-content)\n  * [Usage Guidelines](https://docs.crawl4ai.com/branding/#toc-heading-30-usage-guidelines)\n  * [When to Use Each Style](https://docs.crawl4ai.com/branding/#when-to-use-each-style)\n  * [Do's and Don'ts](https://docs.crawl4ai.com/branding/#dos-and-donts)\n  * [New Feature](https://docs.crawl4ai.com/branding/#toc-heading-33-new-feature)\n  * [New Feature (Beta)](https://docs.crawl4ai.com/branding/#toc-heading-34-new-feature-beta)\n  * [Accessibility](https://docs.crawl4ai.com/branding/#toc-heading-35-accessibility)\n  * [Color Contrast](https://docs.crawl4ai.com/branding/#color-contrast)\n  * [Focus States](https://docs.crawl4ai.com/branding/#focus-states)\n  * [Motion](https://docs.crawl4ai.com/branding/#motion)\n  * [CSS Variables](https://docs.crawl4ai.com/branding/#toc-heading-39-css-variables)\n  * [Resources](https://docs.crawl4ai.com/branding/#toc-heading-40-resources)\n  * [Download Assets](https://docs.crawl4ai.com/branding/#download-assets)\n  * [Reference Files](https://docs.crawl4ai.com/branding/#reference-files)\n  * [Questions?](https://docs.crawl4ai.com/branding/#questions)\n  * [ðŸŽ¨ Keep It Terminal](https://docs.crawl4ai.com/branding/#toc-heading-44--keep-it-terminal)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/complete-sdk-reference",
    "depth": 1,
    "title": "ðŸ“š Complete SDK Reference - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "ef7013ab2d68fee8050af48d4db03871",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/complete-sdk-reference/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * ðŸ“š Complete SDK Reference\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Crawl4AI Complete SDK Documentation](https://docs.crawl4ai.com/complete-sdk-reference/#crawl4ai-complete-sdk-documentation)\n  * [Navigation](https://docs.crawl4ai.com/complete-sdk-reference/#navigation)\n  * [Installation & Setup](https://docs.crawl4ai.com/complete-sdk-reference/#installation-setup)\n  * [Installation & Setup (2023 Edition)](https://docs.crawl4ai.com/complete-sdk-reference/#installation-setup-2023-edition)\n  * [1. Basic Installation](https://docs.crawl4ai.com/complete-sdk-reference/#1-basic-installation)\n  * [2. Initial Setup & Diagnostics](https://docs.crawl4ai.com/complete-sdk-reference/#2-initial-setup-diagnostics)\n  * [3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor)](https://docs.crawl4ai.com/complete-sdk-reference/#3-verifying-installation-a-simple-crawl-skip-this-step-if-you-already-run-crawl4ai-doctor)\n  * [4. Advanced Installation (Optional)](https://docs.crawl4ai.com/complete-sdk-reference/#4-advanced-installation-optional)\n  * [5. Docker (Experimental)](https://docs.crawl4ai.com/complete-sdk-reference/#5-docker-experimental)\n  * [6. Local Server Mode (Legacy)](https://docs.crawl4ai.com/complete-sdk-reference/#6-local-server-mode-legacy)\n  * [Summary](https://docs.crawl4ai.com/complete-sdk-reference/#summary)\n  * [Quick Start](https://docs.crawl4ai.com/complete-sdk-reference/#quick-start)\n  * [Getting Started with Crawl4AI](https://docs.crawl4ai.com/complete-sdk-reference/#getting-started-with-crawl4ai)\n  * [1. Introduction](https://docs.crawl4ai.com/complete-sdk-reference/#1-introduction)\n  * [2. Your First Crawl](https://docs.crawl4ai.com/complete-sdk-reference/#2-your-first-crawl)\n  * [3. Basic Configuration (Light Introduction)](https://docs.crawl4ai.com/complete-sdk-reference/#3-basic-configuration-light-introduction)\n  * [4. Generating Markdown Output](https://docs.crawl4ai.com/complete-sdk-reference/#4-generating-markdown-output)\n  * [5. Simple Data Extraction (CSS-based)](https://docs.crawl4ai.com/complete-sdk-reference/#5-simple-data-extraction-css-based)\n  * [6. Simple Data Extraction (LLM-based)](https://docs.crawl4ai.com/complete-sdk-reference/#6-simple-data-extraction-llm-based)\n  * [7. Adaptive Crawling (New!)](https://docs.crawl4ai.com/complete-sdk-reference/#7-adaptive-crawling-new)\n  * [8. Multi-URL Concurrency (Preview)](https://docs.crawl4ai.com/complete-sdk-reference/#8-multi-url-concurrency-preview)\n  * [8. Dynamic Content Example](https://docs.crawl4ai.com/complete-sdk-reference/#8-dynamic-content-example)\n  * [9. Next Steps](https://docs.crawl4ai.com/complete-sdk-reference/#9-next-steps)\n  * [Core API](https://docs.crawl4ai.com/complete-sdk-reference/#core-api)\n  * [AsyncWebCrawler](https://docs.crawl4ai.com/complete-sdk-reference/#asyncwebcrawler)\n  * [1. Constructor Overview](https://docs.crawl4ai.com/complete-sdk-reference/#1-constructor-overview)\n  * [2. Lifecycle: Start/Close or Context Manager](https://docs.crawl4ai.com/complete-sdk-reference/#2-lifecycle-startclose-or-context-manager)\n  * [3. Primary Method: arun()](https://docs.crawl4ai.com/complete-sdk-reference/#3-primary-method-arun)\n  * [4. Batch Processing: arun_many()](https://docs.crawl4ai.com/complete-sdk-reference/#4-batch-processing-arun_many)\n  * [7. Best Practices & Migration Notes](https://docs.crawl4ai.com/complete-sdk-reference/#7-best-practices-migration-notes)\n  * [8. Summary](https://docs.crawl4ai.com/complete-sdk-reference/#8-summary)\n  * [arun() Parameter Guide (New Approach)](https://docs.crawl4ai.com/complete-sdk-reference/#arun-parameter-guide-new-approach)\n  * [1. Core Usage](https://docs.crawl4ai.com/complete-sdk-reference/#1-core-usage)\n  * [2. Cache Control](https://docs.crawl4ai.com/complete-sdk-reference/#2-cache-control)\n  * [3. Content Processing & Selection](https://docs.crawl4ai.com/complete-sdk-reference/#3-content-processing-selection)\n  * [4. Page Navigation & Timing](https://docs.crawl4ai.com/complete-sdk-reference/#4-page-navigation-timing)\n  * [5. Session Management](https://docs.crawl4ai.com/complete-sdk-reference/#5-session-management)\n  * [6. Screenshot, PDF & Media Options](https://docs.crawl4ai.com/complete-sdk-reference/#6-screenshot-pdf-media-options)\n  * [7. Extraction Strategy](https://docs.crawl4ai.com/complete-sdk-reference/#7-extraction-strategy)\n  * [8. Comprehensive Example](https://docs.crawl4ai.com/complete-sdk-reference/#8-comprehensive-example)\n  * [9. Best Practices](https://docs.crawl4ai.com/complete-sdk-reference/#9-best-practices)\n  * [10. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#10-conclusion)\n  * [arun_many(...) Reference](https://docs.crawl4ai.com/complete-sdk-reference/#arun_many-reference)\n  * [Function Signature](https://docs.crawl4ai.com/complete-sdk-reference/#function-signature)\n  * [Differences from arun()](https://docs.crawl4ai.com/complete-sdk-reference/#differences-from-arun)\n  * [Dispatcher Reference](https://docs.crawl4ai.com/complete-sdk-reference/#dispatcher-reference)\n  * [Common Pitfalls](https://docs.crawl4ai.com/complete-sdk-reference/#common-pitfalls)\n  * [Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#conclusion)\n  * [CrawlResult Reference](https://docs.crawl4ai.com/complete-sdk-reference/#crawlresult-reference)\n  * [1. Basic Crawl Info](https://docs.crawl4ai.com/complete-sdk-reference/#1-basic-crawl-info)\n  * [2. Raw / Cleaned Content](https://docs.crawl4ai.com/complete-sdk-reference/#2-raw-cleaned-content)\n  * [3. Markdown Fields](https://docs.crawl4ai.com/complete-sdk-reference/#3-markdown-fields)\n  * [4. Media & Links](https://docs.crawl4ai.com/complete-sdk-reference/#4-media-links)\n  * [5. Additional Fields](https://docs.crawl4ai.com/complete-sdk-reference/#5-additional-fields)\n  * [6. dispatch_result (optional)](https://docs.crawl4ai.com/complete-sdk-reference/#6-dispatch_result-optional)\n  * [7. Network Requests & Console Messages](https://docs.crawl4ai.com/complete-sdk-reference/#7-network-requests-console-messages)\n  * [8. Example: Accessing Everything](https://docs.crawl4ai.com/complete-sdk-reference/#8-example-accessing-everything)\n  * [9. Key Points & Future](https://docs.crawl4ai.com/complete-sdk-reference/#9-key-points-future)\n  * [Configuration](https://docs.crawl4ai.com/complete-sdk-reference/#configuration)\n  * [Browser, Crawler & LLM Configuration (Quick Overview)](https://docs.crawl4ai.com/complete-sdk-reference/#browser-crawler-llm-configuration-quick-overview)\n  * [1. BrowserConfig Essentials](https://docs.crawl4ai.com/complete-sdk-reference/#1-browserconfig-essentials)\n  * [2. CrawlerRunConfig Essentials](https://docs.crawl4ai.com/complete-sdk-reference/#2-crawlerrunconfig-essentials)\n  * [3. LLMConfig Essentials](https://docs.crawl4ai.com/complete-sdk-reference/#3-llmconfig-essentials)\n  * [4. Putting It All Together](https://docs.crawl4ai.com/complete-sdk-reference/#4-putting-it-all-together)\n  * [5. Next Steps](https://docs.crawl4ai.com/complete-sdk-reference/#5-next-steps)\n  * [6. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#6-conclusion)\n  * [1. BrowserConfig â€“ Controlling the Browser](https://docs.crawl4ai.com/complete-sdk-reference/#1-browserconfig-controlling-the-browser)\n  * [1.1 Parameter Highlights](https://docs.crawl4ai.com/complete-sdk-reference/#11-parameter-highlights)\n  * [Crawling Patterns](https://docs.crawl4ai.com/complete-sdk-reference/#crawling-patterns)\n  * [Simple Crawling](https://docs.crawl4ai.com/complete-sdk-reference/#simple-crawling)\n  * [Basic Usage](https://docs.crawl4ai.com/complete-sdk-reference/#basic-usage)\n  * [Understanding the Response](https://docs.crawl4ai.com/complete-sdk-reference/#understanding-the-response)\n  * [Adding Basic Options](https://docs.crawl4ai.com/complete-sdk-reference/#adding-basic-options)\n  * [Handling Errors](https://docs.crawl4ai.com/complete-sdk-reference/#handling-errors)\n  * [Logging and Debugging](https://docs.crawl4ai.com/complete-sdk-reference/#logging-and-debugging)\n  * [Complete Example](https://docs.crawl4ai.com/complete-sdk-reference/#complete-example)\n  * [Content Processing](https://docs.crawl4ai.com/complete-sdk-reference/#content-processing)\n  * [Markdown Generation Basics](https://docs.crawl4ai.com/complete-sdk-reference/#markdown-generation-basics)\n  * [1. Quick Example](https://docs.crawl4ai.com/complete-sdk-reference/#1-quick-example)\n  * [2. How Markdown Generation Works](https://docs.crawl4ai.com/complete-sdk-reference/#2-how-markdown-generation-works)\n  * [3. Configuring the Default Markdown Generator](https://docs.crawl4ai.com/complete-sdk-reference/#3-configuring-the-default-markdown-generator)\n  * [4. Selecting the HTML Source for Markdown Generation](https://docs.crawl4ai.com/complete-sdk-reference/#4-selecting-the-html-source-for-markdown-generation)\n  * [5. Content Filters](https://docs.crawl4ai.com/complete-sdk-reference/#5-content-filters)\n  * [6. Using Fit Markdown](https://docs.crawl4ai.com/complete-sdk-reference/#6-using-fit-markdown)\n  * [7. The MarkdownGenerationResult Object](https://docs.crawl4ai.com/complete-sdk-reference/#7-the-markdowngenerationresult-object)\n  * [8. Combining Filters (BM25 + Pruning) in Two Passes](https://docs.crawl4ai.com/complete-sdk-reference/#8-combining-filters-bm25-pruning-in-two-passes)\n  * [9. Common Pitfalls & Tips](https://docs.crawl4ai.com/complete-sdk-reference/#9-common-pitfalls-tips)\n  * [10. Summary & Next Steps](https://docs.crawl4ai.com/complete-sdk-reference/#10-summary-next-steps)\n  * [Fit Markdown with Pruning & BM25](https://docs.crawl4ai.com/complete-sdk-reference/#fit-markdown-with-pruning-bm25)\n  * [1. How â€œFit Markdownâ€ Works](https://docs.crawl4ai.com/complete-sdk-reference/#1-how-fit-markdown-works)\n  * [2. PruningContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#2-pruningcontentfilter)\n  * [3. BM25ContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#3-bm25contentfilter)\n  * [4. Accessing the â€œFitâ€ Output](https://docs.crawl4ai.com/complete-sdk-reference/#4-accessing-the-fit-output)\n  * [5. Code Patterns Recap](https://docs.crawl4ai.com/complete-sdk-reference/#5-code-patterns-recap)\n  * [6. Combining with â€œword_count_thresholdâ€ & Exclusions](https://docs.crawl4ai.com/complete-sdk-reference/#6-combining-with-word_count_threshold-exclusions)\n  * [7. Custom Filters](https://docs.crawl4ai.com/complete-sdk-reference/#7-custom-filters)\n  * [8. Final Thoughts](https://docs.crawl4ai.com/complete-sdk-reference/#8-final-thoughts)\n  * [Content Selection](https://docs.crawl4ai.com/complete-sdk-reference/#content-selection)\n  * [1. CSS-Based Selection](https://docs.crawl4ai.com/complete-sdk-reference/#1-css-based-selection)\n  * [2. Content Filtering & Exclusions](https://docs.crawl4ai.com/complete-sdk-reference/#2-content-filtering-exclusions)\n  * [3. Handling Iframes](https://docs.crawl4ai.com/complete-sdk-reference/#3-handling-iframes)\n  * [4. Structured Extraction Examples](https://docs.crawl4ai.com/complete-sdk-reference/#4-structured-extraction-examples)\n  * [5. Comprehensive Example](https://docs.crawl4ai.com/complete-sdk-reference/#5-comprehensive-example)\n  * [6. Scraping Modes](https://docs.crawl4ai.com/complete-sdk-reference/#6-scraping-modes)\n  * [7. Combining CSS Selection Methods](https://docs.crawl4ai.com/complete-sdk-reference/#7-combining-css-selection-methods)\n  * [8. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#8-conclusion)\n  * [Page Interaction](https://docs.crawl4ai.com/complete-sdk-reference/#page-interaction)\n  * [1. JavaScript Execution](https://docs.crawl4ai.com/complete-sdk-reference/#1-javascript-execution)\n  * [2. Wait Conditions](https://docs.crawl4ai.com/complete-sdk-reference/#2-wait-conditions)\n  * [3. Handling Dynamic Content](https://docs.crawl4ai.com/complete-sdk-reference/#3-handling-dynamic-content)\n  * [4. Timing Control](https://docs.crawl4ai.com/complete-sdk-reference/#4-timing-control)\n  * [5. Multi-Step Interaction Example](https://docs.crawl4ai.com/complete-sdk-reference/#5-multi-step-interaction-example)\n  * [6. Combine Interaction with Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#6-combine-interaction-with-extraction)\n  * [7. Relevant CrawlerRunConfig Parameters](https://docs.crawl4ai.com/complete-sdk-reference/#7-relevant-crawlerrunconfig-parameters)\n  * [8. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#8-conclusion_1)\n  * [9. Virtual Scrolling](https://docs.crawl4ai.com/complete-sdk-reference/#9-virtual-scrolling)\n  * [Link & Media](https://docs.crawl4ai.com/complete-sdk-reference/#link-media)\n  * [1. Link Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#1-link-extraction)\n  * [2. Advanced Link Head Extraction & Scoring](https://docs.crawl4ai.com/complete-sdk-reference/#2-advanced-link-head-extraction-scoring)\n  * [3. Domain Filtering](https://docs.crawl4ai.com/complete-sdk-reference/#3-domain-filtering)\n  * [4. Media Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#4-media-extraction)\n  * [5. Putting It All Together: Link & Media Filtering](https://docs.crawl4ai.com/complete-sdk-reference/#5-putting-it-all-together-link-media-filtering)\n  * [6. Common Pitfalls & Tips](https://docs.crawl4ai.com/complete-sdk-reference/#6-common-pitfalls-tips)\n  * [Extraction Strategies](https://docs.crawl4ai.com/complete-sdk-reference/#extraction-strategies)\n  * [Extracting JSON (No LLM)](https://docs.crawl4ai.com/complete-sdk-reference/#extracting-json-no-llm)\n  * [1. Intro to Schema-Based Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#1-intro-to-schema-based-extraction)\n  * [2. Simple Example: Crypto Prices](https://docs.crawl4ai.com/complete-sdk-reference/#2-simple-example-crypto-prices)\n  * [3. Advanced Schema & Nested Structures](https://docs.crawl4ai.com/complete-sdk-reference/#3-advanced-schema-nested-structures)\n  * [4. RegexExtractionStrategy - Fast Pattern-Based Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#4-regexextractionstrategy-fast-pattern-based-extraction)\n  * [5. Why \"No LLM\" Is Often Better](https://docs.crawl4ai.com/complete-sdk-reference/#5-why-no-llm-is-often-better)\n  * [6. Base Element Attributes & Additional Fields](https://docs.crawl4ai.com/complete-sdk-reference/#6-base-element-attributes-additional-fields)\n  * [7. Putting It All Together: Larger Example](https://docs.crawl4ai.com/complete-sdk-reference/#7-putting-it-all-together-larger-example)\n  * [8. Tips & Best Practices](https://docs.crawl4ai.com/complete-sdk-reference/#8-tips-best-practices)\n  * [9. Schema Generation Utility](https://docs.crawl4ai.com/complete-sdk-reference/#9-schema-generation-utility)\n  * [10. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#10-conclusion_1)\n  * [Extracting JSON (LLM)](https://docs.crawl4ai.com/complete-sdk-reference/#extracting-json-llm)\n  * [1. Why Use an LLM?](https://docs.crawl4ai.com/complete-sdk-reference/#1-why-use-an-llm)\n  * [2. Provider-Agnostic via LiteLLM](https://docs.crawl4ai.com/complete-sdk-reference/#2-provider-agnostic-via-litellm)\n  * [3. How LLM Extraction Works](https://docs.crawl4ai.com/complete-sdk-reference/#3-how-llm-extraction-works)\n  * [4. Key Parameters](https://docs.crawl4ai.com/complete-sdk-reference/#4-key-parameters)\n  * [5. Putting It in CrawlerRunConfig](https://docs.crawl4ai.com/complete-sdk-reference/#5-putting-it-in-crawlerrunconfig)\n  * [6. Chunking Details](https://docs.crawl4ai.com/complete-sdk-reference/#6-chunking-details)\n  * [7. Input Format](https://docs.crawl4ai.com/complete-sdk-reference/#7-input-format)\n  * [8. Token Usage & Show Usage](https://docs.crawl4ai.com/complete-sdk-reference/#8-token-usage-show-usage)\n  * [9. Example: Building a Knowledge Graph](https://docs.crawl4ai.com/complete-sdk-reference/#9-example-building-a-knowledge-graph)\n  * [10. Best Practices & Caveats](https://docs.crawl4ai.com/complete-sdk-reference/#10-best-practices-caveats)\n  * [11. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#11-conclusion)\n  * [Advanced Features](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-features)\n  * [Session Management](https://docs.crawl4ai.com/complete-sdk-reference/#session-management)\n  * [Example 1: Basic Session-Based Crawling](https://docs.crawl4ai.com/complete-sdk-reference/#example-1-basic-session-based-crawling)\n  * [Advanced Technique 1: Custom Execution Hooks](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-technique-1-custom-execution-hooks)\n  * [Advanced Technique 2: Integrated JavaScript Execution and Waiting](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-technique-2-integrated-javascript-execution-and-waiting)\n  * [Hooks & Auth in AsyncWebCrawler](https://docs.crawl4ai.com/complete-sdk-reference/#hooks-auth-in-asyncwebcrawler)\n  * [Example: Using Hooks in AsyncWebCrawler](https://docs.crawl4ai.com/complete-sdk-reference/#example-using-hooks-in-asyncwebcrawler)\n  * [Hook Lifecycle Summary](https://docs.crawl4ai.com/complete-sdk-reference/#hook-lifecycle-summary)\n  * [When to Handle Authentication](https://docs.crawl4ai.com/complete-sdk-reference/#when-to-handle-authentication)\n  * [Additional Considerations](https://docs.crawl4ai.com/complete-sdk-reference/#additional-considerations)\n  * [Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#conclusion_1)\n  * [Quick Reference](https://docs.crawl4ai.com/complete-sdk-reference/#quick-reference)\n  * [Core Imports](https://docs.crawl4ai.com/complete-sdk-reference/#core-imports)\n  * [Basic Pattern](https://docs.crawl4ai.com/complete-sdk-reference/#basic-pattern)\n  * [Advanced Pattern](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-pattern)\n  * [Multi-URL Pattern](https://docs.crawl4ai.com/complete-sdk-reference/#multi-url-pattern)\n\n\n# Crawl4AI Complete SDK Documentation\n**Generated:** 2025-10-19 12:56 **Format:** Ultra-Dense Reference (Optimized for AI Assistants) **Crawl4AI Version:** 0.7.4\n* * *\n## Navigation\n  * [Installation & Setup](https://docs.crawl4ai.com/complete-sdk-reference/#installation--setup)\n  * [Quick Start](https://docs.crawl4ai.com/complete-sdk-reference/#quick-start)\n  * [Core API](https://docs.crawl4ai.com/complete-sdk-reference/#core-api)\n  * [Configuration](https://docs.crawl4ai.com/complete-sdk-reference/#configuration)\n  * [Crawling Patterns](https://docs.crawl4ai.com/complete-sdk-reference/#crawling-patterns)\n  * [Content Processing](https://docs.crawl4ai.com/complete-sdk-reference/#content-processing)\n  * [Extraction Strategies](https://docs.crawl4ai.com/complete-sdk-reference/#extraction-strategies)\n  * [Advanced Features](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-features)\n\n\n* * *\n# Installation & Setup\n# Installation & Setup (2023 Edition)\n## 1. Basic Installation\n```\npip install crawl4ai\nCopy\n```\n\n## 2. Initial Setup & Diagnostics\n### 2.1 Run the Setup Command\n```\ncrawl4ai-setup\nCopy\n```\n\n- Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl\n### 2.2 Diagnostics\n```\ncrawl4ai-doctor\nCopy\n```\n\n- Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run `crawl4ai-setup`.\n## 3. Verifying Installation: A Simple Crawl (Skip this step if you already run `crawl4ai-doctor`)\nBelow is a minimal Python script demonstrating a **basic** crawl. It uses our new **`BrowserConfig`**and**`CrawlerRunConfig`**for clarity, though no custom settings are passed in this example:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n        )\n        print(result.markdown[:300])  # Show the first 300 characters of extracted text\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- A headless browser session loads `example.com` - Crawl4AI returns ~300 characters of markdown.  \nIf errors occur, rerun `crawl4ai-doctor` or manually ensure Playwright is installed correctly.\n## 4. Advanced Installation (Optional)\n### 4.1 Torch, Transformers, or All\n  * **Text Clustering (Torch)**  \n\n```\npip install crawl4ai[torch]\ncrawl4ai-setup\nCopy\n```\n\n  * **Transformers**  \n\n```\npip install crawl4ai[transformer]\ncrawl4ai-setup\nCopy\n```\n\n  * **All Features**  \n\n```\npip install crawl4ai[all]\ncrawl4ai-setup\nCopy\n```\n\n```\ncrawl4ai-download-models\nCopy\n```\n\n\n\n## 5. Docker (Experimental)\n```\ndocker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic\nCopy\n```\n\nYou can then make POST requests to `http://localhost:11235/crawl` to perform crawls. **Production usage** is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025).\n## 6. Local Server Mode (Legacy)\n## Summary\n1. **Install** with `pip install crawl4ai` and run `crawl4ai-setup`. 2. **Diagnose** with `crawl4ai-doctor` if you see errors. 3. **Verify** by crawling `example.com` with minimal `BrowserConfig` + `CrawlerRunConfig`.\n# Quick Start\n# Getting Started with Crawl4AI\n  1. Run your **first crawl** using minimal configuration. \n  2. Experiment with a simple **CSS-based extraction** strategy. \n  3. Crawl a **dynamic** page that loads content via JavaScript.\n\n\n## 1. Introduction\n  * An asynchronous crawler, **`AsyncWebCrawler`**.\n  * Configurable browser and run settings via **`BrowserConfig`**and**`CrawlerRunConfig`**.\n  * Automatic HTML-to-Markdown conversion via **`DefaultMarkdownGenerator`**(supports optional filters).\n  * Multiple extraction strategies (LLM-based or â€œtraditionalâ€ CSS/XPath-based).\n\n\n## 2. Your First Crawl\nHereâ€™s a minimal Python script that creates an **`AsyncWebCrawler`**, fetches a webpage, and prints the first 300 characters of its Markdown output:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(result.markdown[:300])  # Print first 300 chars\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **`AsyncWebCrawler`**launches a headless browser (Chromium by default). - It fetches`https://example.com`. - Crawl4AI automatically converts the HTML into Markdown.\n## 3. Basic Configuration (Light Introduction)\n1. **`BrowserConfig`**: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.).  \n2. **`CrawlerRunConfig`**: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.).\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_conf\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n> IMPORTANT: By default cache mode is set to `CacheMode.BYPASS` to have fresh content. Set `CacheMode.ENABLED` to enable caching.\n## 4. Generating Markdown Output\n  * **`result.markdown`**:\n  * **`result.markdown.fit_markdown`**:  \nThe same content after applying any configured **content filter** (e.g., `PruningContentFilter`).\n\n\n### Example: Using a Filter with `DefaultMarkdownGenerator`\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\nCopy\n```\n\n**Note** : If you do **not** specify a content filter or markdown generator, youâ€™ll typically see only the raw Markdown. `PruningContentFilter` may adds around `50ms` in processing time. Weâ€™ll dive deeper into these strategies in a dedicated **Markdown Generation** tutorial.\n## 5. Simple Data Extraction (CSS-based)\n```\nfrom crawl4ai import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")  # Required for OpenAI\n)\n\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\nCopy\n```\n\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        # The JSON output is stored in 'extracted_content'\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store.\n> Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with `raw://`.\n## 6. Simple Data Extraction (LLM-based)\n  * **Open-Source Models** (e.g., `ollama/llama3.3`, `no_token`) \n  * **OpenAI Models** (e.g., `openai/gpt-4`, requires `api_token`) \n  * Or any provider supported by the underlying library \n```\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config = LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n        )\n    )\nCopy\n```\n\n  * We define a Pydantic schema (`PricingInfo`) describing the fields we want.\n\n\n## 7. Adaptive Crawling (New!)\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, AdaptiveCrawler\n\nasync def adaptive_example():\n    async with AsyncWebCrawler() as crawler:\n        adaptive = AdaptiveCrawler(crawler)\n\n        # Start adaptive crawling\n        result = await adaptive.digest(\n            start_url=\"https://docs.python.org/3/\",\n            query=\"async context managers\"\n        )\n\n        # View results\n        adaptive.print_stats()\n        print(f\"Crawled {len(result.crawled_urls)} pages\")\n        print(f\"Achieved {adaptive.confidence:.0%} confidence\")\n\nif __name__ == \"__main__\":\n    asyncio.run(adaptive_example())\nCopy\n```\n\n- **Automatic stopping** : Stops when sufficient information is gathered - **Intelligent link selection** : Follows only relevant links - **Confidence scoring** : Know how complete your information is\n## 8. Multi-URL Concurrency (Preview)\nIf you need to crawl multiple URLs in **parallel** , you can use `arun_many()`. By default, Crawl4AI employs a **MemoryAdaptiveDispatcher** , automatically adjusting concurrency based on system resources. Hereâ€™s a quick glimpse: \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def quick_parallel_example():\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Stream results as they complete\n        async for result in await crawler.arun_many(urls, config=run_conf):\n            if result.success:\n                print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {result.url} => {result.error_message}\")\n\n        # Or get all results at once (default behavior)\n        run_conf = run_conf.clone(stream=False)\n        results = await crawler.arun_many(urls, config=run_conf)\n        for res in results:\n            if res.success:\n                print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {res.url} => {res.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(quick_parallel_example())\nCopy\n```\n\n1. **Streaming mode** (`stream=True`): Process results as they become available using `async for` 2. **Batch mode** (`stream=False`): Wait for all results to complete\n## 8. Dynamic Content Example\nSome sites require multiple â€œpage clicksâ€ or dynamic JavaScript updates. Below is an example showing how to **click** a â€œNext Pageâ€ button and wait for new commits to load on GitHub, using **`BrowserConfig`**and**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n        \"fields\": [\n            {\n                \"name\": \"section_title\",\n                \"selector\": \"h3.heading-50\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"section_description\",\n                \"selector\": \".charge-content\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_name\",\n                \"selector\": \".text-block-93\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_description\",\n                \"selector\": \".course-content-text\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_icon\",\n                \"selector\": \".image-92\",\n                \"type\": \"attribute\",\n                \"attribute\": \"src\",\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500));\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\nasync def main():\n    await extract_structured_data_using_css_extractor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **`BrowserConfig(headless=False)`**: We want to watch it click â€œNext Page.â€  \n- **`CrawlerRunConfig(...)`**: We specify the extraction strategy, pass`session_id` to reuse the same page.  \n- **`js_code`**and**`wait_for`**are used for subsequent pages (`page > 0`) to click the â€œNextâ€ button and wait for new commits to load.  \n- **`js_only=True`**indicates weâ€™re not re-navigating but continuing the existing session.  \n- Finally, we call `kill_session()` to clean up the page and browser session.\n## 9. Next Steps\n  1. Performed a basic crawl and printed Markdown. \n  2. Used **content filters** with a markdown generator. \n  3. Extracted JSON via **CSS** or **LLM** strategies. \n  4. Handled **dynamic** pages with JavaScript triggers.\n\n\n# Core API\n# AsyncWebCrawler\nThe **`AsyncWebCrawler`**is the core class for asynchronous web crawling in Crawl4AI. You typically create it**once** , optionally customize it with a **`BrowserConfig`**(e.g., headless, user agent), then**run** multiple **`arun()`**calls with different**`CrawlerRunConfig`**objects. 1.**Create** a `BrowserConfig` for global browser settings. 2. **Instantiate** `AsyncWebCrawler(config=browser_config)`. 3. **Use** the crawler in an async context manager (`async with`) or manage start/close manually. 4. **Call** `arun(url, config=crawler_run_config)` for each page you want.\n## 1. Constructor Overview\n```\nclass AsyncWebCrawler:\n    def __init__(\n        self,\n        crawler_strategy: Optional[AsyncCrawlerStrategy] = None,\n        config: Optional[BrowserConfig] = None,\n        always_bypass_cache: bool = False,           # deprecated\n        always_by_pass_cache: Optional[bool] = None, # also deprecated\n        base_directory: str = ...,\n        thread_safe: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Create an AsyncWebCrawler instance.\n\n        Args:\n            crawler_strategy: \n                (Advanced) Provide a custom crawler strategy if needed.\n            config: \n                A BrowserConfig object specifying how the browser is set up.\n            always_bypass_cache: \n                (Deprecated) Use CrawlerRunConfig.cache_mode instead.\n            base_directory:     \n                Folder for storing caches/logs (if relevant).\n            thread_safe: \n                If True, attempts some concurrency safeguards.â€€Usually False.\n            **kwargs: \n                Additional legacy or debugging parameters.\n        \"\"\"\n    )\n\n### Typical Initialization\n\n```python\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    verbose=True\ncrawler = AsyncWebCrawler(config=browser_cfg)\nCopy\n```\n\n**Notes** :\n  * **Legacy** parameters like `always_bypass_cache` remain for backward compatibility, but prefer to set **caching** in `CrawlerRunConfig`.\n\n\n* * *\n## 2. Lifecycle: Start/Close or Context Manager\n### 2.1 Context Manager (Recommended)\n```\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    # The crawler automatically starts/closes resources\nCopy\n```\n\nWhen the `async with` block ends, the crawler cleans up (closes the browser, etc.).\n### 2.2 Manual Start & Close\n```\ncrawler = AsyncWebCrawler(config=browser_cfg)\nawait crawler.start()\nresult1 = await crawler.arun(\"https://example.com\")\nresult2 = await crawler.arun(\"https://another.com\")\nawait crawler.close()\nCopy\n```\n\nUse this style if you have a **long-running** application or need full control of the crawlerâ€™s lifecycle.\n* * *\n## 3. Primary Method: `arun()`\n```\nasync def arun(\n    url: str,\n    config: Optional[CrawlerRunConfig] = None,\n    # Legacy parameters for backward compatibility...\nCopy\n```\n\n### 3.1 New Approach\nYou pass a `CrawlerRunConfig` object that sets up everything about a crawlâ€”content filtering, caching, session reuse, JS code, screenshots, etc.\n```\nimport asyncio\nfrom crawl4ai import CrawlerRunConfig, CacheMode\nrun_cfg = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    css_selector=\"main.article\",\n    word_count_threshold=10,\n    screenshot=True\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com/news\", config=run_cfg)\nCopy\n```\n\n### 3.2 Legacy Parameters Still Accepted\nFor **backward** compatibility, `arun()` can still accept direct arguments like `css_selector=...`, `word_count_threshold=...`, etc., but we strongly advise migrating them into a **`CrawlerRunConfig`**.\n* * *\n## 4. Batch Processing: `arun_many()`\n```\nasync def arun_many(\n    urls: List[str],\n    config: Optional[CrawlerRunConfig] = None,\n    # Legacy parameters maintained for backwards compatibility...\nCopy\n```\n\n### 4.1 Resource-Aware Crawling\nThe `arun_many()` method now uses an intelligent dispatcher that:\n  * Monitors system memory usage\n  * Implements adaptive rate limiting\n  * Provides detailed progress monitoring\n  * Manages concurrent crawls efficiently\n\n\n### 4.2 Example Usage\nCheck page [Multi-url Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling.md) for a detailed example of how to use `arun_many()`.\n```\n### 4.3 Key Features\n1.â€€**Rate Limiting**\n   - Automatic delay between requests\n   - Exponential backoff on rate limit detection\n   - Domain-specific rate limiting\n   - Configurable retry strategy\n2.â€€**Resource Monitoring**\n   - Memory usage tracking\n   - Adaptive concurrency based on system load\n   - Automatic pausing when resources are constrained\n3.â€€**Progress Monitoring**\n   - Detailed or aggregated progress display\n   - Real-time status updates\n   - Memory usage statistics\n4.â€€**Error Handling**\n   - Graceful handling of rate limits\n   - Automatic retries with backoff\n   - Detailed error reporting\n## 5.â€€`CrawlResult` Output\nEach `arun()` returns a **`CrawlResult`** containing:\n- `url`: Final URL (if redirected).\n- `html`: Original HTML.\n- `cleaned_html`: Sanitized HTML.\n- `markdown_v2`: Deprecated. Instead just use regular `markdown`\n- `extracted_content`: If an extraction strategy was used (JSON for CSS/LLM strategies).\n- `screenshot`, `pdf`: If screenshots/PDF requested.\n- `media`, `links`: Information about discovered images/links.\n- `success`, `error_message`: Status info.\n## 6.â€€Quick Example\n```python\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    # 1.â€€Browser config\n    browser_cfg = BrowserConfig(\n        browser_type=\"firefox\",\n        headless=False,\n        verbose=True\n    )\n\n    # 2.â€€Run config\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\n                \"name\": \"title\", \n                \"selector\": \"h2\", \n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"url\", \n                \"selector\": \"a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        word_count_threshold=15,\n        remove_overlay_elements=True,\n        wait_for=\"css:.post\"  # Wait for posts to appear\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog\",\n            config=run_cfg\n        )\n\n        if result.success:\n            print(\"Cleaned HTML length:\", len(result.cleaned_html))\n            if result.extracted_content:\n                articles = json.loads(result.extracted_content)\n                print(\"Extracted articles:\", articles[:2])\n        else:\n            print(\"Error:\", result.error_message)\n\nasyncio.run(main())\nCopy\n```\n\n- We define a **`BrowserConfig`**with Firefox, no headless, and`verbose=True`. - We define a **`CrawlerRunConfig`**that**bypasses cache** , uses a **CSS** extraction schema, has a `word_count_threshold=15`, etc. - We pass them to `AsyncWebCrawler(config=...)` and `arun(url=..., config=...)`.\n## 7. Best Practices & Migration Notes\n1. **Use** `BrowserConfig` for **global** settings about the browserâ€™s environment. 2. **Use** `CrawlerRunConfig` for **per-crawl** logic (caching, content filtering, extraction strategies, wait conditions). 3. **Avoid** legacy parameters like `css_selector` or `word_count_threshold` directly in `arun()`. Instead: \n```\nrun_cfg = CrawlerRunConfig(css_selector=\".main-content\", word_count_threshold=20)\nresult = await crawler.arun(url=\"...\", config=run_cfg)\nCopy\n```\n\n## 8. Summary\n  * **Constructor** accepts **`BrowserConfig`**(or defaults).\n  * **`arun(url, config=CrawlerRunConfig)`**is the main method for single-page crawls.\n  * **`arun_many(urls, config=CrawlerRunConfig)`**handles concurrency across multiple URLs.\n  * For advanced lifecycle control, use `start()` and `close()` explicitly. \n  * If you used `AsyncWebCrawler(browser_type=\"chromium\", css_selector=\"...\")`, move browser settings to `BrowserConfig(...)` and content/crawl logic to `CrawlerRunConfig(...)`.\n\n\n#  `arun()` Parameter Guide (New Approach)\nIn Crawl4AIâ€™s **latest** configuration model, nearly all parameters that once went directly to `arun()` are now part of **`CrawlerRunConfig`**. When calling`arun()` , you provide: \n```\nawait crawler.arun(\n    url=\"https://example.com\",  \n    config=my_run_config\n)\nCopy\n```\n\nBelow is an organized look at the parameters that can go inside `CrawlerRunConfig`, divided by their functional areas. For **Browser** settings (e.g., `headless`, `browser_type`), see [BrowserConfig](https://docs.crawl4ai.com/complete-sdk-reference/parameters.md).\n## 1. Core Usage\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    run_config = CrawlerRunConfig(\n        verbose=True,            # Detailed logging\n        cache_mode=CacheMode.ENABLED,  # Use normal read/write cache\n        check_robots_txt=True,   # Respect robots.txt rules\n        # ...â€€other parameters\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n\n        # Check if blocked by robots.txt\n        if not result.success and result.status_code == 403:\n            print(f\"Error: {result.error_message}\")\nCopy\n```\n\n- `verbose=True` logs each crawl step. - `cache_mode` decides how to read/write the local crawl cache.\n## 2. Cache Control\n**`cache_mode`**(default:`CacheMode.ENABLED`)  \nUse a built-in enum from `CacheMode`: - `ENABLED`: Normal cachingâ€”reads if available, writes if missing. - `DISABLED`: No cachingâ€”always refetch pages. - `READ_ONLY`: Reads from cache only; no new writes. - `WRITE_ONLY`: Writes to cache but doesnâ€™t read existing data. - `BYPASS`: Skips reading cache for this crawl (though it might still write if set up that way). \n```\nrun_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS\n)\nCopy\n```\n\n- `bypass_cache=True` acts like `CacheMode.BYPASS`. - `disable_cache=True` acts like `CacheMode.DISABLED`. - `no_cache_read=True` acts like `CacheMode.WRITE_ONLY`. - `no_cache_write=True` acts like `CacheMode.READ_ONLY`.\n## 3. Content Processing & Selection\n### 3.1 Text Processing\n```\nrun_config = CrawlerRunConfig(\n    word_count_threshold=10,   # Ignore text blocks <10 words\n    only_text=False,           # If True, tries to remove non-text elements\n    keep_data_attributes=False # Keep or discard data-* attributes\n)\nCopy\n```\n\n### 3.2 Content Selection\n```\nrun_config = CrawlerRunConfig(\n    css_selector=\".main-content\",  # Focus on .main-content region only\n    excluded_tags=[\"form\", \"nav\"], # Remove entire tag blocks\n    remove_forms=True,             # Specifically strip <form> elements\n    remove_overlay_elements=True,  # Attempt to remove modals/popups\n)\nCopy\n```\n\n### 3.3 Link Handling\n```\nrun_config = CrawlerRunConfig(\n    exclude_external_links=True,         # Remove external links from final content\n    exclude_social_media_links=True,     # Remove links to known social sites\n    exclude_domains=[\"ads.example.com\"], # Exclude links to these domains\n    exclude_social_media_domains=[\"facebook.com\",\"twitter.com\"], # Extend the default list\n)\nCopy\n```\n\n### 3.4 Media Filtering\n```\nrun_config = CrawlerRunConfig(\n    exclude_external_images=True  # Strip images from other domains\n)\nCopy\n```\n\n## 4. Page Navigation & Timing\n### 4.1 Basic Browser Flow\n```\nrun_config = CrawlerRunConfig(\n    wait_for=\"css:.dynamic-content\", # Wait for .dynamic-content\n    delay_before_return_html=2.0,    # Wait 2s before capturing final HTML\n    page_timeout=60000,             # Navigation & script timeout (ms)\n)\nCopy\n```\n\n- `wait_for`:  \n- `\"css:selector\"` or  \n- `\"js:() => boolean\"`  \ne.g. `js:() => document.querySelectorAll('.item').length > 10`. - `mean_delay` & `max_range`: define random delays for `arun_many()` calls. - `semaphore_count`: concurrency limit when crawling multiple URLs.\n### 4.2 JavaScript Execution\n```\nrun_config = CrawlerRunConfig(\n    js_code=[\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        \"document.querySelector('.load-more')?.click();\"\n    ],\n    js_only=False\n)\nCopy\n```\n\n- `js_code` can be a single string or a list of strings. - `js_only=True` means â€œIâ€™m continuing in the same session with new JS steps, no new full navigation.â€\n### 4.3 Anti-Bot\n```\nrun_config = CrawlerRunConfig(\n    magic=True,\n    simulate_user=True,\n    override_navigator=True\n)\nCopy\n```\n\n- `magic=True` tries multiple stealth features. - `simulate_user=True` mimics mouse movements or random delays. - `override_navigator=True` fakes some navigator properties (like user agent checks).\n## 5. Session Management\n**`session_id`**:\n```\nrun_config = CrawlerRunConfig(\n    session_id=\"my_session123\"\n)\nCopy\n```\n\nIf re-used in subsequent `arun()` calls, the same tab/page context is continued (helpful for multi-step tasks or stateful browsing).\n## 6. Screenshot, PDF & Media Options\n```\nrun_config = CrawlerRunConfig(\n    screenshot=True,             # Grab a screenshot as base64\n    screenshot_wait_for=1.0,     # Wait 1s before capturing\n    pdf=True,                    # Also produce a PDF\n    image_description_min_word_threshold=5,  # If analyzing alt text\n    image_score_threshold=3,                # Filter out low-score images\n)\nCopy\n```\n\n- `result.screenshot` â†’ Base64 screenshot string. - `result.pdf` â†’ Byte array with PDF data.\n## 7. Extraction Strategy\n**For advanced data extraction** (CSS/LLM-based), set `extraction_strategy`: \n```\nrun_config = CrawlerRunConfig(\n    extraction_strategy=my_css_or_llm_strategy\n)\nCopy\n```\n\nThe extracted data will appear in `result.extracted_content`.\n## 8. Comprehensive Example\nBelow is a snippet combining many parameters: \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    # Example schema\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\",  \"selector\": \"a\",  \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    run_config = CrawlerRunConfig(\n        # Core\n        verbose=True,\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,   # Respect robots.txt rules\n\n        # Content\n        word_count_threshold=10,\n        css_selector=\"main.content\",\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n\n        # Page & JS\n        js_code=\"document.querySelector('.show-more')?.click();\",\n        wait_for=\"css:.loaded-block\",\n        page_timeout=30000,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n\n        # Session\n        session_id=\"persistent_session\",\n\n        # Media\n        screenshot=True,\n        pdf=True,\n\n        # Anti-bot\n        simulate_user=True,\n        magic=True,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/posts\", config=run_config)\n        if result.success:\n            print(\"HTML length:\", len(result.cleaned_html))\n            print(\"Extraction JSON:\", result.extracted_content)\n            if result.screenshot:\n                print(\"Screenshot length:\", len(result.screenshot))\n            if result.pdf:\n                print(\"PDF bytes length:\", len(result.pdf))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n1. **Crawling** the main content region, ignoring external links. 2. Running **JavaScript** to click â€œ.show-moreâ€. 3. **Waiting** for â€œ.loaded-blockâ€ to appear. 4. Generating a **screenshot** & **PDF** of the final page. \n## 9. Best Practices\n1. **Use`BrowserConfig` for global browser** settings (headless, user agent). 2. **Use`CrawlerRunConfig`** to handle the **specific** crawl needs: content filtering, caching, JS, screenshot, extraction, etc. 4. **Limit** large concurrency (`semaphore_count`) if the site or your system canâ€™t handle it. 5. For dynamic pages, set `js_code` or `scan_full_page` so you load all content.\n## 10. Conclusion\nAll parameters that used to be direct arguments to `arun()` now belong in **`CrawlerRunConfig`**. This approach: - Makes code**clearer** and **more maintainable**. \n#  `arun_many(...)` Reference\n> **Note** : This function is very similar to [`arun()`](https://docs.crawl4ai.com/complete-sdk-reference/arun.md) but focused on **concurrent** or **batch** crawling. If youâ€™re unfamiliar with `arun()` usage, please read that doc first, then review this for differences.\n## Function Signature\n```\nasync def arun_many(\n    urls: Union[List[str], List[Any]],\n    config: Optional[Union[CrawlerRunConfig, List[CrawlerRunConfig]]] = None,\n    dispatcher: Optional[BaseDispatcher] = None,\n    ...\n) -> Union[List[CrawlResult], AsyncGenerator[CrawlResult, None]]:\n    \"\"\"\n    Crawl multiple URLs concurrently or in batches.\n\n    :param urls: A list of URLs (or tasks) to crawl.\n    :param config: (Optional) Either:\n        - A single `CrawlerRunConfig` applying to all URLs\n        - A list of `CrawlerRunConfig` objects with url_matcher patterns\n    :param dispatcher: (Optional) A concurrency controller (e.g.â€€MemoryAdaptiveDispatcher).\n    ...\n    :return: Either a list of `CrawlResult` objects, or an async generator if streaming is enabled.\n    \"\"\"\nCopy\n```\n\n## Differences from `arun()`\n1. **Multiple URLs** :  \n- Instead of crawling a single URL, you pass a list of them (strings or tasks). - The function returns either a **list** of `CrawlResult` or an **async generator** if streaming is enabled. 2. **Concurrency & Dispatchers**:  \n- **`dispatcher`**param allows advanced concurrency control. - If omitted, a default dispatcher (like`MemoryAdaptiveDispatcher`) is used internally. 3. **Streaming Support** :  \n- Enable streaming by setting `stream=True` in your `CrawlerRunConfig`. - When streaming, use `async for` to process results as they become available. 4. **Parallel** Execution**:  \n- `arun_many()` can run multiple requests concurrently under the hood. - Each `CrawlResult` might also include a **`dispatch_result`** with concurrency details (like memory usage, start/end times).\n### Basic Example (Batch Mode)\n```\n# Minimal usage: The default dispatcher will be used\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\"],\n    config=CrawlerRunConfig(stream=False)  # Default behavior\n)\n\nfor res in results:\n    if res.success:\n        print(res.url, \"crawled OK!\")\n    else:\n        print(\"Failed:\", res.url, \"-\", res.error_message)\nCopy\n```\n\n### Streaming Example\n```\nconfig = CrawlerRunConfig(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\n# Process results as they complete\nasync for result in await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=config\n):\n    if result.success:\n        print(f\"Just completed: {result.url}\")\n        # Process each result immediately\n        process_result(result)\nCopy\n```\n\n### With a Custom Dispatcher\n```\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=70.0,\n    max_session_permit=10\n)\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=my_run_config,\n    dispatcher=dispatcher\n)\nCopy\n```\n\n### URL-Specific Configurations\nInstead of using one config for all URLs, provide a list of configs with `url_matcher` patterns: \n```\nfrom crawl4ai import CrawlerRunConfig, MatchMode\nfrom crawl4ai.processors.pdf import PDFContentScrapingStrategy\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\n# PDF files - specialized extraction\npdf_config = CrawlerRunConfig(\n    url_matcher=\"*.pdf\",\n    scraping_strategy=PDFContentScrapingStrategy()\n)\n\n# Blog/article pages - content filtering\nblog_config = CrawlerRunConfig(\n    url_matcher=[\"*/blog/*\", \"*/article/*\", \"*python.org*\"],\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.48)\n    )\n)\n\n# Dynamic pages - JavaScript execution\ngithub_config = CrawlerRunConfig(\n    url_matcher=lambda url: 'github.com' in url,\n    js_code=\"window.scrollTo(0, 500);\"\n)\n\n# API endpoints - JSON extraction\napi_config = CrawlerRunConfig(\n    url_matcher=lambda url: 'api' in url or url.endswith('.json'),\n    # Custome settings for JSON extraction\n)\n\n# Default fallback config\ndefault_config = CrawlerRunConfig()  # No url_matcher means it never matches except as fallback\n\n# Pass the list of configs - first match wins!\nresults = await crawler.arun_many(\n    urls=[\n        \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\",  # â†’ pdf_config\n        \"https://blog.python.org/\",  # â†’ blog_config\n        \"https://github.com/microsoft/playwright\",  # â†’ github_config\n        \"https://httpbin.org/json\",  # â†’ api_config\n        \"https://example.com/\"  # â†’ default_config\n    ],\n    config=[pdf_config, blog_config, github_config, api_config, default_config]\n)\nCopy\n```\n\n- **String patterns** : `\"*.pdf\"`, `\"*/blog/*\"`, `\"*python.org*\"` - **Function matchers** : `lambda url: 'api' in url` - **Mixed patterns** : Combine strings and functions with `MatchMode.OR` or `MatchMode.AND` - **First match wins** : Configs are evaluated in order - `dispatch_result` in each `CrawlResult` (if using concurrency) can hold memory and timing info. - **Important** : Always include a default config (without `url_matcher`) as the last item if you want to handle all URLs. Otherwise, unmatched URLs will fail.\n### Return Value\nEither a **list** of [`CrawlResult`](https://docs.crawl4ai.com/complete-sdk-reference/crawl-result.md) objects, or an **async generator** if streaming is enabled. You can iterate to check `result.success` or read each itemâ€™s `extracted_content`, `markdown`, or `dispatch_result`.\n## Dispatcher Reference\n  * **`MemoryAdaptiveDispatcher`**: Dynamically manages concurrency based on system memory usage.\n  * **`SemaphoreDispatcher`**: Fixed concurrency limit, simpler but less adaptive.\n\n\n## Common Pitfalls\n3. **Error Handling** : Each `CrawlResult` might fail for different reasonsâ€”always check `result.success` or the `error_message` before proceeding.\n## Conclusion\nUse `arun_many()` when you want to **crawl multiple URLs** simultaneously or in controlled parallel tasks. If you need advanced concurrency features (like memory-based adaptive throttling or complex rate-limiting), provide a **dispatcher**. Each result is a standard `CrawlResult`, possibly augmented with concurrency stats (`dispatch_result`) for deeper inspection. For more details on concurrency logic and dispatchers, see the [Advanced Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling.md) docs.\n#  `CrawlResult` Reference\nThe **`CrawlResult`**class encapsulates everything returned after a single crawl operation. It provides the**raw or processed content** , details on links and media, plus optional metadata (like screenshots, PDFs, or extracted JSON). **Location** : `crawl4ai/crawler/models.py` (for reference) \n```\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    fit_html: Optional[str] = None  # Preprocessed HTML optimized for extraction\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    mhtml: Optional[str] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    dispatch_result: Optional[DispatchResult] = None\n    ...\nCopy\n```\n\n## 1. Basic Crawl Info\n### 1.1 **`url`**_(str)_\n```\nprint(result.url)  # e.g., \"https://example.com/\"\nCopy\n```\n\n### 1.2 **`success`**_(bool)_\n**What** : `True` if the crawl pipeline ended without major errors; `False` otherwise.  \n\n```\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\nCopy\n```\n\n### 1.3 **`status_code`**_(Optional[int])_\n```\nif result.status_code == 404:\n    print(\"Page not found!\")\nCopy\n```\n\n### 1.4 **`error_message`**_(Optional[str])_\n**What** : If `success=False`, a textual description of the failure.  \n\n```\nif not result.success:\n    print(\"Error:\", result.error_message)\nCopy\n```\n\n### 1.5 **`session_id`**_(Optional[str])_\n```\n# If you used session_id=\"login_session\" in CrawlerRunConfig, see it here:\nprint(\"Session:\", result.session_id)\nCopy\n```\n\n### 1.6 **`response_headers`**_(Optional[dict])_\n```\nif result.response_headers:\n    print(\"Server:\", result.response_headers.get(\"Server\", \"Unknown\"))\nCopy\n```\n\n### 1.7 **`ssl_certificate`**_(Optional[SSLCertificate])_\n**What** : If `fetch_ssl_certificate=True` in your CrawlerRunConfig, **`result.ssl_certificate`**contains a[**`SSLCertificate`**](https://docs.crawl4ai.com/advanced/ssl-certificate.md)object describing the site's certificate. You can export the cert in multiple formats (PEM/DER/JSON) or access its properties like`issuer`, `subject`, `valid_from`, `valid_until`, etc. \n```\nif result.ssl_certificate:\n    print(\"Issuer:\", result.ssl_certificate.issuer)\nCopy\n```\n\n## 2. Raw / Cleaned Content\n### 2.1 **`html`**_(str)_\n```\n# Possibly large\nprint(len(result.html))\nCopy\n```\n\n### 2.2 **`cleaned_html`**_(Optional[str])_\n**What** : A sanitized HTML versionâ€”scripts, styles, or excluded tags are removed based on your `CrawlerRunConfig`.  \n\n```\nprint(result.cleaned_html[:500])  # Show a snippet\nCopy\n```\n\n## 3. Markdown Fields\n### 3.1 The Markdown Generation Approach\n  * **Raw** markdown \n  * **Links as citations** (with a references section) \n  * **Fit** markdown if a **content filter** is used (like Pruning or BM25) **`MarkdownGenerationResult`**includes:\n  * **`raw_markdown`**_(str)_ : The full HTMLâ†’Markdown conversion. \n  * **`markdown_with_citations`**_(str)_ : Same markdown, but with link references as academic-style citations. \n  * **`references_markdown`**_(str)_ : The reference list or footnotes at the end. \n  * **`fit_markdown`**_(Optional[str])_ : If content filtering (Pruning/BM25) was applied, the filtered \"fit\" text. \n  * **`fit_html`**_(Optional[str])_ : The HTML that led to `fit_markdown`. \n```\nif result.markdown:\n    md_res = result.markdown\n    print(\"Raw MD:\", md_res.raw_markdown[:300])\n    print(\"Citations MD:\", md_res.markdown_with_citations[:300])\n    print(\"References:\", md_res.references_markdown)\n    if md_res.fit_markdown:\n        print(\"Pruned text:\", md_res.fit_markdown[:300])\nCopy\n```\n\n\n\n### 3.2 **`markdown`**_(Optional[Union[str, MarkdownGenerationResult]])_\n**What** : Holds the `MarkdownGenerationResult`.  \n\n```\nprint(result.markdown.raw_markdown[:200])\nprint(result.markdown.fit_markdown)\nprint(result.markdown.fit_html)\nCopy\n```\n\n**Important** : \"Fit\" content (in `fit_markdown`/`fit_html`) exists in result.markdown, only if you used a **filter** (like **PruningContentFilter** or **BM25ContentFilter**) within a `MarkdownGenerationStrategy`.\n## 4. Media & Links\n### 4.1 **`media`**_(Dict[str, List[Dict]])_\n**What** : Contains info about discovered images, videos, or audio. Typically keys: `\"images\"`, `\"videos\"`, `\"audios\"`.  \n- `src` _(str)_ : Media URL  \n- `alt` or `title` _(str)_ : Descriptive text  \n- `score` _(float)_ : Relevance score if the crawler's heuristic found it \"important\"  \n- `desc` or `description` _(Optional[str])_ : Additional context extracted from surrounding text  \n\n```\nimages = result.media.get(\"images\", [])\nfor img in images:\n    if img.get(\"score\", 0) > 5:\n        print(\"High-value image:\", img[\"src\"])\nCopy\n```\n\n### 4.2 **`links`**_(Dict[str, List[Dict]])_\n**What** : Holds internal and external link data. Usually two keys: `\"internal\"` and `\"external\"`.  \n- `href` _(str)_ : The link target  \n- `text` _(str)_ : Link text  \n- `title` _(str)_ : Title attribute  \n- `context` _(str)_ : Surrounding text snippet  \n- `domain` _(str)_ : If external, the domain \n```\nfor link in result.links[\"internal\"]:\n    print(f\"Internal link to {link['href']} with text {link['text']}\")\nCopy\n```\n\n## 5. Additional Fields\n### 5.1 **`extracted_content`**_(Optional[str])_\n**What** : If you used **`extraction_strategy`**(CSS, LLM, etc.), the structured output (JSON).  \n\n```\nif result.extracted_content:\n    data = json.loads(result.extracted_content)\n    print(data)\nCopy\n```\n\n### 5.2 **`downloaded_files`**_(Optional[List[str]])_\n**What** : If `accept_downloads=True` in your `BrowserConfig` + `downloads_path`, lists local file paths for downloaded items.  \n\n```\nif result.downloaded_files:\n    for file_path in result.downloaded_files:\n        print(\"Downloaded:\", file_path)\nCopy\n```\n\n### 5.3 **`screenshot`**_(Optional[str])_\n**What** : Base64-encoded screenshot if `screenshot=True` in `CrawlerRunConfig`.  \n\n```\nimport base64\nif result.screenshot:\n    with open(\"page.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))\nCopy\n```\n\n### 5.4 **`pdf`**_(Optional[bytes])_\n**What** : Raw PDF bytes if `pdf=True` in `CrawlerRunConfig`.  \n\n```\nif result.pdf:\n    with open(\"page.pdf\", \"wb\") as f:\n        f.write(result.pdf)\nCopy\n```\n\n### 5.5 **`mhtml`**_(Optional[str])_\n**What** : MHTML snapshot of the page if `capture_mhtml=True` in `CrawlerRunConfig`. MHTML (MIME HTML) format preserves the entire web page with all its resources (CSS, images, scripts, etc.) in a single file.  \n\n```\nif result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)\nCopy\n```\n\n### 5.6 **`metadata`**_(Optional[dict])_\n```\nif result.metadata:\n    print(\"Title:\", result.metadata.get(\"title\"))\n    print(\"Author:\", result.metadata.get(\"author\"))\nCopy\n```\n\n## 6. `dispatch_result` (optional)\nA `DispatchResult` object providing additional concurrency and resource usage information when crawling URLs in parallel (e.g., via `arun_many()` with custom dispatchers). It contains: - **`task_id`**: A unique identifier for the parallel task. -**`memory_usage`**(float): The memory (in MB) used at the time of completion. -**`peak_memory`**(float): The peak memory usage (in MB) recorded during the task's execution. -**`start_time`**/**`end_time`**(datetime): Time range for this crawling task. -**`error_message`**(str): Any dispatcher- or concurrency-related error encountered.\n```\n# Example usage:\nfor result in results:\n    if result.success and result.dispatch_result:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}, Task ID: {dr.task_id}\")\n        print(f\"Memory: {dr.memory_usage:.1f} MB (Peak: {dr.peak_memory:.1f} MB)\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")\nCopy\n```\n\n> **Note** : This field is typically populated when using `arun_many(...)` alongside a **dispatcher** (e.g., `MemoryAdaptiveDispatcher` or `SemaphoreDispatcher`). If no concurrency or dispatcher is used, `dispatch_result` may remain `None`. \n## 7. Network Requests & Console Messages\nWhen you enable network and console message capturing in `CrawlerRunConfig` using `capture_network_requests=True` and `capture_console_messages=True`, the `CrawlResult` will include these fields:\n### 7.1 **`network_requests`**_(Optional[List[Dict[str, Any]]])_\n  * Each item has an `event_type` field that can be `\"request\"`, `\"response\"`, or `\"request_failed\"`.\n  * Request events include `url`, `method`, `headers`, `post_data`, `resource_type`, and `is_navigation_request`.\n  * Response events include `url`, `status`, `status_text`, `headers`, and `request_timing`.\n  * Failed request events include `url`, `method`, `resource_type`, and `failure_text`.\n  * All events include a `timestamp` field. \n```\nif result.network_requests:\n    # Count different types of events\n    requests = [r for r in result.network_requests if r.get(\"event_type\") == \"request\"]\n    responses = [r for r in result.network_requests if r.get(\"event_type\") == \"response\"]\n    failures = [r for r in result.network_requests if r.get(\"event_type\") == \"request_failed\"]\n\n    print(f\"Captured {len(requests)} requests, {len(responses)} responses, and {len(failures)} failures\")\n\n    # Analyze API calls\n    api_calls = [r for r in requests if \"api\" in r.get(\"url\", \"\")]\n\n    # Identify failed resources\n    for failure in failures:\n        print(f\"Failed to load: {failure.get('url')} - {failure.get('failure_text')}\")\nCopy\n```\n\n\n\n### 7.2 **`console_messages`**_(Optional[List[Dict[str, Any]]])_\n  * Each item has a `type` field indicating the message type (e.g., `\"log\"`, `\"error\"`, `\"warning\"`, etc.).\n  * The `text` field contains the actual message text.\n  * Some messages include `location` information (URL, line, column).\n  * All messages include a `timestamp` field. \n```\nif result.console_messages:\n    # Count messages by type\n    message_types = {}\n    for msg in result.console_messages:\n        msg_type = msg.get(\"type\", \"unknown\")\n        message_types[msg_type] = message_types.get(msg_type, 0) + 1\n\n    print(f\"Message type counts: {message_types}\")\n\n    # Display errors (which are usually most important)\n    for msg in result.console_messages:\n        if msg.get(\"type\") == \"error\":\n            print(f\"Error: {msg.get('text')}\")\nCopy\n```\n\n\n\n## 8. Example: Accessing Everything\n```\nasync def handle_result(result: CrawlResult):\n    if not result.success:\n        print(\"Crawl error:\", result.error_message)\n        return\n\n    # Basic info\n    print(\"Crawled URL:\", result.url)\n    print(\"Status code:\", result.status_code)\n\n    # HTML\n    print(\"Original HTML size:\", len(result.html))\n    print(\"Cleaned HTML size:\", len(result.cleaned_html or \"\"))\n\n    # Markdown output\n    if result.markdown:\n        print(\"Raw Markdown:\", result.markdown.raw_markdown[:300])\n        print(\"Citations Markdown:\", result.markdown.markdown_with_citations[:300])\n        if result.markdown.fit_markdown:\n            print(\"Fit Markdown:\", result.markdown.fit_markdown[:200])\n\n    # Media & Links\n    if \"images\" in result.media:\n        print(\"Image count:\", len(result.media[\"images\"]))\n    if \"internal\" in result.links:\n        print(\"Internal link count:\", len(result.links[\"internal\"]))\n\n    # Extraction strategy result\n    if result.extracted_content:\n        print(\"Structured data:\", result.extracted_content)\n\n    # Screenshot/PDF/MHTML\n    if result.screenshot:\n        print(\"Screenshot length:\", len(result.screenshot))\n    if result.pdf:\n        print(\"PDF bytes length:\", len(result.pdf))\n    if result.mhtml:\n        print(\"MHTML length:\", len(result.mhtml))\n\n    # Network and console capturing\n    if result.network_requests:\n        print(f\"Network requests captured: {len(result.network_requests)}\")\n        # Analyze request types\n        req_types = {}\n        for req in result.network_requests:\n            if \"resource_type\" in req:\n                req_types[req[\"resource_type\"]] = req_types.get(req[\"resource_type\"], 0) + 1\n        print(f\"Resource types: {req_types}\")\n\n    if result.console_messages:\n        print(f\"Console messages captured: {len(result.console_messages)}\")\n        # Count by message type\n        msg_types = {}\n        for msg in result.console_messages:\n            msg_types[msg.get(\"type\", \"unknown\")] = msg_types.get(msg.get(\"type\", \"unknown\"), 0) + 1\n        print(f\"Message types: {msg_types}\")\nCopy\n```\n\n## 9. Key Points & Future\n1. **Deprecated legacy properties of CrawlResult**  \n- `markdown_v2` - Deprecated in v0.5. Just use `markdown`. It holds the `MarkdownGenerationResult` now! - `fit_markdown` and `fit_html` - Deprecated in v0.5. They can now be accessed via `MarkdownGenerationResult` in `result.markdown`. eg: `result.markdown.fit_markdown` and `result.markdown.fit_html` 2. **Fit Content**  \n- **`fit_markdown`**and**`fit_html`**appear in MarkdownGenerationResult, only if you used a content filter (like**PruningContentFilter** or **BM25ContentFilter**) inside your **MarkdownGenerationStrategy** or set them directly.  \n- If no filter is used, they remain `None`. 3. **References & Citations**  \n- If you enable link citations in your `DefaultMarkdownGenerator` (`options={\"citations\": True}`), youâ€™ll see `markdown_with_citations` plus a **`references_markdown`**block. This helps large language models or academic-like referencing. 4.**Links & Media**  \n- `links[\"internal\"]` and `links[\"external\"]` group discovered anchors by domain.  \n- `media[\"images\"]` / `[\"videos\"]` / `[\"audios\"]` store extracted media elements with optional scoring or context. 5. **Error Cases**  \n- If `success=False`, check `error_message` (e.g., timeouts, invalid URLs).  \n- `status_code` might be `None` if we failed before an HTTP response. Use **`CrawlResult`**to glean all final outputs and feed them into your data pipelines, AI models, or archives. With the synergy of a properly configured**BrowserConfig** and **CrawlerRunConfig** , the crawler can produce robust, structured results here in **`CrawlResult`**.\n# Configuration\n# Browser, Crawler & LLM Configuration (Quick Overview)\nCrawl4AI's flexibility stems from two key classes: 1. **`BrowserConfig`**â€“ Dictates**how** the browser is launched and behaves (e.g., headless or visible, proxy, user agent).  \n2. **`CrawlerRunConfig`**â€“ Dictates**how** each **crawl** operates (e.g., caching, extraction, timeouts, JavaScript code to run, etc.).  \n3. **`LLMConfig`**- Dictates**how** LLM providers are configured. (model, api token, base url, temperature etc.) In most examples, you create **one** `BrowserConfig` for the entire crawler session, then pass a **fresh** or re-used `CrawlerRunConfig` whenever you call `arun()`. This tutorial shows the most commonly used parameters. If you need advanced or rarely used fields, see the [Configuration Parameters](https://docs.crawl4ai.com/api/parameters.md).\n## 1. BrowserConfig Essentials\n```\nclass BrowserConfig:\n    def __init__(\n        browser_type=\"chromium\",\n        headless=True,\n        proxy_config=None,\n        viewport_width=1080,\n        viewport_height=600,\n        verbose=True,\n        use_persistent_context=False,\n        user_data_dir=None,\n        cookies=None,\n        headers=None,\n        user_agent=None,\n        text_mode=False,\n        light_mode=False,\n        extra_args=None,\n        enable_stealth=False,\n        # ... other advanced parameters omitted here\n    ):\n        ...\nCopy\n```\n\n### Key Fields to Note\n  1. **`browser_type`**\n  2. Options: `\"chromium\"`, `\"firefox\"`, or `\"webkit\"`. \n  3. Defaults to `\"chromium\"`. \n  4. If you need a different engine, specify it here.\n  5. **`headless`**\n  6. `True`: Runs the browser in headless mode (invisible browser). \n  7. `False`: Runs the browser in visible mode, which helps with debugging.\n  8. **`proxy_config`**\n  9. A dictionary with fields like:  \n\n```\n{\n    \"server\": \"http://proxy.example.com:8080\", \n    \"username\": \"...\", \n    \"password\": \"...\"\n}\nCopy\n```\n\n  10. Leave as `None` if a proxy is not required.\n  11. **`viewport_width` & `viewport_height`**: \n  12. The initial window size. \n  13. Some sites behave differently with smaller or bigger viewports.\n  14. **`verbose`**:\n  15. If `True`, prints extra logs. \n  16. Handy for debugging.\n  17. **`use_persistent_context`**:\n  18. If `True`, uses a **persistent** browser profile, storing cookies/local storage across runs. \n  19. Typically also set `user_data_dir` to point to a folder.\n  20. **`cookies`** & **`headers`**:\n  21. E.g. `cookies=[{\"name\": \"session\", \"value\": \"abc123\", \"domain\": \"example.com\"}]`.\n  22. **`user_agent`**:\n  23. Custom User-Agent string. If `None`, a default is used. \n  24. You can also set `user_agent_mode=\"random\"` for randomization (if you want to fight bot detection).\n  25. **`text_mode`** & **`light_mode`**:\n  26. `text_mode=True` disables images, possibly speeding up text-only crawls. \n  27. `light_mode=True` turns off certain background features for performance. \n  28. **`extra_args`**:\n     * Additional flags for the underlying browser. \n     * E.g. `[\"--disable-extensions\"]`.\n  29. **`enable_stealth`**:\n     * If `True`, enables stealth mode using playwright-stealth. \n     * Modifies browser fingerprints to avoid basic bot detection. \n     * Default is `False`. Recommended for sites with bot protection.\n\n\n### Helper Methods\nBoth configuration classes provide a `clone()` method to create modified copies: \n```\n# Create a base browser config\nbase_browser = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    text_mode=True\n)\n\n# Create a visible browser config for debugging\ndebug_browser = base_browser.clone(\n    headless=False,\n    verbose=True\n)\nCopy\n```\n\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_conf = BrowserConfig(\n    browser_type=\"firefox\",\n    headless=False,\n    text_mode=True\n)\n\nasync with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300])\nCopy\n```\n\n## 2. CrawlerRunConfig Essentials\n```\nclass CrawlerRunConfig:\n    def __init__(\n        word_count_threshold=200,\n        extraction_strategy=None,\n        markdown_generator=None,\n        cache_mode=None,\n        js_code=None,\n        wait_for=None,\n        screenshot=False,\n        pdf=False,\n        capture_mhtml=False,\n        # Location and Identity Parameters\n        locale=None,            # e.g. \"en-US\", \"fr-FR\"\n        timezone_id=None,       # e.g. \"America/New_York\"\n        geolocation=None,       # GeolocationConfig object\n        # Resource Management\n        enable_rate_limiting=False,\n        rate_limit_config=None,\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=20,\n        display_mode=None,\n        verbose=True,\n        stream=False,  # Enable streaming for arun_many()\n        # ... other advanced parameters omitted\n    ):\n        ...\nCopy\n```\n\n### Key Fields to Note\n  1. **`word_count_threshold`**:\n  2. The minimum word count before a block is considered. \n  3. If your site has lots of short paragraphs or items, you can lower it.\n  4. **`extraction_strategy`**:\n  5. Where you plug in JSON-based extraction (CSS, LLM, etc.). \n  6. If `None`, no structured extraction is done (only raw/cleaned HTML + markdown).\n  7. **`markdown_generator`**:\n  8. E.g., `DefaultMarkdownGenerator(...)`, controlling how HTMLâ†’Markdown conversion is done. \n  9. If `None`, a default approach is used.\n  10. **`cache_mode`**:\n  11. Controls caching behavior (`ENABLED`, `BYPASS`, `DISABLED`, etc.). \n  12. If `None`, defaults to some level of caching or you can specify `CacheMode.ENABLED`.\n  13. **`js_code`**:\n  14. A string or list of JS strings to execute. \n  15. Great for \"Load More\" buttons or user interactions. \n  16. **`wait_for`**:\n  17. A CSS or JS expression to wait for before extracting content. \n  18. Common usage: `wait_for=\"css:.main-loaded\"` or `wait_for=\"js:() => window.loaded === true\"`.\n  19. **`screenshot`**,**`pdf`**, & **`capture_mhtml`**:\n  20. If `True`, captures a screenshot, PDF, or MHTML snapshot after the page is fully loaded. \n  21. The results go to `result.screenshot` (base64), `result.pdf` (bytes), or `result.mhtml` (string).\n  22. **Location Parameters** : \n  23. **`locale`**: Browser's locale (e.g.,`\"en-US\"` , `\"fr-FR\"`) for language preferences\n  24. **`timezone_id`**: Browser's timezone (e.g.,`\"America/New_York\"` , `\"Europe/Paris\"`)\n  25. **`geolocation`**: GPS coordinates via`GeolocationConfig(latitude=48.8566, longitude=2.3522)`\n  26. **`verbose`**:\n  27. Logs additional runtime details. \n  28. Overlaps with the browser's verbosity if also set to `True` in `BrowserConfig`.\n  29. **`enable_rate_limiting`**:\n  30. If `True`, enables rate limiting for batch processing. \n  31. Requires `rate_limit_config` to be set.\n  32. **`memory_threshold_percent`**:\n     * The memory threshold (as a percentage) to monitor. \n     * If exceeded, the crawler will pause or slow down.\n  33. **`check_interval`**:\n     * The interval (in seconds) to check system resources. \n     * Affects how often memory and CPU usage are monitored.\n  34. **`max_session_permit`**:\n     * The maximum number of concurrent crawl sessions. \n     * Helps prevent overwhelming the system.\n  35. **`url_matcher`** & **`match_mode`**:\n     * Enable URL-specific configurations when used with `arun_many()`.\n     * Set `url_matcher` to patterns (glob, function, or list) to match specific URLs.\n     * Use `match_mode` (OR/AND) to control how multiple patterns combine.\n  36. **`display_mode`**:\n     * The display mode for progress information (`DETAILED`, `BRIEF`, etc.). \n     * Affects how much information is printed during the crawl.\n\n\n### Helper Methods\nThe `clone()` method is particularly useful for creating variations of your crawler configuration: \n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200,\n    wait_until=\"networkidle\"\n)\n\n# Create variations for different use cases\nstream_config = base_config.clone(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\ndebug_config = base_config.clone(\n    page_timeout=120000,  # Longer timeout for debugging\n    verbose=True\n)\nCopy\n```\n\nThe `clone()` method: - Creates a new instance with all the same settings - Updates only the specified parameters - Leaves the original configuration unchanged - Perfect for creating variations without repeating all parameters\n## 3. LLMConfig Essentials\n### Key fields to note\n  1. **`provider`**:\n  2. Which LLM provider to use. \n  3. Possible values are `\"ollama/llama3\",\"groq/llama3-70b-8192\",\"groq/llama3-8b-8192\", \"openai/gpt-4o-mini\" ,\"openai/gpt-4o\",\"openai/o1-mini\",\"openai/o1-preview\",\"openai/o3-mini\",\"openai/o3-mini-high\",\"anthropic/claude-3-haiku-20240307\",\"anthropic/claude-3-opus-20240229\",\"anthropic/claude-3-sonnet-20240229\",\"anthropic/claude-3-5-sonnet-20240620\",\"gemini/gemini-pro\",\"gemini/gemini-1.5-pro\",\"gemini/gemini-2.0-flash\",\"gemini/gemini-2.0-flash-exp\",\"gemini/gemini-2.0-flash-lite-preview-02-05\",\"deepseek/deepseek-chat\"`  \n_(default:`\"openai/gpt-4o-mini\"`)_\n  4. **`api_token`**:\n     * Optional. When not provided explicitly, api_token will be read from environment variables based on provider. For example: If a gemini model is passed as provider then,`\"GEMINI_API_KEY\"` will be read from environment variables \n     * API token of LLM provider   \neg: `api_token = \"gsk_1ClHGGJ7Lpn4WGybR7vNWGdyb3FY7zXEw3SCiy0BAVM9lL8CQv\"`\n     * Environment variable - use with prefix \"env:\"   \neg:`api_token = \"env: GROQ_API_KEY\"`\n  5. **`base_url`**:\n  6. If your provider has a custom endpoint\n  7. **Backoff controls** _(optional)_ : \n  8. `backoff_base_delay` _(default`2` seconds)_ â€“ how long to pause before the first retry if the provider rate-limits you. \n  9. `backoff_max_attempts` _(default`3`)_ â€“ total tries for the same prompt (initial call + retries). \n  10. `backoff_exponential_factor` _(default`2`)_ â€“ how quickly the pause grows between retries. A factor of 2 yields waits like 2s â†’ 4s â†’ 8s. \n  11. Because these plug into Crawl4AIâ€™s retry helper, every LLM strategy automatically follows the pacing you define here. \n```\nllm_config = LLMConfig(\n    provider=\"openai/gpt-4o-mini\",\n    api_token=os.getenv(\"OPENAI_API_KEY\"),\n    backoff_base_delay=1, # optional\n    backoff_max_attempts=5, # optional\n    backoff_exponential_factor=3, # optional\n)\nCopy\n```\n\n\n\n## 4. Putting It All Together\nIn a typical scenario, you define **one** `BrowserConfig` for your crawler session, then create **one or more** `CrawlerRunConfig` & `LLMConfig` depending on each call's needs: \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig, LLMContentFilter, DefaultMarkdownGenerator\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    # 1) Browser config: headless, bigger viewport, no proxy\n    browser_conf = BrowserConfig(\n        headless=True,\n        viewport_width=1280,\n        viewport_height=720\n    )\n\n    # 2) Example extraction strategy\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"div.article\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    extraction = JsonCssExtractionStrategy(schema)\n\n    # 3) Example LLM content filtering\n\n    gemini_config = LLMConfig(\n        provider=\"gemini/gemini-1.5-pro\", \n        api_token = \"env:GEMINI_API_TOKEN\"\n    )\n\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config=gemini_config,  # or your preferred provider\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=500,  # Adjust based on your needs\n        verbose=True\n    )\n\n    md_generator = DefaultMarkdownGenerator(\n        content_filter=filter,\n        options={\"ignore_links\": True}\n    )\n\n    # 4) Crawler run config: skip cache, use extraction\n    run_conf = CrawlerRunConfig(\n        markdown_generator=md_generator,\n        extraction_strategy=extraction,\n        cache_mode=CacheMode.BYPASS,\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        # 4) Execute the crawl\n        result = await crawler.arun(url=\"https://example.com/news\", config=run_conf)\n\n        if result.success:\n            print(\"Extracted content:\", result.extracted_content)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 5. Next Steps\n  * [BrowserConfig, CrawlerRunConfig & LLMConfig Reference](https://docs.crawl4ai.com/api/parameters.md)\n  * **Custom Hooks & Auth** (Inject JavaScript or handle login forms). \n  * **Session Management** (Re-use pages, preserve state across multiple calls). \n  * **Advanced Caching** (Fine-tune read/write cache modes). \n\n\n## 6. Conclusion\n# 1. **BrowserConfig** â€“ Controlling the Browser\n`BrowserConfig` focuses on **how** the browser is launched and behaves. This includes headless mode, proxies, user agents, and other environment tweaks. \n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    viewport_width=1280,\n    viewport_height=720,\n    proxy=\"http://user:pass@proxy:8080\",\n    user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\",\n)\nCopy\n```\n\n## 1.1 Parameter Highlights\n**Parameter** | **Type / Default** | **What It Does**  \n---|---|---  \n**`browser_type`**|  `\"chromium\"`, `\"firefox\"`, `\"webkit\"`  \n_(default:`\"chromium\"`)_ | Which browser engine to use. `\"chromium\"` is typical for many sites, `\"firefox\"` or `\"webkit\"` for specialized tests.  \n**`headless`**|  `bool` (default: `True`) | Headless means no visible UI. `False` is handy for debugging.  \n**`viewport_width`**|  `int` (default: `1080`) | Initial page width (in px). Useful for testing responsive layouts.  \n**`viewport_height`**|  `int` (default: `600`) | Initial page height (in px).  \n**`proxy`**|  `str` (deprecated) | Deprecated. Use `proxy_config` instead. If set, it will be auto-converted internally.  \n**`proxy_config`**|  `dict` (default: `None`) | For advanced or multi-proxy needs, specify details like `{\"server\": \"...\", \"username\": \"...\", ...}`.  \n**`use_persistent_context`**|  `bool` (default: `False`) | If `True`, uses a **persistent** browser context (keep cookies, sessions across runs). Also sets `use_managed_browser=True`.  \n**`user_data_dir`**|  `str or None` (default: `None`) | Directory to store user data (profiles, cookies). Must be set if you want permanent sessions.  \n**`ignore_https_errors`**|  `bool` (default: `True`) | If `True`, continues despite invalid certificates (common in dev/staging).  \n**`java_script_enabled`**|  `bool` (default: `True`) | Disable if you want no JS overhead, or if only static content is needed.  \n**`cookies`**|  `list` (default: `[]`) | Pre-set cookies, each a dict like `{\"name\": \"session\", \"value\": \"...\", \"url\": \"...\"}`.  \n**`headers`**|  `dict` (default: `{}`) | Extra HTTP headers for every request, e.g. `{\"Accept-Language\": \"en-US\"}`.  \n**`user_agent`**|  `str` (default: Chrome-based UA) | Your custom or random user agent. `user_agent_mode=\"random\"` can shuffle it.  \n**`light_mode`**|  `bool` (default: `False`) | Disables some background features for performance gains.  \n**`text_mode`**|  `bool` (default: `False`) | If `True`, tries to disable images/other heavy content for speed.  \n**`use_managed_browser`**|  `bool` (default: `False`) | For advanced â€œmanagedâ€ interactions (debugging, CDP usage). Typically set automatically if persistent context is on.  \n**`extra_args`**|  `list` (default: `[]`) | Additional flags for the underlying browser process, e.g. `[\"--disable-extensions\"]`.  \n- Set `headless=False` to visually **debug** how pages load or how interactions proceed. |  |   \n- If you need **authentication** storage or repeated sessions, consider `use_persistent_context=True` and specify `user_data_dir`. |  |   \n- For large pages, you might need a bigger `viewport_width` and `viewport_height` to handle dynamic content. |  |   \n# 2. **CrawlerRunConfig** â€“ Controlling Each Crawl |  |   \nWhile `BrowserConfig` sets up the **environment** , `CrawlerRunConfig` details **how** each **crawl operation** should behave: caching, content filtering, link or domain blocking, timeouts, JavaScript code, etc. |  |   \n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nrun_cfg = CrawlerRunConfig(\n    wait_for=\"css:.main-content\",\n    word_count_threshold=15,\n    excluded_tags=[\"nav\", \"footer\"],\n    exclude_external_links=True,\n    stream=True,  # Enable streaming for arun_many()\n)\nCopy\n```\n|  |   \n## 2.1 Parameter Highlights |  |   \n### A) **Content Processing** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n------------------------------ | -------------------------------------- | -------------------------------------------------------------------------------------------------  \n**`word_count_threshold`**|  `int` (default: ~200) | Skips text blocks below X words. Helps ignore trivial sections.  \n**`extraction_strategy`**|  `ExtractionStrategy` (default: None) | If set, extracts structured data (CSS-based, LLM-based, etc.).  \n**`markdown_generator`**|  `MarkdownGenerationStrategy` (None) | If you want specialized markdown output (citations, filtering, chunking, etc.). Can be customized with options such as `content_source` parameter to select the HTML input source ('cleaned_html', 'raw_html', or 'fit_html').  \n**`css_selector`**|  `str` (None) | Retains only the part of the page matching this selector. Affects the entire extraction process.  \n**`target_elements`**|  `List[str]` (None) | List of CSS selectors for elements to focus on for markdown generation and data extraction, while still processing the entire page for links, media, etc. Provides more flexibility than `css_selector`.  \n**`excluded_tags`**|  `list` (None) | Removes entire tags (e.g. `[\"script\", \"style\"]`).  \n**`excluded_selector`**|  `str` (None) | Like `css_selector` but to exclude. E.g. `\"#ads, .tracker\"`.  \n**`only_text`**|  `bool` (False) | If `True`, tries to extract text-only content.  \n**`prettiify`**|  `bool` (False) | If `True`, beautifies final HTML (slower, purely cosmetic).  \n**`keep_data_attributes`**|  `bool` (False) | If `True`, preserve `data-*` attributes in cleaned HTML.  \n**`remove_forms`**|  `bool` (False) | If `True`, remove all `<form>` elements.  \n### B) **Caching & Session** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n------------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------  \n**`cache_mode`**| `CacheMode or None` | Controls how caching is handled (`ENABLED`, `BYPASS`, `DISABLED`, etc.). If `None`, typically defaults to `ENABLED`.  \n**`session_id`**| `str or None` | Assign a unique ID to reuse a single browser session across multiple `arun()` calls.  \n**`bypass_cache`**|  `bool` (False) | If `True`, acts like `CacheMode.BYPASS`.  \n**`disable_cache`**|  `bool` (False) | If `True`, acts like `CacheMode.DISABLED`.  \n**`no_cache_read`**|  `bool` (False) | If `True`, acts like `CacheMode.WRITE_ONLY` (writes cache but never reads).  \n**`no_cache_write`**|  `bool` (False) | If `True`, acts like `CacheMode.READ_ONLY` (reads cache but never writes).  \n### C) **Page Navigation & Timing** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n---------------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------  \n**`wait_until`**|  `str` (domcontentloaded) | Condition for navigation to â€œcompleteâ€. Often `\"networkidle\"` or `\"domcontentloaded\"`.  \n**`page_timeout`**|  `int` (60000 ms) | Timeout for page navigation or JS steps. Increase for slow sites.  \n**`wait_for`**| `str or None` | Wait for a CSS (`\"css:selector\"`) or JS (`\"js:() => bool\"`) condition before content extraction.  \n**`wait_for_images`**|  `bool` (False) | Wait for images to load before finishing. Slows down if you only want text.  \n**`delay_before_return_html`**|  `float` (0.1) | Additional pause (seconds) before final HTML is captured. Good for last-second updates.  \n**`check_robots_txt`**|  `bool` (False) | Whether to check and respect robots.txt rules before crawling. If True, caches robots.txt for efficiency.  \n**`mean_delay`**and**`max_range`** |  `float` (0.1, 0.3) | If you call `arun_many()`, these define random delay intervals between crawls, helping avoid detection or rate limits.  \n**`semaphore_count`**|  `int` (5) | Max concurrency for `arun_many()`. Increase if you have resources for parallel crawls.  \n### D) **Page Interaction** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n---------------------------- | -------------------------------- | -----------------------------------------------------------------------------------------------------------------------------------------  \n**`js_code`**|  `str or list[str]` (None) | JavaScript to run after load. E.g. `\"document.querySelector('button')?.click();\"`.  \n**`js_only`**|  `bool` (False) | If `True`, indicates weâ€™re reusing an existing session and only applying JS. No full reload.  \n**`ignore_body_visibility`**|  `bool` (True) | Skip checking if `<body>` is visible. Usually best to keep `True`.  \n**`scan_full_page`**|  `bool` (False) | If `True`, auto-scroll the page to load dynamic content (infinite scroll).  \n**`scroll_delay`**|  `float` (0.2) | Delay between scroll steps if `scan_full_page=True`.  \n**`process_iframes`**|  `bool` (False) | Inlines iframe content for single-page extraction.  \n**`remove_overlay_elements`**|  `bool` (False) | Removes potential modals/popups blocking the main content.  \n**`simulate_user`**|  `bool` (False) | Simulate user interactions (mouse movements) to avoid bot detection.  \n**`override_navigator`**|  `bool` (False) | Override `navigator` properties in JS for stealth.  \n**`magic`**|  `bool` (False) | Automatic handling of popups/consent banners. Experimental.  \n**`adjust_viewport_to_content`**|  `bool` (False) | Resizes viewport to match page content height.  \nIf your page is a single-page app with repeated JS updates, set `js_only=True` in subsequent calls, plus a `session_id` for reusing the same tab. |  |   \n### E) **Media Handling** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n-------------------------------------------- | --------------------- | -----------------------------------------------------------------------------------------------------------  \n**`screenshot`**|  `bool` (False) | Capture a screenshot (base64) in `result.screenshot`.  \n**`screenshot_wait_for`**| `float or None` | Extra wait time before the screenshot.  \n**`screenshot_height_threshold`**|  `int` (~20000) | If the page is taller than this, alternate screenshot strategies are used.  \n**`pdf`**|  `bool` (False) | If `True`, returns a PDF in `result.pdf`.  \n**`capture_mhtml`**|  `bool` (False) | If `True`, captures an MHTML snapshot of the page in `result.mhtml`. MHTML includes all page resources (CSS, images, etc.) in a single file.  \n**`image_description_min_word_threshold`**|  `int` (~50) | Minimum words for an imageâ€™s alt text or description to be considered valid.  \n**`image_score_threshold`**|  `int` (~3) | Filter out low-scoring images. The crawler scores images by relevance (size, context, etc.).  \n**`exclude_external_images`**|  `bool` (False) | Exclude images from other domains.  \n### F) **Link/Domain Handling** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n------------------------------ | ------------------------- | -----------------------------------------------------------------------------------------------------------------------------  \n**`exclude_social_media_domains`**|  `list` (e.g. Facebook/Twitter) | A default list can be extended. Any link to these domains is removed from final output.  \n**`exclude_external_links`**|  `bool` (False) | Removes all links pointing outside the current domain.  \n**`exclude_social_media_links`**|  `bool` (False) | Strips links specifically to social sites (like Facebook or Twitter).  \n**`exclude_domains`**|  `list` ([]) | Provide a custom list of domains to exclude (like `[\"ads.com\", \"trackers.io\"]`).  \n**`preserve_https_for_internal_links`**|  `bool` (False) | If `True`, preserves HTTPS scheme for internal links even when the server redirects to HTTP. Useful for security-conscious crawling.  \n### G) **Debug & Logging** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n---------------- | -------------------- | ---------------------------------------------------------------------------  \n**`verbose`**|  `bool` (True) | Prints logs detailing each step of crawling, interactions, or errors.  \n**`log_console`**|  `bool` (False) | Logs the pageâ€™s JavaScript console output if you want deeper JS debugging.  \n### H) **Virtual Scroll Configuration** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n------------------------------ | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------  \n**`virtual_scroll_config`**|  `VirtualScrollConfig or dict` (None) | Configuration for handling virtualized scrolling on sites like Twitter/Instagram where content is replaced rather than appended.  \nWhen sites use virtual scrolling (content replaced as you scroll), use `VirtualScrollConfig`: |  |   \n```\nfrom crawl4ai import VirtualScrollConfig\n\nvirtual_config = VirtualScrollConfig(\n    container_selector=\"#timeline\",    # CSS selector for scrollable container\n    scroll_count=30,                   # Number of times to scroll\n    scroll_by=\"container_height\",      # How much to scroll: \"container_height\", \"page_height\", or pixels (e.g. 500)\n    wait_after_scroll=0.5             # Seconds to wait after each scroll for content to load\n)\n\nconfig = CrawlerRunConfig(\n    virtual_scroll_config=virtual_config\n)\nCopy\n```\n|  |   \n**VirtualScrollConfig Parameters:** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n------------------------ | --------------------------- | -------------------------------------------------------------------------------------------  \n**`container_selector`**|  `str` (required) | CSS selector for the scrollable container (e.g., `\"#feed\"`, `\".timeline\"`)  \n**`scroll_count`**|  `int` (10) | Maximum number of scrolls to perform  \n**`scroll_by`**|  `str or int` (\"container_height\") | Scroll amount: `\"container_height\"`, `\"page_height\"`, or pixels (e.g., `500`)  \n**`wait_after_scroll`**|  `float` (0.5) | Time in seconds to wait after each scroll for new content to load  \n- Use `virtual_scroll_config` when content is **replaced** during scroll (Twitter, Instagram) |  |   \n- Use `scan_full_page` when content is **appended** during scroll (traditional infinite scroll) |  |   \n### I) **URL Matching Configuration** |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n------------------------ | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------  \n**`url_matcher`**|  `UrlMatcher` (None) | Pattern(s) to match URLs against. Can be: string (glob), function, or list of mixed types. **None means match ALL URLs**  \n**`match_mode`**|  `MatchMode` (MatchMode.OR) | How to combine multiple matchers in a list: `MatchMode.OR` (any match) or `MatchMode.AND` (all must match)  \nThe `url_matcher` parameter enables URL-specific configurations when used with `arun_many()`: |  |   \n```\nfrom crawl4ai import CrawlerRunConfig, MatchMode\nfrom crawl4ai.processors.pdf import PDFContentScrapingStrategy\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\n# Simple string pattern (glob-style)\npdf_config = CrawlerRunConfig(\n    url_matcher=\"*.pdf\",\n    scraping_strategy=PDFContentScrapingStrategy()\n)\n\n# Multiple patterns with OR logic (default)\nblog_config = CrawlerRunConfig(\n    url_matcher=[\"*/blog/*\", \"*/article/*\", \"*/news/*\"],\n    match_mode=MatchMode.OR  # Any pattern matches\n)\n\n# Function matcher\napi_config = CrawlerRunConfig(\n    url_matcher=lambda url: 'api' in url or url.endswith('.json'),\n    # Other settings like extraction_strategy\n)\n\n# Mixed: String + Function with AND logic\ncomplex_config = CrawlerRunConfig(\n    url_matcher=[\n        lambda url: url.startswith('https://'),  # Must be HTTPS\n        \"*.org/*\",                               # Must be .org domain\n        lambda url: 'docs' in url                # Must contain 'docs'\n    ],\n    match_mode=MatchMode.AND  # ALL conditions must match\n)\n\n# Combined patterns and functions with AND logic\nsecure_docs = CrawlerRunConfig(\n    url_matcher=[\"https://*\", lambda url: '.doc' in url],\n    match_mode=MatchMode.AND  # Must be HTTPS AND contain .doc\n)\n\n# Default config - matches ALL URLs\ndefault_config = CrawlerRunConfig()  # No url_matcher = matches everything\nCopy\n```\n|  |   \n**UrlMatcher Types:** |  |   \n- **None (default)** : When `url_matcher` is None or not set, the config matches ALL URLs |  |   \n- **String patterns** : Glob-style patterns like `\"*.pdf\"`, `\"*/api/*\"`, `\"https://*.example.com/*\"` |  |   \n- **Functions** : `lambda url: bool` - Custom logic for complex matching |  |   \n- **Lists** : Mix strings and functions, combined with `MatchMode.OR` or `MatchMode.AND` |  |   \n**Important Behavior:** |  |   \n- When passing a list of configs to `arun_many()`, URLs are matched against each config's `url_matcher` in order. First match wins! |  |   \n- If no config matches a URL and there's no default config (one without `url_matcher`), the URL will fail with \"No matching configuration found\" |  |   \nBoth `BrowserConfig` and `CrawlerRunConfig` provide a `clone()` method to create modified copies: |  |   \n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200\n)\n\n# Create variations using clone()\nstream_config = base_config.clone(stream=True)\nno_cache_config = base_config.clone(\n    cache_mode=CacheMode.BYPASS,\n    stream=True\n)\nCopy\n```\n|  |   \nThe `clone()` method is particularly useful when you need slightly different configurations for different use cases, without modifying the original config. |  |   \n## 2.3 Example Usage |  |   \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Configure the browser\n    browser_cfg = BrowserConfig(\n        headless=False,\n        viewport_width=1280,\n        viewport_height=720,\n        proxy=\"http://user:pass@myproxy:8080\",\n        text_mode=True\n    )\n\n    # Configure the run\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        session_id=\"my_session\",\n        css_selector=\"main.article\",\n        excluded_tags=[\"script\", \"style\"],\n        exclude_external_links=True,\n        wait_for=\"css:.article-loaded\",\n        screenshot=True,\n        stream=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/news\",\n            config=run_cfg\n        )\n        if result.success:\n            print(\"Final cleaned_html length:\", len(result.cleaned_html))\n            if result.screenshot:\n                print(\"Screenshot captured (base64, length):\", len(result.screenshot))\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n|  |   \n## 2.4 Compliance & Ethics |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n----------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------  \n**`check_robots_txt`**|  `bool` (False) | When True, checks and respects robots.txt rules before crawling. Uses efficient caching with SQLite backend.  \n**`user_agent`**|  `str` (None) | User agent string to identify your crawler. Used for robots.txt checking when enabled.  \n```\nrun_config = CrawlerRunConfig(\n    check_robots_txt=True,  # Enable robots.txt compliance\n    user_agent=\"MyBot/1.0\"  # Identify your crawler\n)\nCopy\n```\n|  |   \n# 3. **LLMConfig** - Setting up LLM providers |  |   \n1. LLMExtractionStrategy |  |   \n2. LLMContentFilter |  |   \n3. JsonCssExtractionStrategy.generate_schema |  |   \n4. JsonXPathExtractionStrategy.generate_schema |  |   \n## 3.1 Parameters |  |   \n**Parameter** | **Type / Default** | **What It Does**  \n----------------------- | ---------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------  \n**`provider`**|  `\"ollama/llama3\",\"groq/llama3-70b-8192\",\"groq/llama3-8b-8192\", \"openai/gpt-4o-mini\" ,\"openai/gpt-4o\",\"openai/o1-mini\",\"openai/o1-preview\",\"openai/o3-mini\",\"openai/o3-mini-high\",\"anthropic/claude-3-haiku-20240307\",\"anthropic/claude-3-opus-20240229\",\"anthropic/claude-3-sonnet-20240229\",\"anthropic/claude-3-5-sonnet-20240620\",\"gemini/gemini-pro\",\"gemini/gemini-1.5-pro\",\"gemini/gemini-2.0-flash\",\"gemini/gemini-2.0-flash-exp\",\"gemini/gemini-2.0-flash-lite-preview-02-05\",\"deepseek/deepseek-chat\"`  \n_(default:`\"openai/gpt-4o-mini\"`)_ | Which LLM provider to use.  \n**`api_token`**|  1.Optional. When not provided explicitly, api_token will be read from environment variables based on provider. For example: If a gemini model is passed as provider then,`\"GEMINI_API_KEY\"` will be read from environment variables   \n2. API token of LLM provider   \neg: `api_token = \"gsk_1ClHGGJ7Lpn4WGybR7vNWGdyb3FY7zXEw3SCiy0BAVM9lL8CQv\"`   \n3. Environment variable - use with prefix \"env:\"   \neg:`api_token = \"env: GROQ_API_KEY\"` | API token to use for the given provider  \n**`base_url`**|  Optional. Custom API endpoint | If your provider has a custom endpoint  \n## 3.2 Example Usage |  |   \n```\nllm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))\nCopy\n```\n|  |   \n## 4. Putting It All Together |  |   \n- **Use** `BrowserConfig` for **global** browser settings: engine, headless, proxy, user agent. |  |   \n- **Use** `CrawlerRunConfig` for each crawlâ€™s **context** : how to filter content, handle caching, wait for dynamic elements, or run JS. |  |   \n- **Pass** both configs to `AsyncWebCrawler` (the `BrowserConfig`) and then to `arun()` (the `CrawlerRunConfig`). |  |   \n- **Use** `LLMConfig` for LLM provider configurations that can be used across all extraction, filtering, schema generation tasks. Can be used in - `LLMExtractionStrategy`, `LLMContentFilter`, `JsonCssExtractionStrategy.generate_schema` & `JsonXPathExtractionStrategy.generate_schema` |  |   \n```\n# Create a modified copy with the clone() method\nstream_cfg = run_cfg.clone(\n    stream=True,\n    cache_mode=CacheMode.BYPASS\n)\nCopy\n```\n|  |   \n# Crawling Patterns\n# Simple Crawling\n## Basic Usage\nSet up a simple crawl using `BrowserConfig` and `CrawlerRunConfig`: \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_config = BrowserConfig()  # Default browser configuration\n    run_config = CrawlerRunConfig()   # Default crawl run configuration\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Understanding the Response\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](https://docs.crawl4ai.com/api/crawl-result.md) for complete details): \n```\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.6),\n        options={\"ignore_links\": True}\n    )\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=config\n)\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown.raw_markdown) # Raw markdown from cleaned html\nprint(result.markdown.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\nCopy\n```\n\n## Adding Basic Options\nCustomize your crawl using `CrawlerRunConfig`: \n```\nrun_config = CrawlerRunConfig(\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=run_config\n)\nCopy\n```\n\n## Handling Errors\n```\nrun_config = CrawlerRunConfig()\nresult = await crawler.arun(url=\"https://example.com\", config=run_config)\n\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\nCopy\n```\n\n## Logging and Debugging\nEnable verbose logging in `BrowserConfig`: \n```\nbrowser_config = BrowserConfig(verbose=True)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    run_config = CrawlerRunConfig()\n    result = await crawler.arun(url=\"https://example.com\", config=run_config)\nCopy\n```\n\n## Complete Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        # Content filtering\n        word_count_threshold=10,\n        excluded_tags=['form', 'header'],\n        exclude_external_links=True,\n\n        # Content processing\n        process_iframes=True,\n        remove_overlay_elements=True,\n\n        # Cache control\n        cache_mode=CacheMode.ENABLED  # Use cache if available\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n\n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n\n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n\n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n\n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n# Content Processing\n# Markdown Generation Basics\n  1. How to configure the **Default Markdown Generator**\n  2. The difference between raw markdown (`result.markdown`) and filtered markdown (`fit_markdown`) \n>      * You know how to configure `CrawlerRunConfig`.\n\n\n## 1. Quick Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator()\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n\n        if result.success:\n            print(\"Raw Markdown Output:\\n\")\n            print(result.markdown)  # The unfiltered markdown from the page\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- `CrawlerRunConfig( markdown_generator = DefaultMarkdownGenerator() )` instructs Crawl4AI to convert the final HTML into markdown at the end of each crawl.  \n- The resulting markdown is accessible via `result.markdown`.\n## 2. How Markdown Generation Works\n### 2.1 HTML-to-Text Conversion (Forked & Modified)\n  * Preserves headings, code blocks, bullet points, etc. \n  * Removes extraneous tags (scripts, styles) that donâ€™t add meaningful content. \n  * Can optionally generate references for links or skip them altogether.\n\n\n### 2.2 Link Citations & References\nBy default, the generator can convert `<a href=\"...\">` elements into `[text][1]` citations, then place the actual links at the bottom of the document. This is handy for research workflows that demand references in a structured manner.\n### 2.3 Optional Content Filters\n## 3. Configuring the Default Markdown Generator\nYou can tweak the output by passing an `options` dict to `DefaultMarkdownGenerator`. For example: \n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters\n    md_generator = DefaultMarkdownGenerator(\n        options={\n            \"ignore_links\": True,\n            \"escape_html\": False,\n            \"body_width\": 80\n        }\n    )\n\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/docs\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown[:500])  # Just a snippet\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\nSome commonly used `options`: - **`ignore_links`**(bool): Whether to remove all hyperlinks in the final markdown.  \n- **`ignore_images`**(bool): Remove all`![image]()` references.  \n- **`escape_html`**(bool): Turn HTML entities into text (default is often`True`).  \n- **`body_width`**(int): Wrap text at N characters.`0` or `None` means no wrapping.  \n- **`skip_internal_links`**(bool): If`True` , omit `#localAnchors` or internal links referencing the same page.  \n- **`include_sup_sub`**(bool): Attempt to handle`<sup>` / `<sub>` in a more readable way.\n## 4. Selecting the HTML Source for Markdown Generation\nThe `content_source` parameter allows you to control which HTML content is used as input for markdown generation. This gives you flexibility in how the HTML is processed before conversion to markdown. \n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Option 1: Use the raw HTML directly from the webpage (before any processing)\n    raw_md_generator = DefaultMarkdownGenerator(\n        content_source=\"raw_html\",\n        options={\"ignore_links\": True}\n    )\n\n    # Option 2: Use the cleaned HTML (after scraping strategy processing - default)\n    cleaned_md_generator = DefaultMarkdownGenerator(\n        content_source=\"cleaned_html\",  # This is the default\n        options={\"ignore_links\": True}\n    )\n\n    # Option 3: Use preprocessed HTML optimized for schema extraction\n    fit_md_generator = DefaultMarkdownGenerator(\n        content_source=\"fit_html\",\n        options={\"ignore_links\": True}\n    )\n\n    # Use one of the generators in your crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=raw_md_generator  # Try each of the generators\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown.raw_markdown[:500])\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\n### HTML Source Options\n  * **`\"cleaned_html\"`**(default): Uses the HTML after it has been processed by the scraping strategy. This HTML is typically cleaner and more focused on content, with some boilerplate removed.\n  * **`\"raw_html\"`**: Uses the original HTML directly from the webpage, before any cleaning or processing. This preserves more of the original content, but may include navigation bars, ads, footers, and other elements that might not be relevant to the main content.\n  * **`\"fit_html\"`**: Uses HTML preprocessed for schema extraction. This HTML is optimized for structured data extraction and may have certain elements simplified or removed.\n\n\n### When to Use Each Option\n  * Use **`\"cleaned_html\"`**(default) for most cases where you want a balance of content preservation and noise removal.\n  * Use **`\"raw_html\"`**when you need to preserve all original content, or when the cleaning process is removing content you actually want to keep.\n  * Use **`\"fit_html\"`**when working with structured data or when you need HTML that's optimized for schema extraction.\n\n\n## 5. Content Filters\n### 5.1 BM25ContentFilter\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai import CrawlerRunConfig\n\nbm25_filter = BM25ContentFilter(\n    user_query=\"machine learning\",\n    bm25_threshold=1.2,\n    language=\"english\"\n)\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=bm25_filter,\n    options={\"ignore_links\": True}\n)\n\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n- **`user_query`**: The term you want to focus on. BM25 tries to keep only content blocks relevant to that query.  \n- **`bm25_threshold`**: Raise it to keep fewer blocks; lower it to keep more.  \n- **`use_stemming`**_(default`True`)_: Whether to apply stemming to the query and content. - **`language (str)`**: Language for stemming (default: 'english').\n### 5.2 PruningContentFilter\nIf you **donâ€™t** have a specific query, or if you just want a robust â€œjunk remover,â€ use `PruningContentFilter`. It analyzes text density, link density, HTML structure, and known patterns (like â€œnav,â€ â€œfooterâ€) to systematically prune extraneous or repetitive sections. \n```\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",  # or \"dynamic\"\n    min_word_threshold=50\n)\nCopy\n```\n\n- **`threshold`**: Score boundary. Blocks below this score get removed.  \n- **`threshold_type`**:  \n- `\"fixed\"`: Straight comparison (`score >= threshold` keeps the block).  \n- `\"dynamic\"`: The filter adjusts threshold in a data-driven manner.  \n- **`min_word_threshold`**: Discard blocks under N words as likely too short or unhelpful. - You want a broad cleanup without a user query.\n### 5.3 LLMContentFilter\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig, DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\nasync def main():\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-api-token\"), #or use environment variable\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=4096,  # Adjust based on your needs\n        verbose=True\n    )\n    md_generator = DefaultMarkdownGenerator(\n        content_filter=filter,\n        options={\"ignore_links\": True}\n    )\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        print(result.markdown.fit_markdown)  # Filtered markdown content\nCopy\n```\n\n- **Chunk Processing** : Handles large documents by processing them in chunks (controlled by `chunk_token_threshold`) - **Parallel Processing** : For better performance, use smaller `chunk_token_threshold` (e.g., 2048 or 4096) to enable parallel processing of content chunks 1. **Exact Content Preservation** : \n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Extract the main educational content while preserving its original wording and substance completely.\n    1. Maintain the exact language and terminology\n    2. Keep all technical explanations and examples intact\n    3. Preserve the original flow and structure\n    4. Remove only clearly irrelevant elements like navigation menus and ads\n    \"\"\",\n    chunk_token_threshold=4096\n)\nCopy\n```\n\n2. **Focused Content Extraction** : \n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Focus on extracting specific types of content:\n    - Technical documentation\n    - Code examples\n    - API references\n    Reformat the content into clear, well-structured markdown\n    \"\"\",\n    chunk_token_threshold=4096\n)\nCopy\n```\n\n> **Performance Tip** : Set a smaller `chunk_token_threshold` (e.g., 2048 or 4096) to enable parallel processing of content chunks. The default value is infinity, which processes the entire content as a single chunk.\n## 6. Using Fit Markdown\nWhen a content filter is active, the library produces two forms of markdown inside `result.markdown`: 1. **`raw_markdown`**: The full unfiltered markdown.  \n2. **`fit_markdown`**: A â€œfitâ€ version where the filter has removed or trimmed noisy segments.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.6),\n            options={\"ignore_links\": True}\n        )\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://news.example.com/tech\", config=config)\n        if result.success:\n            print(\"Raw markdown:\\n\", result.markdown)\n\n            # If a filter is used, we also have .fit_markdown:\n            md_object = result.markdown  # or your equivalent\n            print(\"Filtered markdown:\\n\", md_object.fit_markdown)\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 7. The `MarkdownGenerationResult` Object\nIf your library stores detailed markdown output in an object like `MarkdownGenerationResult`, youâ€™ll see fields such as: - **`raw_markdown`**: The direct HTML-to-markdown transformation (no filtering).  \n- **`markdown_with_citations`**: A version that moves links to reference-style footnotes.  \n- **`references_markdown`**: A separate string or section containing the gathered references.  \n- **`fit_markdown`**: The filtered markdown if you used a content filter.  \n- **`fit_html`**: The corresponding HTML snippet used to generate`fit_markdown` (helpful for debugging or advanced usage). \n```\nmd_obj = result.markdown  # your libraryâ€™s naming may vary\nprint(\"RAW:\\n\", md_obj.raw_markdown)\nprint(\"CITED:\\n\", md_obj.markdown_with_citations)\nprint(\"REFERENCES:\\n\", md_obj.references_markdown)\nprint(\"FIT:\\n\", md_obj.fit_markdown)\nCopy\n```\n\n- You can supply `raw_markdown` to an LLM if you want the entire text.  \n- Or feed `fit_markdown` into a vector database to reduce token usage.  \n- `references_markdown` can help you keep track of link provenance.\n## 8. Combining Filters (BM25 + Pruning) in Two Passes\nYou might want to **prune out** noisy boilerplate first (with `PruningContentFilter`), and then **rank whatâ€™s left** against a user query (with `BM25ContentFilter`). You donâ€™t have to crawl the page twice. Instead: 1. **First pass** : Apply `PruningContentFilter` directly to the raw HTML from `result.html` (the crawlerâ€™s downloaded HTML).  \n2. **Second pass** : Take the pruned HTML (or text) from step 1, and feed it into `BM25ContentFilter`, focusing on a user query.\n### Two-Pass Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom bs4 import BeautifulSoup\n\nasync def main():\n    # 1. Crawl with minimal or no markdown generator, just get raw HTML\n    config = CrawlerRunConfig(\n        # If you only want raw HTML, you can skip passing a markdown_generator\n        # or provide one but focus on .html in this example\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/tech-article\", config=config)\n\n        if not result.success or not result.html:\n            print(\"Crawl failed or no HTML content.\")\n            return\n\n        raw_html = result.html\n\n        # 2. First pass: PruningContentFilter on raw HTML\n        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)\n\n        # filter_content returns a list of \"text chunks\" or cleaned HTML sections\n        pruned_chunks = pruning_filter.filter_content(raw_html)\n        # This list is basically pruned content blocks, presumably in HTML or text form\n\n        # For demonstration, let's combine these chunks back into a single HTML-like string\n        # or you could do further processing. It's up to your pipeline design.\n        pruned_html = \"\\n\".join(pruned_chunks)\n\n        # 3. Second pass: BM25ContentFilter with a user query\n        bm25_filter = BM25ContentFilter(\n            user_query=\"machine learning\",\n            bm25_threshold=1.2,\n            language=\"english\"\n        )\n\n        # returns a list of text chunks\n        bm25_chunks = bm25_filter.filter_content(pruned_html)  \n\n        if not bm25_chunks:\n            print(\"Nothing matched the BM25 query after pruning.\")\n            return\n\n        # 4. Combine or display final results\n        final_text = \"\\n---\\n\".join(bm25_chunks)\n\n        print(\"==== PRUNED OUTPUT (first pass) ====\")\n        print(pruned_html[:500], \"... (truncated)\")  # preview\n\n        print(\"\\n==== BM25 OUTPUT (second pass) ====\")\n        print(final_text[:500], \"... (truncated)\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### Whatâ€™s Happening?\n1. **Raw HTML** : We crawl once and store the raw HTML in `result.html`.  \n4. **BM25ContentFilter** : We feed the pruned string into `BM25ContentFilter` with a user query. This second pass further narrows the content to chunks relevant to â€œmachine learning.â€ **No Re-Crawling** : We used `raw_html` from the first pass, so thereâ€™s no need to run `arun()` againâ€”**no second network request**.\n### Tips & Variations\n  * **Plain Text vs. HTML** : If your pruned output is mostly text, BM25 can still handle it; just keep in mind it expects a valid string input. If you supply partial HTML (like `\"<p>some text</p>\"`), it will parse it as HTML. \n  * **Adjust Thresholds** : If you see too much or too little text in step one, tweak `threshold=0.5` or `min_word_threshold=50`. Similarly, `bm25_threshold=1.2` can be raised/lowered for more or fewer chunks in step two.\n\n\n### One-Pass Combination?\n## 9. Common Pitfalls & Tips\n1. **No Markdown Output?**  \n2. **Performance Considerations**  \n- Very large pages with multiple filters can be slower. Consider `cache_mode` to avoid re-downloading.  \n3. **Take Advantage of`fit_markdown`**  \n4. **Adjusting`html2text` Options**  \n- If you see lots of raw HTML slipping into the text, turn on `escape_html`.  \n- If code blocks look messy, experiment with `mark_code` or `handle_code_in_pre`.\n## 10. Summary & Next Steps\n  * Configure the **DefaultMarkdownGenerator** with HTML-to-text options. \n  * Select different HTML sources using the `content_source` parameter. \n  * Distinguish between raw and filtered markdown (`fit_markdown`). \n  * Leverage the `MarkdownGenerationResult` object to handle different forms of output (citations, references, etc.).\n\n\n# Fit Markdown with Pruning & BM25\n## 1. How â€œFit Markdownâ€ Works\n### 1.1 The `content_filter`\nIn **`CrawlerRunConfig`**, you can specify a**`content_filter`**to shape how content is pruned or ranked before final markdown generation. A filterâ€™s logic is applied**before** or **during** the HTMLâ†’Markdown process, producing: - **`result.markdown.raw_markdown`**(unfiltered) -**`result.markdown.fit_markdown`**(filtered or â€œfitâ€ version) -**`result.markdown.fit_html`**(the corresponding HTML snippet that produced`fit_markdown`)\n### 1.2 Common Filters\n## 2. PruningContentFilter\n### 2.1 Usage Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # Step 1: Create a pruning filter\n    prune_filter = PruningContentFilter(\n        # Lower â†’ more content retained, higher â†’ more content pruned\n        threshold=0.45,           \n        # \"fixed\" or \"dynamic\"\n        threshold_type=\"dynamic\",  \n        # Ignore nodes with <5 words\n        min_word_threshold=5      \n    )\n\n    # Step 2: Insert it into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n\n    # Step 3: Pass it to CrawlerRunConfig\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n\n        if result.success:\n            # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n            print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n            print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 2.2 Key Parameters\n  * **`min_word_threshold`**(int): If a block has fewer words than this, itâ€™s pruned.\n  * **`threshold_type`**(str):\n  * `\"fixed\"` â†’ each node must exceed `threshold` (0â€“1). \n  * `\"dynamic\"` â†’ node scoring adjusts according to tag type, text/link density, etc. \n  * **`threshold`**(float, default ~0.48): The base or â€œanchorâ€ cutoff.\n  * **Link density** â€“ Penalizes sections that are mostly links. \n  * **Tag importance** â€“ e.g., an `<article>` or `<p>` might be more important than a `<div>`. \n\n\n## 3. BM25ContentFilter\n### 3.1 Usage Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # 1) A BM25 filter with a user query\n    bm25_filter = BM25ContentFilter(\n        user_query=\"startup fundraising tips\",\n        # Adjust for stricter or looser results\n        bm25_threshold=1.2  \n    )\n\n    # 2) Insert into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\n\n    # 3) Pass to crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        if result.success:\n            print(\"Fit Markdown (BM25 query-based):\")\n            print(result.markdown.fit_markdown)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 3.2 Parameters\n  * **`user_query`**(str, optional): E.g.`\"machine learning\"`. If blank, the filter tries to glean a query from page metadata. \n  * **`bm25_threshold`**(float, default 1.0):\n  * Higher â†’ fewer chunks but more relevant. \n  * Lower â†’ more inclusive. \n> In more advanced scenarios, you might see parameters like `language`, `case_sensitive`, or `priority_tags` to refine how text is tokenized or weighted.\n\n\n## 4. Accessing the â€œFitâ€ Output\nAfter the crawl, your â€œfitâ€ content is found in **`result.markdown.fit_markdown`**.\n```\nfit_md = result.markdown.fit_markdown\nfit_html = result.markdown.fit_html\nCopy\n```\n\nIf the content filter is **BM25** , you might see additional logic or references in `fit_markdown` that highlight relevant segments. If itâ€™s **Pruning** , the text is typically well-cleaned but not necessarily matched to a query.\n## 5. Code Patterns Recap\n### 5.1 Pruning\n```\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",\n    min_word_threshold=10\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n### 5.2 BM25\n```\nbm25_filter = BM25ContentFilter(\n    user_query=\"health benefits fruit\",\n    bm25_threshold=1.2\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n## 6. Combining with â€œword_count_thresholdâ€ & Exclusions\n```\nconfig = CrawlerRunConfig(\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\", \"header\"],\n    exclude_external_links=True,\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.5)\n    )\n)\nCopy\n```\n\n1. The crawlerâ€™s `excluded_tags` are removed from the HTML first.  \n3. The final â€œfitâ€ content is generated in `result.markdown.fit_markdown`.\n## 7. Custom Filters\nIf you need a different approach (like a specialized ML model or site-specific heuristics), you can create a new class inheriting from `RelevantContentFilter` and implement `filter_content(html)`. Then inject it into your **markdown generator** : \n```\nfrom crawl4ai.content_filter_strategy import RelevantContentFilter\n\nclass MyCustomFilter(RelevantContentFilter):\n    def filter_content(self, html, min_word_threshold=None):\n        # parse HTML, implement custom logic\n        return [block for block in ... if ... some condition...]\nCopy\n```\n\n1. Subclass `RelevantContentFilter`.  \n2. Implement `filter_content(...)`.  \n3. Use it in your `DefaultMarkdownGenerator(content_filter=MyCustomFilter(...))`.\n## 8. Final Thoughts\n  * **Summaries** : Quickly get the important text from a cluttered page. \n  * **Search** : Combine with **BM25** to produce content relevant to a query. \n  * **BM25ContentFilter** : Perfect for query-based extraction or searching. \n  * Combine with **`excluded_tags`,`exclude_external_links` , `word_count_threshold`** to refine your final â€œfitâ€ text. \n  * Fit markdown ends up in **`result.markdown.fit_markdown`**; eventually**`result.markdown.fit_markdown`**in future versions.\n  * Last Updated: 2025-01-01\n\n\n# Content Selection\nCrawl4AI provides multiple ways to **select** , **filter** , and **refine** the content from your crawls. Whether you need to target a specific CSS region, exclude entire tags, filter out external links, or remove certain domains and images, **`CrawlerRunConfig`**offers a wide range of parameters.\n## 1. CSS-Based Selection\nThere are two ways to select content from a page: using `css_selector` or the more flexible `target_elements`.\n### 1.1 Using `css_selector`\nA straightforward way to **limit** your crawl results to a certain region of the page is **`css_selector`**in**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # e.g., first 30 items from Hacker News\n        css_selector=\".athing:nth-child(-n+30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        print(\"Partial HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Result** : Only elements matching that selector remain in `result.cleaned_html`.\n### 1.2 Using `target_elements`\nThe `target_elements` parameter provides more flexibility by allowing you to target **multiple elements** for content extraction while preserving the entire page context for other features: \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Target article body and sidebar, but not other content\n        target_elements=[\"article.main-content\", \"aside.sidebar\"]\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog-post\", \n            config=config\n        )\n        print(\"Markdown focused on target elements\")\n        print(\"Links from entire page still available:\", len(result.links.get(\"internal\", [])))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key difference** : With `target_elements`, the markdown generation and structural data extraction focus on those elements, but other page elements (like links, images, and tables) are still extracted from the entire page. This gives you fine-grained control over what appears in your markdown content while preserving full page context for link analysis and media collection.\n## 2. Content Filtering & Exclusions\n### 2.1 Basic Overview\n```\nconfig = CrawlerRunConfig(\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n\n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n\n    # Link filtering\n    exclude_external_links=True,    \n    exclude_social_media_links=True,\n    # Block entire domains\n    exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],    \n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n\n    # Media filtering\n    exclude_external_images=True\n)\nCopy\n```\n\n- **`word_count_threshold`**: Ignores text blocks under X words. Helps skip trivial blocks like short nav or disclaimers.  \n- **`excluded_tags`**: Removes entire tags (`<form>` , `<header>`, `<footer>`, etc.).  \n- **Link Filtering** :  \n- `exclude_external_links`: Strips out external links and may remove them from `result.links`.  \n- `exclude_social_media_links`: Removes links pointing to known social media domains.  \n- `exclude_domains`: A custom list of domains to block if discovered in links.  \n- `exclude_social_media_domains`: A curated list (override or add to it) for social media sites.  \n- **Media Filtering** :  \n- `exclude_external_images`: Discards images not hosted on the same domain as the main page (or its subdomains). By default in case you set `exclude_social_media_links=True`, the following social media domains are excluded: \n```\n[\n    'facebook.com',\n    'twitter.com',\n    'x.com',\n    'linkedin.com',\n    'instagram.com',\n    'pinterest.com',\n    'tiktok.com',\n    'snapchat.com',\n    'reddit.com',\n]\nCopy\n```\n\n### 2.2 Example Usage\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        css_selector=\"main.content\", \n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        exclude_social_media_links=True,\n        exclude_domains=[\"ads.com\", \"spammytrackers.net\"],\n        exclude_external_images=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        print(\"Cleaned HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 3. Handling Iframes\nSome sites embed content in `<iframe>` tags. If you want that inline: \n```\nconfig = CrawlerRunConfig(\n    # Merge iframe content into the final output\n    process_iframes=True,    \n    remove_overlay_elements=True\n)\nCopy\n```\n\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        process_iframes=True,\n        remove_overlay_elements=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.org/iframe-demo\", \n            config=config\n        )\n        print(\"Iframe-merged length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 4. Structured Extraction Examples\n### 4.1 Pattern-Based with `JsonCssExtractionStrategy`\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    # Minimal schema for repeated items\n    schema = {\n        \"name\": \"News Items\",\n        \"baseSelector\": \"tr.athing\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"span.titleline a\", \"type\": \"text\"},\n            {\n                \"name\": \"link\", \n                \"selector\": \"span.titleline a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Content filtering\n        excluded_tags=[\"form\", \"header\"],\n        exclude_domains=[\"adsite.com\"],\n\n        # CSS selection or entire page\n        css_selector=\"table.itemlist\",\n\n        # No caching for demonstration\n        cache_mode=CacheMode.BYPASS,\n\n        # Extraction strategy\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        data = json.loads(result.extracted_content)\n        print(\"Sample extracted item:\", data[:1])  # Show first item\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 4.2 LLM-Based Extraction\n```\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass ArticleData(BaseModel):\n    headline: str\n    summary: str\n\nasync def main():\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4\",api_token=\"sk-YOUR_API_KEY\")\n        schema=ArticleData.schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract 'headline' and a short 'summary' from the content.\"\n    )\n\n    config = CrawlerRunConfig(\n        exclude_external_links=True,\n        word_count_threshold=20,\n        extraction_strategy=llm_strategy\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        article = json.loads(result.extracted_content)\n        print(article)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- Filters out external links (`exclude_external_links=True`).  \n- Ignores very short text blocks (`word_count_threshold=20`).  \n- Passes the final HTML to your LLM strategy for an AI-driven parse.\n## 5. Comprehensive Example\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def extract_main_articles(url: str):\n    schema = {\n        \"name\": \"ArticleBlock\",\n        \"baseSelector\": \"div.article-block\",\n        \"fields\": [\n            {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n            {\n                \"name\": \"metadata\",\n                \"type\": \"nested\",\n                \"fields\": [\n                    {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                    {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n                ]\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Keep only #main-content\n        css_selector=\"#main-content\",\n\n        # Filtering\n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],  \n        exclude_external_links=True,\n        exclude_domains=[\"somebadsite.com\"],\n        exclude_external_images=True,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url, config=config)\n        if not result.success:\n            print(f\"Error: {result.error_message}\")\n            return None\n        return json.loads(result.extracted_content)\n\nasync def main():\n    articles = await extract_main_articles(\"https://news.ycombinator.com/newest\")\n    if articles:\n        print(\"Extracted Articles:\", articles[:2])  # Show first 2\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **CSS** scoping with `#main-content`.  \n- Multiple **exclude_** parameters to remove domains, external images, etc.  \n- A **JsonCssExtractionStrategy** to parse repeated article blocks.\n## 6. Scraping Modes\nCrawl4AI uses `LXMLWebScrapingStrategy` (LXML-based) as the default scraping strategy for HTML content processing. This strategy offers excellent performance, especially for large HTML documents. **Note:** For backward compatibility, `WebScrapingStrategy` is still available as an alias for `LXMLWebScrapingStrategy`. \n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy\n\nasync def main():\n    # Default configuration already uses LXMLWebScrapingStrategy\n    config = CrawlerRunConfig()\n\n    # Or explicitly specify it if desired\n    config_explicit = CrawlerRunConfig(\n        scraping_strategy=LXMLWebScrapingStrategy()\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\", \n            config=config\n        )\nCopy\n```\n\nYou can also create your own custom scraping strategy by inheriting from `ContentScrapingStrategy`. The strategy must return a `ScrapingResult` object with the following structure: \n```\nfrom crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\n\nclass CustomScrapingStrategy(ContentScrapingStrategy):\n    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # Implement your custom scraping logic here\n        return ScrapingResult(\n            cleaned_html=\"<html>...</html>\",  # Cleaned HTML content\n            success=True,                     # Whether scraping was successful\n            media=Media(\n                images=[                      # List of images found\n                    MediaItem(\n                        src=\"https://example.com/image.jpg\",\n                        alt=\"Image description\",\n                        desc=\"Surrounding text\",\n                        score=1,\n                        type=\"image\",\n                        group_id=1,\n                        format=\"jpg\",\n                        width=800\n                    )\n                ],\n                videos=[],                    # List of videos (same structure as images)\n                audios=[]                     # List of audio files (same structure as images)\n            ),\n            links=Links(\n                internal=[                    # List of internal links\n                    Link(\n                        href=\"https://example.com/page\",\n                        text=\"Link text\",\n                        title=\"Link title\",\n                        base_domain=\"example.com\"\n                    )\n                ],\n                external=[]                   # List of external links (same structure)\n            ),\n            metadata={                        # Additional metadata\n                \"title\": \"Page Title\",\n                \"description\": \"Page description\"\n            }\n        )\n\n    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # For simple cases, you can use the sync version\n        return await asyncio.to_thread(self.scrap, url, html, **kwargs)\nCopy\n```\n\n### Performance Considerations\n  * Fast processing of large HTML documents (especially >100KB)\n  * Efficient memory usage\n  * Good handling of well-formed HTML\n  * Robust table detection and extraction\n\n\n### Backward Compatibility\nFor users upgrading from earlier versions: - `WebScrapingStrategy` is now an alias for `LXMLWebScrapingStrategy` - Existing code using `WebScrapingStrategy` will continue to work without modification - No changes are required to your existing code\n## 7. Combining CSS Selection Methods\nYou can combine `css_selector` and `target_elements` in powerful ways to achieve fine-grained control over your output: \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Target specific content but preserve page context\n    config = CrawlerRunConfig(\n        # Focus markdown on main content and sidebar\n        target_elements=[\"#main-content\", \".sidebar\"],\n\n        # Global filters applied to entire page\n        excluded_tags=[\"nav\", \"footer\", \"header\"],\n        exclude_external_links=True,\n\n        # Use basic content thresholds\n        word_count_threshold=15,\n\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/article\",\n            config=config\n        )\n\n        print(f\"Content focuses on specific elements, but all links still analyzed\")\n        print(f\"Internal links: {len(result.links.get('internal', []))}\")\n        print(f\"External links: {len(result.links.get('external', []))}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- Links, images and other page data still give you the full context of the page - Content filtering still applies globally\n## 8. Conclusion\nBy mixing **target_elements** or **css_selector** scoping, **content filtering** parameters, and advanced **extraction strategies** , you can precisely **choose** which data to keep. Key parameters in **`CrawlerRunConfig`**for content selection include: 1.**`target_elements`**â€“ Array of CSS selectors to focus markdown generation and data extraction, while preserving full page context for links and media. 2.**`css_selector`**â€“ Basic scoping to an element or region for all extraction processes.  \n3. **`word_count_threshold`**â€“ Skip short blocks.  \n4. **`excluded_tags`**â€“ Remove entire HTML tags.  \n5. **`exclude_external_links`**,**`exclude_social_media_links`**,**`exclude_domains`**â€“ Filter out unwanted links or domains.  \n6. **`exclude_external_images`**â€“ Remove images from external sources.  \n7. **`process_iframes`**â€“ Merge iframe content if needed.\n# Page Interaction\n  1. Click â€œLoad Moreâ€ buttons \n  2. Fill forms and submit them \n  3. Wait for elements or data to appear \n  4. Reuse sessions across multiple steps \n\n\n## 1. JavaScript Execution\n### Basic Execution\n**`js_code`**in**`CrawlerRunConfig`**accepts either a single JS string or a list of JS snippets.  \n\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Single JS command\n    config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Example site\n            config=config\n        )\n        print(\"Crawled length:\", len(result.cleaned_html))\n\n    # Multiple commands\n    js_commands = [\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # 'More' link on Hacker News\n        \"document.querySelector('a.morelink')?.click();\",  \n    ]\n    config = CrawlerRunConfig(js_code=js_commands)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Another pass\n            config=config\n        )\n        print(\"After scroll+click, length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Relevant`CrawlerRunConfig` params**: - **`js_code`**: A string or list of strings with JavaScript to run after the page loads. -**`js_only`**: If set to`True` on subsequent calls, indicates weâ€™re continuing an existing session without a new full navigation.  \n- **`session_id`**: If you want to keep the same page across multiple calls, specify an ID.\n## 2. Wait Conditions\n### 2.1 CSS-Based Waiting\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Wait for at least 30 items on Hacker News\n        wait_for=\"css:.athing:nth-child(30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"We have at least 30 items loaded!\")\n        # Rough check\n        print(\"Total items in HTML:\", result.cleaned_html.count(\"athing\"))  \n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **`wait_for=\"css:...\"`**: Tells the crawler to wait until that CSS selector is present.\n### 2.2 JavaScript-Based Waiting\nFor more complex conditions (e.g., waiting for content length to exceed a threshold), prefix `js:`: \n```\nwait_condition = \"\"\"() => {\n    const items = document.querySelectorAll('.athing');\n    return items.length > 50;  // Wait for at least 51 items\n}\"\"\"\n\nconfig = CrawlerRunConfig(wait_for=f\"js:{wait_condition}\")\nCopy\n```\n\n**Behind the Scenes** : Crawl4AI keeps polling the JS function until it returns `true` or a timeout occurs.\n## 3. Handling Dynamic Content\n### 3.1 Load More Example (Hacker News â€œMoreâ€ Link)\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Step 1: Load initial Hacker News page\n    config = CrawlerRunConfig(\n        wait_for=\"css:.athing:nth-child(30)\"  # Wait for 30 items\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"Initial items loaded.\")\n\n        # Step 2: Let's scroll and click the \"More\" link\n        load_more_js = [\n            \"window.scrollTo(0, document.body.scrollHeight);\",\n            # The \"More\" link at page bottom\n            \"document.querySelector('a.morelink')?.click();\"  \n        ]\n\n        next_page_conf = CrawlerRunConfig(\n            js_code=load_more_js,\n            wait_for=\"\"\"js:() => {\n                return document.querySelectorAll('.athing').length > 30;\n            }\"\"\",\n            # Mark that we do not re-navigate, but run JS in the same session:\n            js_only=True,\n            session_id=\"hn_session\"\n        )\n\n        # Re-use the same crawler session\n        result2 = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # same URL but continuing session\n            config=next_page_conf\n        )\n        total_items = result2.cleaned_html.count(\"athing\")\n        print(\"Items after load-more:\", total_items)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **`session_id=\"hn_session\"`**: Keep the same page across multiple calls to`arun()`. - **`js_only=True`**: Weâ€™re not performing a full reload, just applying JS in the existing page. -**`wait_for`**with`js:` : Wait for item count to grow beyond 30.\n### 3.2 Form Interaction\nIf the site has a search or login form, you can fill fields and submit them with **`js_code`**. For instance, if GitHub had a local search form:\n```\njs_form_interaction = \"\"\"\ndocument.querySelector('#your-search').value = 'TypeScript commits';\ndocument.querySelector('form').submit();\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    js_code=js_form_interaction,\n    wait_for=\"css:.commit\"\n)\nresult = await crawler.arun(url=\"https://github.com/search\", config=config)\nCopy\n```\n\n## 4. Timing Control\n1. **`page_timeout`**(ms): Overall page load or script execution time limit.  \n2. **`delay_before_return_html`**(seconds): Wait an extra moment before capturing the final HTML.  \n3. **`mean_delay`** & **`max_range`**: If you call`arun_many()` with multiple URLs, these add a random pause between each request. \n```\nconfig = CrawlerRunConfig(\n    page_timeout=60000,  # 60s limit\n    delay_before_return_html=2.5\n)\nCopy\n```\n\n## 5. Multi-Step Interaction Example\nBelow is a simplified script that does multiple â€œLoad Moreâ€ clicks on GitHubâ€™s TypeScript commits page. It **re-uses** the same session to accumulate new commits each time. The code includes the relevant **`CrawlerRunConfig`**parameters youâ€™d rely on.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def multi_page_commits():\n    browser_cfg = BrowserConfig(\n        headless=False,  # Visible for demonstration\n        verbose=True\n    )\n    session_id = \"github_ts_commits\"\n\n    base_wait = \"\"\"js:() => {\n        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n        return commits.length > 0;\n    }\"\"\"\n\n    # Step 1: Load initial commits\n    config1 = CrawlerRunConfig(\n        wait_for=base_wait,\n        session_id=session_id,\n        cache_mode=CacheMode.BYPASS,\n        # Not using js_only yet since it's our first load\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://github.com/microsoft/TypeScript/commits/main\",\n            config=config1\n        )\n        print(\"Initial commits loaded. Count:\", result.cleaned_html.count(\"commit\"))\n\n        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists\n        js_next_page = \"\"\"\n        const selector = 'a[data-testid=\"pagination-next-button\"]';\n        const button = document.querySelector(selector);\n        if (button) button.click();\n        \"\"\"\n\n        # Wait until new commits appear\n        wait_for_more = \"\"\"js:() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (!window.firstCommit && commits.length>0) {\n                window.firstCommit = commits[0].textContent;\n                return false;\n            }\n            // If top commit changes, we have new commits\n            const topNow = commits[0]?.textContent.trim();\n            return topNow && topNow !== window.firstCommit;\n        }\"\"\"\n\n        for page in range(2):  # let's do 2 more \"Next\" pages\n            config_next = CrawlerRunConfig(\n                session_id=session_id,\n                js_code=js_next_page,\n                wait_for=wait_for_more,\n                js_only=True,       # We're continuing from the open tab\n                cache_mode=CacheMode.BYPASS\n            )\n            result2 = await crawler.arun(\n                url=\"https://github.com/microsoft/TypeScript/commits/main\",\n                config=config_next\n            )\n            print(f\"Page {page+2} commits count:\", result2.cleaned_html.count(\"commit\"))\n\n        # Optionally kill session\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasync def main():\n    await multi_page_commits()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **`session_id`**: Keep the same page open.  \n- **`js_code`**+**`wait_for`**+**`js_only=True`**: We do partial refreshes, waiting for new commits to appear.  \n- **`cache_mode=CacheMode.BYPASS`**ensures we always see fresh data each step.\n## 6. Combine Interaction with Extraction\nOnce dynamic content is loaded, you can attach an **`extraction_strategy`**(like`JsonCssExtractionStrategy` or `LLMExtractionStrategy`). For example: \n```\nfrom crawl4ai import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Commits\",\n    \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"}\n    ]\n}\nconfig = CrawlerRunConfig(\n    session_id=\"ts_commits_session\",\n    js_code=js_next_page,\n    wait_for=wait_for_more,\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\nCopy\n```\n\nWhen done, check `result.extracted_content` for the JSON.\n## 7. Relevant `CrawlerRunConfig` Parameters\nBelow are the key interaction-related parameters in `CrawlerRunConfig`. For a full list, see [Configuration Parameters](https://docs.crawl4ai.com/api/parameters.md). - **`js_code`**: JavaScript to run after initial load.  \n- **`js_only`**: If`True` , no new page navigationâ€”only JS in the existing session.  \n- **`wait_for`**: CSS (`\"css:...\"`) or JS (`\"js:...\"`) expression to wait for.  \n- **`session_id`**: Reuse the same page across calls.  \n- **`cache_mode`**: Whether to read/write from the cache or bypass.  \n- **`remove_overlay_elements`**: Remove certain popups automatically.  \n- **`simulate_user`,`override_navigator` , `magic`**: Anti-bot or â€œhuman-likeâ€ interactions.\n## 8. Conclusion\n1. **Execute JavaScript** for scrolling, clicks, or form filling.  \n2. **Wait** for CSS or custom JS conditions before capturing data.  \n4. Combine with **structured extraction** for dynamic sites.\n## 9. Virtual Scrolling\nFor sites that use **virtual scrolling** (where content is replaced rather than appended as you scroll, like Twitter or Instagram), Crawl4AI provides a dedicated `VirtualScrollConfig`: \n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, VirtualScrollConfig\n\nasync def crawl_twitter_timeline():\n    # Configure virtual scroll for Twitter-like feeds\n    virtual_config = VirtualScrollConfig(\n        container_selector=\"[data-testid='primaryColumn']\",  # Twitter's main column\n        scroll_count=30,                # Scroll 30 times\n        scroll_by=\"container_height\",   # Scroll by container height each time\n        wait_after_scroll=1.0          # Wait 1 second after each scroll\n    )\n\n    config = CrawlerRunConfig(\n        virtual_scroll_config=virtual_config\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://twitter.com/search?q=AI\",\n            config=config\n        )\n        # result.html now contains ALL tweets from the virtual scroll\nCopy\n```\n\n### Virtual Scroll vs JavaScript Scrolling\nFeature | Virtual Scroll | JS Code Scrolling  \n---|---|---  \n**Use Case** | Content replaced during scroll | Content appended or simple scroll  \n**Configuration** |  `VirtualScrollConfig` object |  `js_code` with scroll commands  \n**Automatic Merging** | Yes - merges all unique content | No - captures final state only  \n**Best For** | Twitter, Instagram, virtual tables | Traditional pages, load more buttons  \n# Link & Media\n  1. Extract links (internal, external) from crawled pages \n  2. Filter or exclude specific domains (e.g., social media or custom domains) \n  3. Access and ma### 3.2 Excluding Images \n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\nCopy\n```\n\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_all_images=True\n)\nCopy\n```\n\n  4. You don't need image data in your results\n  5. You're crawling image-heavy pages that cause memory issues\n  6. You want to focus only on text content\n  7. Configure your crawler to exclude or prioritize certain images Below is a revised version of the **Link Extraction** and **Media Extraction** sections that includes example data structures showing how links and media items are stored in `CrawlResult`. Feel free to adjust any field names or descriptions to match your actual output.\n\n\n## 1. Link Extraction\n### 1.1 `result.links`\nWhen you call `arun()` or `arun_many()` on a URL, Crawl4AI automatically extracts links and stores them in the `links` field of `CrawlResult`. By default, the crawler tries to distinguish **internal** links (same domain) from **external** links (different domains). \n```\nfrom crawl4ai import AsyncWebCrawler\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://www.example.com\")\n    if result.success:\n        internal_links = result.links.get(\"internal\", [])\n        external_links = result.links.get(\"external\", [])\n        print(f\"Found {len(internal_links)} internal links.\")\n        print(f\"Found {len(internal_links)} external links.\")\n        print(f\"Found {len(result.media)} media items.\")\n\n        # Each link is typically a dictionary with fields like:\n        # { \"href\": \"...\", \"text\": \"...\", \"title\": \"...\", \"base_domain\": \"...\" }\n        if internal_links:\n            print(\"Sample Internal Link:\", internal_links[0])\n    else:\n        print(\"Crawl failed:\", result.error_message)\nCopy\n```\n\n```\nresult.links = {\n  \"internal\": [\n    {\n      \"href\": \"https://kidocode.com/\",\n      \"text\": \"\",\n      \"title\": \"\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    {\n      \"href\": \"https://kidocode.com/degrees/technology\",\n      \"text\": \"Technology Degree\",\n      \"title\": \"KidoCode Tech Program\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    # ...\n  ],\n  \"external\": [\n    # possibly other links leading to third-party sites\n  ]\n}\nCopy\n```\n\n- **`href`**: The raw hyperlink URL.  \n- **`text`**: The link text (if any) within the`<a>` tag.  \n- **`title`**: The`title` attribute of the link (if present).  \n- **`base_domain`**: The domain extracted from`href`. Helpful for filtering or grouping by domain.\n## 2. Advanced Link Head Extraction & Scoring\nEver wanted to not just extract links, but also get the actual content (title, description, metadata) from those linked pages? And score them for relevance? This is exactly what Link Head Extraction does - it fetches the `<head>` section from each discovered link and scores them using multiple algorithms.\n### 2.1 Why Link Head Extraction?\n  1. **Fetching head content** from each link (title, description, meta tags)\n  2. **Combining scores intelligently** to give you a final relevance ranking\n\n\n### 2.2 Complete Working Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai import LinkPreviewConfig\n\nasync def extract_link_heads_example():\n    \"\"\"\n    Complete example showing link head extraction with scoring.\n    This will crawl a documentation site and extract head content from internal links.\n    \"\"\"\n\n    # Configure link head extraction\n    config = CrawlerRunConfig(\n        # Enable link head extraction with detailed configuration\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,           # Extract from internal links\n            include_external=False,          # Skip external links for this example\n            max_links=10,                   # Limit to 10 links for demo\n            concurrency=5,                  # Process 5 links simultaneously\n            timeout=10,                     # 10 second timeout per link\n            query=\"API documentation guide\", # Query for contextual scoring\n            score_threshold=0.3,            # Only include links scoring above 0.3\n            verbose=True                    # Show detailed progress\n        ),\n        # Enable intrinsic scoring (URL quality, text relevance)\n        score_links=True,\n        # Keep output clean\n        only_text=True,\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Crawl a documentation site (great for testing)\n        result = await crawler.arun(\"https://docs.python.org/3/\", config=config)\n\n        if result.success:\n            print(f\"âœ… Successfully crawled: {result.url}\")\n            print(f\"ðŸ“„ Page title: {result.metadata.get('title', 'No title')}\")\n\n            # Access links (now enhanced with head data and scores)\n            internal_links = result.links.get(\"internal\", [])\n            external_links = result.links.get(\"external\", [])\n\n            print(f\"\\nðŸ”— Found {len(internal_links)} internal links\")\n            print(f\"ðŸŒ Found {len(external_links)} external links\")\n\n            # Count links with head data\n            links_with_head = [link for link in internal_links \n                             if link.get(\"head_data\") is not None]\n            print(f\"ðŸ§  Links with head data extracted: {len(links_with_head)}\")\n\n            # Show the top 3 scoring links\n            print(f\"\\nðŸ† Top 3 Links with Full Scoring:\")\n            for i, link in enumerate(links_with_head[:3]):\n                print(f\"\\n{i+1}. {link['href']}\")\n                print(f\"   Link Text: '{link.get('text', 'No text')[:50]}...'\")\n\n                # Show all three score types\n                intrinsic = link.get('intrinsic_score')\n                contextual = link.get('contextual_score') \n                total = link.get('total_score')\n\n                if intrinsic is not None:\n                    print(f\"   ðŸ“Š Intrinsic Score: {intrinsic:.2f}/10.0 (URL quality & context)\")\n                if contextual is not None:\n                    print(f\"   ðŸŽ¯ Contextual Score: {contextual:.3f} (BM25 relevance to query)\")\n                if total is not None:\n                    print(f\"   â­ Total Score: {total:.3f} (combined final score)\")\n\n                # Show extracted head data\n                head_data = link.get(\"head_data\", {})\n                if head_data:\n                    title = head_data.get(\"title\", \"No title\")\n                    description = head_data.get(\"meta\", {}).get(\"description\", \"No description\")\n\n                    print(f\"   ðŸ“° Title: {title[:60]}...\")\n                    if description:\n                        print(f\"   ðŸ“ Description: {description[:80]}...\")\n\n                    # Show extraction status\n                    status = link.get(\"head_extraction_status\", \"unknown\")\n                    print(f\"   âœ… Extraction Status: {status}\")\n        else:\n            print(f\"âŒ Crawl failed: {result.error_message}\")\n\n# Run the example\nif __name__ == \"__main__\":\n    asyncio.run(extract_link_heads_example())\nCopy\n```\n\n```\nâœ… Successfully crawled: https://docs.python.org/3/\nðŸ“„ Page title: 3.13.5 Documentation\nðŸ”— Found 53 internal links\nðŸŒ Found 1 external links\nðŸ§  Links with head data extracted: 10\n\nðŸ† Top 3 Links with Full Scoring:\n\n1. https://docs.python.org/3.15/\n   Link Text: 'Python 3.15 (in development)...'\n   ðŸ“Š Intrinsic Score: 4.17/10.0 (URL quality & context)\n   ðŸŽ¯ Contextual Score: 1.000 (BM25 relevance to query)\n   â­ Total Score: 5.917 (combined final score)\n   ðŸ“° Title: 3.15.0a0 Documentation...\n   ðŸ“ Description: The official Python documentation...\n   âœ… Extraction Status: valid\nCopy\n```\n\n### 2.3 Configuration Deep Dive\nThe `LinkPreviewConfig` class supports these options: \n```\nfrom crawl4ai import LinkPreviewConfig\n\nlink_preview_config = LinkPreviewConfig(\n    # BASIC SETTINGS\n    verbose=True,                    # Show detailed logs (recommended for learning)\n\n    # LINK FILTERING\n    include_internal=True,           # Include same-domain links\n    include_external=True,           # Include different-domain links\n    max_links=50,                   # Maximum links to process (prevents overload)\n\n    # PATTERN FILTERING\n    include_patterns=[               # Only process links matching these patterns\n        \"*/docs/*\", \n        \"*/api/*\", \n        \"*/reference/*\"\n    ],\n    exclude_patterns=[               # Skip links matching these patterns\n        \"*/login*\",\n        \"*/admin*\"\n    ],\n\n    # PERFORMANCE SETTINGS\n    concurrency=10,                  # How many links to process simultaneously\n    timeout=5,                      # Seconds to wait per link\n\n    # RELEVANCE SCORING\n    query=\"machine learning API\",    # Query for BM25 contextual scoring\n    score_threshold=0.3,            # Only include links above this score\n)\nCopy\n```\n\n### 2.4 Understanding the Three Score Types\n```\n# High intrinsic score indicators:\n# âœ… Clean URL structure (docs.python.org/api/reference)\n# âœ… Meaningful link text (\"API Reference Guide\")\n# âœ… Relevant to page context\n# âœ… Not buried deep in navigation\n\n# Low intrinsic score indicators:\n# âŒ Random URLs (site.com/x7f9g2h)\n# âŒ No link text or generic text (\"Click here\")\n# âŒ Unrelated to page content\nCopy\n```\n\nOnly available when you provide a `query`. Uses BM25 algorithm against head content: \n```\n# Example: query = \"machine learning tutorial\"\n# High contextual score: Link to \"Complete Machine Learning Guide\"\n# Low contextual score: Link to \"Privacy Policy\"\nCopy\n```\n\n```\n# When both scores available: (intrinsic * 0.3) + (contextual * 0.7)\n# When only intrinsic: uses intrinsic score\n# When only contextual: uses contextual score\n# When neither: not calculated\nCopy\n```\n\n### 2.5 Practical Use Cases\n```\nasync def research_assistant():\n    config = CrawlerRunConfig(\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,\n            include_external=True,\n            include_patterns=[\"*/docs/*\", \"*/tutorial/*\", \"*/guide/*\"],\n            query=\"machine learning neural networks\",\n            max_links=20,\n            score_threshold=0.5,  # Only high-relevance links\n            verbose=True\n        ),\n        score_links=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://scikit-learn.org/\", config=config)\n\n        if result.success:\n            # Get high-scoring links\n            good_links = [link for link in result.links.get(\"internal\", [])\n                         if link.get(\"total_score\", 0) > 0.7]\n\n            print(f\"ðŸŽ¯ Found {len(good_links)} highly relevant links:\")\n            for link in good_links[:5]:\n                print(f\"â­ {link['total_score']:.3f} - {link['href']}\")\n                print(f\"   {link.get('head_data', {}).get('title', 'No title')}\")\nCopy\n```\n\n```\nasync def api_discovery():\n    config = CrawlerRunConfig(\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,\n            include_patterns=[\"*/api/*\", \"*/reference/*\"],\n            exclude_patterns=[\"*/deprecated/*\"],\n            max_links=100,\n            concurrency=15,\n            verbose=False  # Clean output\n        ),\n        score_links=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://docs.example-api.com/\", config=config)\n\n        if result.success:\n            api_links = result.links.get(\"internal\", [])\n\n            # Group by endpoint type\n            endpoints = {}\n            for link in api_links:\n                if link.get(\"head_data\"):\n                    title = link[\"head_data\"].get(\"title\", \"\")\n                    if \"GET\" in title:\n                        endpoints.setdefault(\"GET\", []).append(link)\n                    elif \"POST\" in title:\n                        endpoints.setdefault(\"POST\", []).append(link)\n\n            for method, links in endpoints.items():\n                print(f\"\\n{method} Endpoints ({len(links)}):\")\n                for link in links[:3]:\n                    print(f\"  â€¢ {link['href']}\")\nCopy\n```\n\n```\nasync def quality_analysis():\n    config = CrawlerRunConfig(\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,\n            max_links=200,\n            concurrency=20,\n        ),\n        score_links=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://your-website.com/\", config=config)\n\n        if result.success:\n            links = result.links.get(\"internal\", [])\n\n            # Analyze intrinsic scores\n            scores = [link.get('intrinsic_score', 0) for link in links]\n            avg_score = sum(scores) / len(scores) if scores else 0\n\n            print(f\"ðŸ“Š Link Quality Analysis:\")\n            print(f\"   Average intrinsic score: {avg_score:.2f}/10.0\")\n            print(f\"   High quality links (>7.0): {len([s for s in scores if s > 7.0])}\")\n            print(f\"   Low quality links (<3.0): {len([s for s in scores if s < 3.0])}\")\n\n            # Find problematic links\n            bad_links = [link for link in links \n                        if link.get('intrinsic_score', 0) < 2.0]\n\n            if bad_links:\n                print(f\"\\nâš ï¸  Links needing attention:\")\n                for link in bad_links[:5]:\n                    print(f\"   {link['href']} (score: {link.get('intrinsic_score', 0):.1f})\")\nCopy\n```\n\n### 2.6 Performance Tips\n  1. **Start Small** : Begin with `max_links: 10` to understand the feature\n  2. **Use Patterns** : Filter with `include_patterns` to focus on relevant sections\n  3. **Adjust Concurrency** : Higher concurrency = faster but more resource usage\n  4. **Set Timeouts** : Use `timeout: 5` to prevent hanging on slow sites\n  5. **Use Score Thresholds** : Filter out low-quality links with `score_threshold`\n\n\n### 2.7 Troubleshooting\n```\n# Check your configuration:\nconfig = CrawlerRunConfig(\n    link_preview_config=LinkPreviewConfig(\n        verbose=True   # â† Enable to see what's happening\n    )\n)\nCopy\n```\n\n```\n# Make sure scoring is enabled:\nconfig = CrawlerRunConfig(\n    score_links=True,  # â† Enable intrinsic scoring\n    link_preview_config=LinkPreviewConfig(\n        query=\"your search terms\"  # â† For contextual scoring\n    )\n)\nCopy\n```\n\n```\n# Optimize performance:\nlink_preview_config = LinkPreviewConfig(\n    max_links=20,      # â† Reduce number\n    concurrency=10,    # â† Increase parallelism\n    timeout=3,         # â† Shorter timeout\n    include_patterns=[\"*/important/*\"]  # â† Focus on key areas\n)\nCopy\n```\n\n## 3. Domain Filtering\nSome websites contain hundreds of third-party or affiliate links. You can filter out certain domains at **crawl time** by configuring the crawler. The most relevant parameters in `CrawlerRunConfig` are: - **`exclude_external_links`**: If`True` , discard any link pointing outside the root domain.  \n- **`exclude_social_media_domains`**: Provide a list of social media platforms (e.g.,`[\"facebook.com\", \"twitter.com\"]`) to exclude from your crawl.  \n- **`exclude_social_media_links`**: If`True` , automatically skip known social platforms.  \n- **`exclude_domains`**: Provide a list of custom domains you want to exclude (e.g.,`[\"spammyads.com\", \"tracker.net\"]`).\n### 3.1 Example: Excluding External & Social Media Links\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,          # No links outside primary domain\n        exclude_social_media_links=True       # Skip recognized social media domains\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://www.example.com\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            print(\"Internal links count:\", len(result.links.get(\"internal\", [])))\n            print(\"External links count:\", len(result.links.get(\"external\", [])))  \n            # Likely zero external links in this scenario\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 3.2 Example: Excluding Specific Domains\nIf you want to let external links in, but specifically exclude a domain (e.g., `suspiciousads.com`), do this: \n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_domains=[\"suspiciousads.com\"]\n)\nCopy\n```\n\n## 4. Media Extraction\n### 4.1 Accessing `result.media`\nBy default, Crawl4AI collects images, audio and video URLs it finds on the page. These are stored in `result.media`, a dictionary keyed by media type (e.g., `images`, `videos`, `audio`). **Note: Tables have been moved from`result.media[\"tables\"]` to the new `result.tables` format for better organization and direct access.**\n```\nif result.success:\n    # Get images\n    images_info = result.media.get(\"images\", [])\n    print(f\"Found {len(images_info)} images in total.\")\n    for i, img in enumerate(images_info[:3]):  # Inspect just the first 3\n        print(f\"[Image {i}] URL: {img['src']}\")\n        print(f\"           Alt text: {img.get('alt', '')}\")\n        print(f\"           Score: {img.get('score')}\")\n        print(f\"           Description: {img.get('desc', '')}\\n\")\nCopy\n```\n\n```\nresult.media = {\n  \"images\": [\n    {\n      \"src\": \"https://cdn.prod.website-files.com/.../Group%2089.svg\",\n      \"alt\": \"coding school for kids\",\n      \"desc\": \"Trial Class Degrees degrees All Degrees AI Degree Technology ...\",\n      \"score\": 3,\n      \"type\": \"image\",\n      \"group_id\": 0,\n      \"format\": None,\n      \"width\": None,\n      \"height\": None\n    },\n    # ...\n  ],\n  \"videos\": [\n    # Similar structure but with video-specific fields\n  ],\n  \"audio\": [\n    # Similar structure but with audio-specific fields\n  ],\n}\nCopy\n```\n\n- **`src`**: The media URL (e.g., image source)  \n- **`alt`**: The alt text for images (if present)  \n- **`desc`**: A snippet of nearby text or a short description (optional)  \n- **`score`**: A heuristic relevance score if youâ€™re using content-scoring features  \n- **`width`**,**`height`**: If the crawler detects dimensions for the image/video  \n- **`type`**: Usually`\"image\"` , `\"video\"`, or `\"audio\"`  \n- **`group_id`**: If youâ€™re grouping related media items, the crawler might assign an ID\n### 4.2 Excluding External Images\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\nCopy\n```\n\n### 4.3 Additional Media Config\n  * **`screenshot`**: Set to`True` if you want a full-page screenshot stored as `base64` in `result.screenshot`. \n  * **`pdf`**: Set to`True` if you want a PDF version of the page in `result.pdf`. \n  * **`capture_mhtml`**: Set to`True` if you want an MHTML snapshot of the page in `result.mhtml`. This format preserves the entire web page with all its resources (CSS, images, scripts) in a single file, making it perfect for archiving or offline viewing.\n  * **`wait_for_images`**: If`True` , attempts to wait until images are fully loaded before final extraction. \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        capture_mhtml=True  # Enable MHTML capture\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=crawler_cfg)\n\n        if result.success and result.mhtml:\n            # Save the MHTML snapshot to a file\n            with open(\"example.mhtml\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.mhtml)\n            print(\"MHTML snapshot saved to example.mhtml\")\n        else:\n            print(\"Failed to capture MHTML:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n  * It captures the complete page state including all resources\n  * It can be opened in most modern browsers for offline viewing\n  * It preserves the page exactly as it appeared during crawling\n  * It's a single file, making it easy to store and transfer\n\n\n## 5. Putting It All Together: Link & Media Filtering\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # Suppose we want to keep only internal links, remove certain domains, \n    # and discard external images from the final crawl data.\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,\n        exclude_domains=[\"spammyads.com\"],\n        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.\n        exclude_external_images=True,      # keep only images from main domain\n        wait_for_images=True,             # ensure images are loaded\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://www.example.com\", config=crawler_cfg)\n\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n\n            # 1. Links\n            in_links = result.links.get(\"internal\", [])\n            ext_links = result.links.get(\"external\", [])\n            print(\"Internal link count:\", len(in_links))\n            print(\"External link count:\", len(ext_links))  # should be zero with exclude_external_links=True\n\n            # 2. Images\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n\n            # Let's see a snippet of these images\n            for i, img in enumerate(images[:3]):\n                print(f\"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})\")\n        else:\n            print(\"[ERROR] Failed to crawl. Reason:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 6. Common Pitfalls & Tips\n1. **Conflicting Flags** :  \n- `exclude_external_links=True` but then also specifying `exclude_social_media_links=True` is typically fine, but understand that the first setting already discards _all_ external links. The second becomes somewhat redundant.  \n- `exclude_external_images=True` but want to keep some external images? Currently no partial domain-based setting for images, so you might need a custom approach or hook logic. 2. **Relevancy Scores** :  \n- If your version of Crawl4AI or your scraping strategy includes an `img[\"score\"]`, itâ€™s typically a heuristic based on size, position, or content analysis. Evaluate carefully if you rely on it. 3. **Performance** :  \n4. **Social Media Lists** :  \n- `exclude_social_media_links=True` typically references an internal list of known social domains like Facebook, Twitter, LinkedIn, etc. If you need to add or remove from that list, look for library settings or a local config file (depending on your version).\n# Extraction Strategies\n# Extracting JSON (No LLM)\n  1. **Schema-based extraction** with CSS or XPath selectors via `JsonCssExtractionStrategy` and `JsonXPathExtractionStrategy`\n  2. **Regular expression extraction** with `RegexExtractionStrategy` for fast pattern matching\n  3. **Faster & Cheaper**: No API calls or GPU overhead. \n\n\n## 1. Intro to Schema-Based Extraction\n  1. **Nested** or **list** types for repeated or hierarchical structures. \n\n\n## 2. Simple Example: Crypto Prices\nLet's begin with a **simple** schema-based extraction using the `JsonCssExtractionStrategy`. Below is a snippet that extracts cryptocurrency prices from a site (similar to the legacy Coinbase example). Notice we **don't** call any LLM: \n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def extract_crypto_prices():\n    # 1. Define a simple extraction schema\n    schema = {\n        \"name\": \"Crypto Prices\",\n        \"baseSelector\": \"div.crypto-row\",    # Repeated elements\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \"h2.coin-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"span.coin-price\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 2. Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # 3. Set up your crawler config (if needed)\n    config = CrawlerRunConfig(\n        # e.g., pass js_code or wait_for if the page is dynamic\n        # wait_for=\"css:.crypto-row:nth-child(20)\"\n        cache_mode = CacheMode.BYPASS,\n        extraction_strategy=extraction_strategy,\n    )\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # 4. Run the crawl and extraction\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # 5. Parse the extracted JSON\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin entries\")\n        print(json.dumps(data[0], indent=2) if data else \"No data found\")\n\nasyncio.run(extract_crypto_prices())\nCopy\n```\n\n- **`baseSelector`**: Tells us where each \"item\" (crypto row) is.  \n- **`fields`**: Two fields (`coin_name` , `price`) using simple CSS selectors.  \n- Each field defines a **`type`**(e.g.,`text` , `attribute`, `html`, `regex`, etc.).\n### **XPath Example with`raw://` HTML**\nBelow is a short example demonstrating **XPath** extraction plus the **`raw://`**scheme. We'll pass a**dummy HTML** directly (no network request) and define the extraction strategy in `CrawlerRunConfig`. \n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai import JsonXPathExtractionStrategy\n\nasync def extract_crypto_prices_xpath():\n    # 1. Minimal dummy HTML with some repeating rows\n    dummy_html = \"\"\"\n    <html>\n      <body>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Bitcoin</h2>\n          <span class='coin-price'>$28,000</span>\n        </div>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Ethereum</h2>\n          <span class='coin-price'>$1,800</span>\n        </div>\n      </body>\n    </html>\n    \"\"\"\n\n    # 2. Define the JSON schema (XPath version)\n    schema = {\n        \"name\": \"Crypto Prices via XPath\",\n        \"baseSelector\": \"//div[@class='crypto-row']\",\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \".//h2[@class='coin-name']\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \".//span[@class='coin-price']\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 3. Place the strategy in the CrawlerRunConfig\n    config = CrawlerRunConfig(\n        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)\n    )\n\n    # 4. Use raw:// scheme to pass dummy_html directly\n    raw_url = f\"raw://{dummy_html}\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=raw_url,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin rows\")\n        if data:\n            print(\"First item:\", data[0])\n\nasyncio.run(extract_crypto_prices_xpath())\nCopy\n```\n\n1. **`JsonXPathExtractionStrategy`**is used instead of`JsonCssExtractionStrategy`.  \n2. **`baseSelector`**and each field's`\"selector\"` use **XPath** instead of CSS.  \n3. **`raw://`**lets us pass`dummy_html` with no real network requestâ€”handy for local testing.  \n4. Everything (including the extraction strategy) is in **`CrawlerRunConfig`**.  \nThat's how you keep the config self-contained, illustrate **XPath** usage, and demonstrate the **raw** scheme for direct HTML inputâ€”all while avoiding the old approach of passing `extraction_strategy` directly to `arun()`.\n## 3. Advanced Schema & Nested Structures\n### Sample E-Commerce HTML\n```\nhttps://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\nCopy\n```\n\n```\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    # (1) We can define optional baseFields if we want to extract attributes \n    # from the category container\n    \"baseFields\": [\n        {\"name\": \"data_cat_id\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"}, \n    ],\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",    # repeated sub-objects\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",  # single sub-object\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\"name\": \"feature\", \"type\": \"text\"} \n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\", \n                            \"selector\": \"span.reviewer\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\", \n                            \"selector\": \"span.rating\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\", \n                            \"selector\": \"p.review-text\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\", \n                            \"selector\": \"span.related-name\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\", \n                            \"selector\": \"span.related-price\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\nCopy\n```\n\n- **Nested vs. List** :  \n- **`type: \"nested\"`**means a**single** sub-object (like `details`).  \n- **`type: \"list\"`**means multiple items that are**simple** dictionaries or single text fields.  \n- **`type: \"nested_list\"`**means repeated**complex** objects (like `products` or `reviews`). - **Base Fields** : We can extract **attributes** from the container element via `\"baseFields\"`. For instance, `\"data_cat_id\"` might be `data-cat-id=\"elect123\"`.  \n- **Transforms** : We can also define a `transform` if we want to lower/upper case, strip whitespace, or even run a custom function.\n### Running the Extraction\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai import JsonCssExtractionStrategy\n\necommerce_schema = {\n    # ... the advanced schema from above ...\n}\n\nasync def extract_ecommerce_data():\n    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)\n\n    config = CrawlerRunConfig()\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=strategy,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # Parse the JSON output\n        data = json.loads(result.extracted_content)\n        print(json.dumps(data, indent=2) if data else \"No data found.\")\n\nasyncio.run(extract_ecommerce_data())\nCopy\n```\n\nIf all goes well, you get a **structured** JSON array with each \"category,\" containing an array of `products`. Each product includes `details`, `features`, `reviews`, etc. All of that **without** an LLM.\n## 4. RegexExtractionStrategy - Fast Pattern-Based Extraction\nCrawl4AI now offers a powerful new zero-LLM extraction strategy: `RegexExtractionStrategy`. This strategy provides lightning-fast extraction of common data types like emails, phone numbers, URLs, dates, and more using pre-compiled regular expressions.\n### Key Features\n  * **Zero LLM Dependency** : Extracts data without any AI model calls\n  * **Blazing Fast** : Uses pre-compiled regex patterns for maximum performance\n  * **Built-in Patterns** : Includes ready-to-use patterns for common data types\n\n\n### Simple Example: Extracting Common Entities\n```\nimport json\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy\n)\n\nasync def extract_with_regex():\n    # Create a strategy using built-in patterns for URLs and currencies\n    strategy = RegexExtractionStrategy(\n        pattern = RegexExtractionStrategy.Url | RegexExtractionStrategy.Currency\n    )\n\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data[:5]:  # Show first 5 matches\n                print(f\"{item['label']}: {item['value']}\")\n            print(f\"Total matches: {len(data)}\")\n\nasyncio.run(extract_with_regex())\nCopy\n```\n\n### Available Built-in Patterns\n`RegexExtractionStrategy` provides these common patterns as IntFlag attributes for easy combining: \n```\n# Use individual patterns\nstrategy = RegexExtractionStrategy(pattern=RegexExtractionStrategy.Email)\n\n# Combine multiple patterns\nstrategy = RegexExtractionStrategy(\n    pattern = (\n        RegexExtractionStrategy.Email | \n        RegexExtractionStrategy.PhoneUS | \n        RegexExtractionStrategy.Url\n    )\n)\n\n# Use all available patterns\nstrategy = RegexExtractionStrategy(pattern=RegexExtractionStrategy.All)\nCopy\n```\n\n- `Email` - Email addresses - `PhoneIntl` - International phone numbers - `PhoneUS` - US-format phone numbers - `Url` - HTTP/HTTPS URLs - `IPv4` - IPv4 addresses - `IPv6` - IPv6 addresses - `Uuid` - UUIDs - `Currency` - Currency values (USD, EUR, etc.) - `Percentage` - Percentage values - `Number` - Numeric values - `DateIso` - ISO format dates - `DateUS` - US format dates - `Time24h` - 24-hour format times - `PostalUS` - US postal codes - `PostalUK` - UK postal codes - `HexColor` - HTML hex color codes - `TwitterHandle` - Twitter handles - `Hashtag` - Hashtags - `MacAddr` - MAC addresses - `Iban` - International bank account numbers - `CreditCard` - Credit card numbers\n### Custom Pattern Example\n```\nimport json\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy\n)\n\nasync def extract_prices():\n    # Define a custom pattern for US Dollar prices\n    price_pattern = {\"usd_price\": r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\"}\n\n    # Create strategy with custom pattern\n    strategy = RegexExtractionStrategy(custom=price_pattern)\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com/products\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data:\n                print(f\"Found price: {item['value']}\")\n\nasyncio.run(extract_prices())\nCopy\n```\n\n### LLM-Assisted Pattern Generation\n```\nimport json\nimport asyncio\nfrom pathlib import Path\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy,\n    LLMConfig\n)\n\nasync def extract_with_generated_pattern():\n    cache_dir = Path(\"./pattern_cache\")\n    cache_dir.mkdir(exist_ok=True)\n    pattern_file = cache_dir / \"price_pattern.json\"\n\n    # 1. Generate or load pattern\n    if pattern_file.exists():\n        pattern = json.load(pattern_file.open())\n        print(f\"Using cached pattern: {pattern}\")\n    else:\n        print(\"Generating pattern via LLM...\")\n\n        # Configure LLM\n        llm_config = LLMConfig(\n            provider=\"openai/gpt-4o-mini\",\n            api_token=\"env:OPENAI_API_KEY\",\n        )\n\n        # Get sample HTML for context\n        async with AsyncWebCrawler() as crawler:\n            result = await crawler.arun(\"https://example.com/products\")\n            html = result.fit_html\n\n        # Generate pattern (one-time LLM usage)\n        pattern = RegexExtractionStrategy.generate_pattern(\n            label=\"price\",\n            html=html,\n            query=\"Product prices in USD format\",\n            llm_config=llm_config,\n        )\n\n        # Cache pattern for future use\n        json.dump(pattern, pattern_file.open(\"w\"), indent=2)\n\n    # 2. Use pattern for extraction (no LLM calls)\n    strategy = RegexExtractionStrategy(custom=pattern)\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data[:10]:\n                print(f\"Extracted: {item['value']}\")\n            print(f\"Total matches: {len(data)}\")\n\nasyncio.run(extract_with_generated_pattern())\nCopy\n```\n\n1. Use an LLM once to generate a highly optimized regex for your specific site 2. Save the pattern to disk for reuse 3. Extract data using only regex (no further LLM calls) in production\n### Extraction Results Format\nThe `RegexExtractionStrategy` returns results in a consistent format: \n```\n[\n  {\n    \"url\": \"https://example.com\",\n    \"label\": \"email\",\n    \"value\": \"contact@example.com\",\n    \"span\": [145, 163]\n  },\n  {\n    \"url\": \"https://example.com\",\n    \"label\": \"url\",\n    \"value\": \"https://support.example.com\",\n    \"span\": [210, 235]\n  }\n]\nCopy\n```\n\n- `url`: The source URL - `label`: The pattern name that matched (e.g., \"email\", \"phone_us\") - `value`: The extracted text - `span`: The start and end positions in the source content\n## 5. Why \"No LLM\" Is Often Better\n## 6. Base Element Attributes & Additional Fields\nIt's easy to **extract attributes** (like `href`, `src`, or `data-xxx`) from your base or nested elements using: \n```\n{\n  \"name\": \"href\",\n  \"type\": \"attribute\",\n  \"attribute\": \"href\",\n  \"default\": null\n}\nCopy\n```\n\nYou can define them in **`baseFields`**(extracted from the main container element) or in each field's sub-lists. This is especially helpful if you need an item's link or ID stored in the parent`<div>`.\n## 7. Putting It All Together: Larger Example\nConsider a blog site. We have a schema that extracts the **URL** from each post card (via `baseFields` with an `\"attribute\": \"href\"`), plus the title, date, summary, and author: \n```\nschema = {\n  \"name\": \"Blog Posts\",\n  \"baseSelector\": \"a.blog-post-card\",\n  \"baseFields\": [\n    {\"name\": \"post_url\", \"type\": \"attribute\", \"attribute\": \"href\"}\n  ],\n  \"fields\": [\n    {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"},\n    {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"}\n  ]\n}\nCopy\n```\n\nThen run with `JsonCssExtractionStrategy(schema)` to get an array of blog post objects, each with `\"post_url\"`, `\"title\"`, `\"date\"`, `\"summary\"`, `\"author\"`.\n## 8. Tips & Best Practices\n  1. **Test** your schema on partial HTML or a test page before a big crawl. \n  2. **Combine with JS Execution** if the site loads content dynamically. You can pass `js_code` or `wait_for` in `CrawlerRunConfig`. \n  3. **Look at Logs** when `verbose=True`: if your selectors are off or your schema is malformed, it'll often show warnings. \n  4. **Use baseFields** if you need attributes from the container element (e.g., `href`, `data-id`), especially for the \"parent\" item. \n  5. **Consider Using Regex First** : For simple data types like emails, URLs, and dates, `RegexExtractionStrategy` is often the fastest approach.\n\n\n## 9. Schema Generation Utility\n  1. You're dealing with a new website structure and want a quick starting point\n  2. You need to extract complex nested data structures\n  3. You want to avoid the learning curve of CSS/XPath selector syntax\n\n\n### Using the Schema Generator\nThe schema generator is available as a static method on both `JsonCssExtractionStrategy` and `JsonXPathExtractionStrategy`. You can choose between OpenAI's GPT-4 or the open-source Ollama for schema generation: \n```\nfrom crawl4ai import JsonCssExtractionStrategy, JsonXPathExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Sample HTML with product information\nhtml = \"\"\"\n<div class=\"product-card\">\n    <h2 class=\"title\">Gaming Laptop</h2>\n    <div class=\"price\">$999.99</div>\n    <div class=\"specs\">\n        <ul>\n            <li>16GB RAM</li>\n            <li>1TB SSD</li>\n        </ul>\n    </div>\n</div>\n\"\"\"\n\n# Option 1: Using OpenAI (requires API token)\ncss_schema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"css\", \n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")\n)\n\n# Option 2: Using Ollama (open source, no token needed)\nxpath_schema = JsonXPathExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"xpath\",\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the generated schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(css_schema)\nCopy\n```\n\n### LLM Provider Options\n  1. **OpenAI GPT-4 (`openai/gpt4o`)**\n  2. Default provider\n  3. Requires an API token\n  4. Generally provides more accurate schemas\n  5. Set via environment variable: `OPENAI_API_KEY`\n  6. **Ollama (`ollama/llama3.3`)**\n  7. Open source alternative\n  8. No API token required\n  9. Self-hosted option\n  10. Good for development and testing\n\n\n### Benefits of Schema Generation\n### Best Practices\n  1. **Choose Provider Wisely** : \n  2. Use OpenAI for production-quality schemas\n  3. Use Ollama for development, testing, or when you need a self-hosted solution\n\n\n## 10. Conclusion\nWith Crawl4AI's LLM-free extraction strategies - `JsonCssExtractionStrategy`, `JsonXPathExtractionStrategy`, and now `RegexExtractionStrategy` - you can build powerful pipelines that: - Scrape any consistent site for structured data.  \n- Support nested objects, repeating lists, or pattern-based extraction.  \n- Scale to thousands of pages quickly and reliably. - Use **`RegexExtractionStrategy`**for fast extraction of common data types like emails, phones, URLs, dates, etc. - Use**`JsonCssExtractionStrategy`**or**`JsonXPathExtractionStrategy`**for structured data with clear HTML patterns\n# Extracting JSON (LLM)\n**Important** : LLM-based extraction can be slower and costlier than schema-based approaches. If your page data is highly structured, consider using [`JsonCssExtractionStrategy`](https://docs.crawl4ai.com/complete-sdk-reference/no-llm-strategies.md) or [`JsonXPathExtractionStrategy`](https://docs.crawl4ai.com/complete-sdk-reference/no-llm-strategies.md) first. But if you need AI to interpret or reorganize content, read on!\n## 1. Why Use an LLM?\n## 2. Provider-Agnostic via LiteLLM\n```\nllmConfig = LlmConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))\nCopy\n```\n\nCrawl4AI uses a â€œprovider stringâ€ (e.g., `\"openai/gpt-4o\"`, `\"ollama/llama2.0\"`, `\"aws/titan\"`) to identify your LLM. **Any** model that LiteLLM supports is fair game. You just provide: - **`provider`**: The`<provider>/<model_name>` identifier (e.g., `\"openai/gpt-4\"`, `\"ollama/llama2\"`, `\"huggingface/google-flan\"`, etc.).  \n- **`api_token`**: If needed (for OpenAI, HuggingFace, etc.); local models or Ollama might not require it.  \n- **`base_url`**(optional): If your provider has a custom endpoint.\n## 3. How LLM Extraction Works\n### 3.1 Flow\n1. **Chunking** (optional): The HTML or markdown is split into smaller segments if itâ€™s very long (based on `chunk_token_threshold`, overlap, etc.).  \n2. **Prompt Construction** : For each chunk, the library forms a prompt that includes your **`instruction`**(and possibly schema or examples).  \n4. **Combining** : The results from each chunk are merged and parsed into JSON.\n### 3.2 `extraction_type`\n  * **`\"schema\"`**: The model tries to return JSON conforming to your Pydantic-based schema.\n  * **`\"block\"`**: The model returns freeform text, or smaller JSON structures, which the library collects.  \nFor structured data, `\"schema\"` is recommended. You provide `schema=YourPydanticModel.model_json_schema()`.\n\n\n## 4. Key Parameters\nBelow is an overview of important LLM extraction parameters. All are typically set inside `LLMExtractionStrategy(...)`. You then put that strategy in your `CrawlerRunConfig(..., extraction_strategy=...)`. 1. **`llmConfig`**(LlmConfig): e.g.,`\"openai/gpt-4\"` , `\"ollama/llama2\"`.   \n2. **`schema`**(dict): A JSON schema describing the fields you want. Usually generated by`YourModel.model_json_schema()`.  \n3. **`extraction_type`**(str):`\"schema\"` or `\"block\"`.  \n4. **`instruction`**(str): Prompt text telling the LLM what you want extracted. E.g., â€œExtract these fields as a JSON array.â€  \n5. **`chunk_token_threshold`**(int): Maximum tokens per chunk. If your content is huge, you can break it up for the LLM.  \n6. **`overlap_rate`**(float): Overlap ratio between adjacent chunks. E.g.,`0.1` means 10% of each chunk is repeated to preserve context continuity.  \n7. **`apply_chunking`**(bool): Set`True` to chunk automatically. If you want a single pass, set `False`.  \n8. **`input_format`**(str): Determines**which** crawler result is passed to the LLM. Options include:  \n- `\"markdown\"`: The raw markdown (default).  \n- `\"fit_markdown\"`: The filtered â€œfitâ€ markdown if you used a content filter.  \n- `\"html\"`: The cleaned or raw HTML.  \n9. **`extra_args`**(dict): Additional LLM parameters like`temperature` , `max_tokens`, `top_p`, etc.  \n10. **`show_usage()`**: A method you can call to print out usage info (token usage per chunk, total cost if known).  \n\n```\nextraction_strategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"openai/gpt-4\", api_token=\"YOUR_OPENAI_KEY\"),\n    schema=MyModel.model_json_schema(),\n    extraction_type=\"schema\",\n    instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\",\n    chunk_token_threshold=1200,\n    overlap_rate=0.1,\n    apply_chunking=True,\n    input_format=\"html\",\n    extra_args={\"temperature\": 0.1, \"max_tokens\": 1000},\n    verbose=True\n)\nCopy\n```\n\n## 5. Putting It in `CrawlerRunConfig`\n**Important** : In Crawl4AI, all strategy definitions should go inside the `CrawlerRunConfig`, not directly as a param in `arun()`. Hereâ€™s a full example: \n```\nimport os\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: str\n\nasync def main():\n    # 1. Define the LLM extraction strategy\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=Product.schema_json(), # Or use model_json_schema()\n        extraction_type=\"schema\",\n        instruction=\"Extract all product objects with 'name' and 'price' from the content.\",\n        chunk_token_threshold=1000,\n        overlap_rate=0.0,\n        apply_chunking=True,\n        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n        extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n    )\n\n    # 2. Build the crawler config\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strategy,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3. Create a browser config if needed\n    browser_cfg = BrowserConfig(headless=True)\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        # 4. Let's say we want to crawl a single page\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=crawl_config\n        )\n\n        if result.success:\n            # 5. The extracted content is presumably JSON\n            data = json.loads(result.extracted_content)\n            print(\"Extracted items:\", data)\n\n            # 6. Show usage stats\n            llm_strategy.show_usage()  # prints token usage\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## 6. Chunking Details\n### 6.1 `chunk_token_threshold`\nIf your page is large, you might exceed your LLMâ€™s context window. **`chunk_token_threshold`**sets the approximate max tokens per chunk. The library calculates wordâ†’token ratio using`word_token_rate` (often ~0.75 by default). If chunking is enabled (`apply_chunking=True`), the text is split into segments.\n### 6.2 `overlap_rate`\nTo keep context continuous across chunks, we can overlap them. E.g., `overlap_rate=0.1` means each subsequent chunk includes 10% of the previous chunkâ€™s text. This is helpful if your needed info might straddle chunk boundaries.\n### 6.3 Performance & Parallelism\n## 7. Input Format\nBy default, **LLMExtractionStrategy** uses `input_format=\"markdown\"`, meaning the **crawlerâ€™s final markdown** is fed to the LLM. You can change to: - **`html`**: The cleaned HTML or raw HTML (depending on your crawler config) goes into the LLM.  \n- **`fit_markdown`**: If you used, for instance,`PruningContentFilter` , the â€œfitâ€ version of the markdown is used. This can drastically reduce tokens if you trust the filter.  \n- **`markdown`**: Standard markdown output from the crawlerâ€™s`markdown_generator`. This setting is crucial: if the LLM instructions rely on HTML tags, pick `\"html\"`. If you prefer a text-based approach, pick `\"markdown\"`. \n```\nLLMExtractionStrategy(\n    # ...\n    input_format=\"html\",  # Instead of \"markdown\" or \"fit_markdown\"\n)\nCopy\n```\n\n## 8. Token Usage & Show Usage\n  * **`usages`**(list): token usage per chunk or call.\n  * **`total_usage`**: sum of all chunk calls.\n  * **`show_usage()`**: prints a usage report (if the provider returns usage data).\n```\nllm_strategy = LLMExtractionStrategy(...)\n# ...\nllm_strategy.show_usage()\n# e.g. â€œTotal usage: 1241 tokens across 2 chunk callsâ€\nCopy\n```\n\n\n\n## 9. Example: Building a Knowledge Graph\nBelow is a snippet combining **`LLMExtractionStrategy`**with a Pydantic schema for a knowledge graph. Notice how we pass an**`instruction`**telling the model what to parse.\n```\nimport os\nimport json\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass Entity(BaseModel):\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    entity1: Entity\n    entity2: Entity\n    description: str\n    relation_type: str\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[Entity]\n    relationships: List[Relationship]\n\nasync def main():\n    # LLM extraction strategy\n    llm_strat = LLMExtractionStrategy(\n        llmConfig = LLMConfig(provider=\"openai/gpt-4\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=KnowledgeGraph.model_json_schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract entities and relationships from the content. Return valid JSON.\",\n        chunk_token_threshold=1400,\n        apply_chunking=True,\n        input_format=\"html\",\n        extra_args={\"temperature\": 0.1, \"max_tokens\": 1500}\n    )\n\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strat,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        # Example page\n        url = \"https://www.nbcnews.com/business\"\n        result = await crawler.arun(url=url, config=crawl_config)\n\n        print(\"--- LLM RAW RESPONSE ---\")\n        print(result.extracted_content)\n        print(\"--- END LLM RAW RESPONSE ---\")\n\n        if result.success:\n            with open(\"kb_result.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.extracted_content)\n            llm_strat.show_usage()\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n- **`extraction_type=\"schema\"`**ensures we get JSON fitting our`KnowledgeGraph`.  \n- **`input_format=\"html\"`**means we feed HTML to the model.  \n- **`instruction`**guides the model to output a structured knowledge graph.\n## 10. Best Practices & Caveats\n4. **Schema Strictness** : `\"schema\"` extraction tries to parse the model output as JSON. If the model returns invalid JSON, partial extraction might happen, or you might get an error. \n## 11. Conclusion\n  * Put your LLM strategy **in`CrawlerRunConfig`**. \n  * Use **`input_format`**to pick which form (markdown, HTML, fit_markdown) the LLM sees.\n  * Tweak **`chunk_token_threshold`**,**`overlap_rate`**, and**`apply_chunking`**to handle large content efficiently.\n  * Monitor token usage with `show_usage()`. If your siteâ€™s data is consistent or repetitive, consider [`JsonCssExtractionStrategy`](https://docs.crawl4ai.com/complete-sdk-reference/no-llm-strategies.md) first for speed and simplicity. But if you need an **AI-driven** approach, `LLMExtractionStrategy` offers a flexible, multi-provider solution for extracting structured JSON from any website. 1. **Experiment with Different Providers**\n  * Try switching the `provider` (e.g., `\"ollama/llama2\"`, `\"openai/gpt-4o\"`, etc.) to see differences in speed, accuracy, or cost. \n  * Pass different `extra_args` like `temperature`, `top_p`, and `max_tokens` to fine-tune your results. 2. **Performance Tuning**\n  * If pages are large, tweak `chunk_token_threshold`, `overlap_rate`, or `apply_chunking` to optimize throughput. \n  * Check the usage logs with `show_usage()` to keep an eye on token consumption and identify potential bottlenecks. 3. **Validate Outputs**\n  * If using `extraction_type=\"schema\"`, parse the LLMâ€™s JSON with a Pydantic model for a final validation step.  \n4. **Explore Hooks & Automation**\n\n\n# Advanced Features\n# Session Management\n  * **Performing JavaScript actions before and after crawling.** Use `BrowserConfig` and `CrawlerRunConfig` to maintain state with a `session_id`: \n```\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n\n    # Define configurations\n    config1 = CrawlerRunConfig(\n        url=\"https://example.com/page1\", session_id=session_id\n    )\n    config2 = CrawlerRunConfig(\n        url=\"https://example.com/page2\", session_id=session_id\n    )\n\n    # First request\n    result1 = await crawler.arun(config=config1)\n\n    # Subsequent request using the same session\n    result2 = await crawler.arun(config=config2)\n\n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)\nCopy\n```\n\n```\nfrom crawl4ai.async_configs import CrawlerRunConfig\nfrom crawl4ai import JsonCssExtractionStrategy\nfrom crawl4ai.cache_context import CacheMode\n\nasync def crawl_dynamic_content():\n    url = \"https://github.com/microsoft/TypeScript/commits/main\"\n    session_id = \"wait_for_session\"\n    all_commits = []\n\n    js_next_page = \"\"\"\n    const commits = document.querySelectorAll('li[data-testid=\"commit-row-item\"] h4');\n    if (commits.length > 0) {\n        window.lastCommit = commits[0].textContent.trim();\n    }\n    const button = document.querySelector('a[data-testid=\"pagination-next-button\"]');\n    if (button) {button.click(); console.log('button clicked') }\n    \"\"\"\n\n    wait_for = \"\"\"() => {\n        const commits = document.querySelectorAll('li[data-testid=\"commit-row-item\"] h4');\n        if (commits.length === 0) return false;\n        const firstCommit = commits[0].textContent.trim();\n        return firstCommit !== window.lastCommit;\n    }\"\"\"\n\n    schema = {\n        \"name\": \"Commit Extractor\",\n        \"baseSelector\": \"li[data-testid='commit-row-item']\",\n        \"fields\": [\n            {\n                \"name\": \"title\",\n                \"selector\": \"h4 a\",\n                \"type\": \"text\",\n                \"transform\": \"strip\",\n            },\n        ],\n    }\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    browser_config = BrowserConfig(\n        verbose=True,\n        headless=False,\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        for page in range(3):\n            crawler_config = CrawlerRunConfig(\n                session_id=session_id,\n                css_selector=\"li[data-testid='commit-row-item']\",\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS,\n                capture_console_messages=True,\n            )\n\n            result = await crawler.arun(url=url, config=crawler_config)\n\n            if result.console_messages:\n                print(f\"Page {page + 1} console messages:\", result.console_messages)\n\n            if result.extracted_content:\n                # print(f\"Page {page + 1} result:\", result.extracted_content)\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n            else:\n                print(f\"Page {page + 1}: No content extracted\")\n\n        print(f\"Successfully crawled {len(all_commits)} commits across 3 pages\")\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\nCopy\n```\n\n\n\n## Example 1: Basic Session-Based Crawling\n```\nimport asyncio\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nfrom crawl4ai.cache_context import CacheMode\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"dynamic_content_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())\nCopy\n```\n\n1. Reusing the same `session_id` across multiple requests. 2. Executing JavaScript to load more content dynamically. 3. Properly closing the session to free resources.\n## Advanced Technique 1: Custom Execution Hooks\n```\nasync def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\").strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear: {e}\")\n\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"commit_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        js_next_page = \"\"\"document.querySelector('a.pagination-next').click();\"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(advanced_session_crawl_with_hooks())\nCopy\n```\n\n## Advanced Technique 2: Integrated JavaScript Execution and Waiting\n```\nasync def integrated_js_and_wait_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"integrated_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n\n        js_next_page_and_wait = \"\"\"\n        (async () => {\n            const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim();\n            const initialCommit = getCurrentCommit();\n            document.querySelector('a.pagination-next').click();\n            while (getCurrentCommit() === initialCommit) {\n                await new Promise(resolve => setTimeout(resolve, 100));\n            }\n        })();\n        \"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page_and_wait if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(integrated_js_and_wait_crawl())\nCopy\n```\n\n1. **Authentication Flows** : Login and interact with secured pages. 2. **Pagination Handling** : Navigate through multiple pages. 3. **Form Submissions** : Fill forms, submit, and process results. 4. **Multi-step Processes** : Complete workflows that span multiple actions.\n# Hooks & Auth in AsyncWebCrawler\n1. **`on_browser_created`**â€“ After browser creation.  \n2. **`on_page_context_created`**â€“ After a new context & page are created.  \n3. **`before_goto`**â€“ Just before navigating to a page.  \n4. **`after_goto`**â€“ Right after navigation completes.  \n5. **`on_user_agent_updated`**â€“ Whenever the user agent changes.  \n6. **`on_execution_started`**â€“ Once custom JavaScript execution begins.  \n7. **`before_retrieve_html`**â€“ Just before the crawler retrieves final HTML.  \n8. **`before_return_html`**â€“ Right before returning the HTML content.**Important** : Avoid heavy tasks in `on_browser_created` since you donâ€™t yet have a page context. If you need to _log in_ , do so in **`on_page_context_created`**.\n> note \"Important Hook Usage Warning\" **Avoid Misusing Hooks** : Do not manipulate page objects in the wrong hook or at the wrong time, as it can crash the pipeline or produce incorrect results. A common mistake is attempting to handle authentication prematurelyâ€”such as creating or closing pages in `on_browser_created`. **Use the Right Hook for Auth** : If you need to log in or set tokens, use `on_page_context_created`. This ensures you have a valid page/context to work with, without disrupting the main crawling flow.\n## Example: Using Hooks in AsyncWebCrawler\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom playwright.async_api import Page, BrowserContext\n\nasync def main():\n    print(\"ðŸ”— Hooks Example: Demonstrating recommended usage\")\n\n    # 1) Configure the browser\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=True\n    )\n\n    # 2) Configure the crawler run\n    crawler_run_config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"body\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3) Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n\n    #\n    # Define Hook Functions\n    #\n\n    async def on_browser_created(browser, **kwargs):\n        # Called once the browser instance is created (but no pages or contexts yet)\n        print(\"[HOOK] on_browser_created - Browser created successfully!\")\n        # Typically, do minimal setup here if needed\n        return browser\n\n    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):\n        # Called right after a new page + context are created (ideal for auth or route config).\n        print(\"[HOOK] on_page_context_created - Setting up page & context.\")\n\n        # Example 1: Route filtering (e.g., block images)\n        async def route_filter(route):\n            if route.request.resource_type == \"image\":\n                print(f\"[HOOK] Blocking image request: {route.request.url}\")\n                await route.abort()\n            else:\n                await route.continue_()\n\n        await context.route(\"**\", route_filter)\n\n        # Example 2: (Optional) Simulate a login scenario\n        # (We do NOT create or close pages here, just do quick steps if needed)\n        # e.g., await page.goto(\"https://example.com/login\")\n        # e.g., await page.fill(\"input[name='username']\", \"testuser\")\n        # e.g., await page.fill(\"input[name='password']\", \"password123\")\n        # e.g., await page.click(\"button[type='submit']\")\n        # e.g., await page.wait_for_selector(\"#welcome\")\n        # e.g., await context.add_cookies([...])\n        # Then continue\n\n        # Example 3: Adjust the viewport\n        await page.set_viewport_size({\"width\": 1080, \"height\": 600})\n        return page\n\n    async def before_goto(\n        page: Page, context: BrowserContext, url: str, **kwargs\n    ):\n        # Called before navigating to each URL.\n        print(f\"[HOOK] before_goto - About to navigate: {url}\")\n        # e.g., inject custom headers\n        await page.set_extra_http_headers({\n            \"Custom-Header\": \"my-value\"\n        })\n        return page\n\n    async def after_goto(\n        page: Page, context: BrowserContext, \n        url: str, response, **kwargs\n    ):\n        # Called after navigation completes.\n        print(f\"[HOOK] after_goto - Successfully loaded: {url}\")\n        # e.g., wait for a certain element if we want to verify\n        try:\n            await page.wait_for_selector('.content', timeout=1000)\n            print(\"[HOOK] Found .content element!\")\n        except:\n            print(\"[HOOK] .content not found, continuing anyway.\")\n        return page\n\n    async def on_user_agent_updated(\n        page: Page, context: BrowserContext, \n        user_agent: str, **kwargs\n    ):\n        # Called whenever the user agent updates.\n        print(f\"[HOOK] on_user_agent_updated - New user agent: {user_agent}\")\n        return page\n\n    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):\n        # Called after custom JavaScript execution begins.\n        print(\"[HOOK] on_execution_started - JS code is running!\")\n        return page\n\n    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):\n        # Called before final HTML retrieval.\n        print(\"[HOOK] before_retrieve_html - We can do final actions\")\n        # Example: Scroll again\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n        return page\n\n    async def before_return_html(\n        page: Page, context: BrowserContext, html: str, **kwargs\n    ):\n        # Called just before returning the HTML in the result.\n        print(f\"[HOOK] before_return_html - HTML length: {len(html)}\")\n        return page\n\n    #\n    # Attach Hooks\n    #\n\n    crawler.crawler_strategy.set_hook(\"on_browser_created\", on_browser_created)\n    crawler.crawler_strategy.set_hook(\n        \"on_page_context_created\", on_page_context_created\n    )\n    crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n    crawler.crawler_strategy.set_hook(\"after_goto\", after_goto)\n    crawler.crawler_strategy.set_hook(\n        \"on_user_agent_updated\", on_user_agent_updated\n    )\n    crawler.crawler_strategy.set_hook(\n        \"on_execution_started\", on_execution_started\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_retrieve_html\", before_retrieve_html\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_return_html\", before_return_html\n    )\n\n    await crawler.start()\n\n    # 4) Run the crawler on an example page\n    url = \"https://example.com\"\n    result = await crawler.arun(url, config=crawler_run_config)\n\n    if result.success:\n        print(\"\\nCrawled URL:\", result.url)\n        print(\"HTML length:\", len(result.html))\n    else:\n        print(\"Error:\", result.error_message)\n\n    await crawler.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Hook Lifecycle Summary\n1. **`on_browser_created`**:  \n- Browser is up, but **no** pages or contexts yet.  \n- Light setup onlyâ€”donâ€™t try to open or close pages here (that belongs in `on_page_context_created`). 2. **`on_page_context_created`**:  \n- Perfect for advanced **auth** or route blocking.  \n3. **`before_goto`**:  \n4. **`after_goto`**:  \n5. **`on_user_agent_updated`**:  \n- Whenever the user agent changes (for stealth or different UA modes). 6. **`on_execution_started`**:  \n- If you set `js_code` or run custom scripts, this runs once your JS is about to start. 7. **`before_retrieve_html`**:  \n8. **`before_return_html`**:  \n- The last hook before returning HTML to the `CrawlResult`. Good for logging HTML length or minor modifications.\n## When to Handle Authentication\n**Recommended** : Use **`on_page_context_created`**if you need to: - Navigate to a login page or fill forms - Set cookies or localStorage tokens - Block resource routes to avoid ads This ensures the newly created context is under your control**before** `arun()` navigates to the main URL.\n## Additional Considerations\n  * **Session Management** : If you want multiple `arun()` calls to reuse a single session, pass `session_id=` in your `CrawlerRunConfig`. Hooks remain the same. \n  * **Concurrency** : If you run `arun_many()`, each URL triggers these hooks in parallel. Ensure your hooks are thread/async-safe.\n\n\n## Conclusion\n  * **Browser** creation (light tasks only)\n  * **Page** and **context** creation (auth, route blocking)\n  * **Navigation** phases\n  * **Final HTML** retrieval\n  * **Login** or advanced tasks in `on_page_context_created`\n  * **Custom headers** or logs in `before_goto` / `after_goto`\n  * **Scrolling** or final checks in `before_retrieve_html` / `before_return_html`\n\n\n* * *\n# Quick Reference\n## Core Imports\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy, CosineStrategy\nCopy\n```\n\n## Basic Pattern\n```\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n    print(result.markdown)\nCopy\n```\n\n## Advanced Pattern\n```\nbrowser_config = BrowserConfig(headless=True, viewport_width=1920)\ncrawler_config = CrawlerConfig(\n    cache_mode=CacheMode.BYPASS,\n    wait_for=\"css:.content\",\n    screenshot=True,\n    pdf=True\n)\nstrategy = LLMExtractionStrategy(\n    provider=\"openai/gpt-4\",\n    instruction=\"Extract products with name and price\"\n)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=crawler_config,\n        extraction_strategy=strategy\n    )\nCopy\n```\n\n## Multi-URL Pattern\n```\nurls = [\"https://example.com/1\", \"https://example.com/2\"]\nresults = await crawler.arun_many(urls, config=crawler_config)\nCopy\n```\n\n* * *\n**End of Crawl4AI SDK Documentation**\n#### On this page\n  * [Navigation](https://docs.crawl4ai.com/complete-sdk-reference/#navigation)\n  * [1. Basic Installation](https://docs.crawl4ai.com/complete-sdk-reference/#1-basic-installation)\n  * [2. Initial Setup & Diagnostics](https://docs.crawl4ai.com/complete-sdk-reference/#2-initial-setup-diagnostics)\n  * [2.1 Run the Setup Command](https://docs.crawl4ai.com/complete-sdk-reference/#21-run-the-setup-command)\n  * [2.2 Diagnostics](https://docs.crawl4ai.com/complete-sdk-reference/#22-diagnostics)\n  * [3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor)](https://docs.crawl4ai.com/complete-sdk-reference/#3-verifying-installation-a-simple-crawl-skip-this-step-if-you-already-run-crawl4ai-doctor)\n  * [4. Advanced Installation (Optional)](https://docs.crawl4ai.com/complete-sdk-reference/#4-advanced-installation-optional)\n  * [4.1 Torch, Transformers, or All](https://docs.crawl4ai.com/complete-sdk-reference/#41-torch-transformers-or-all)\n  * [5. Docker (Experimental)](https://docs.crawl4ai.com/complete-sdk-reference/#5-docker-experimental)\n  * [6. Local Server Mode (Legacy)](https://docs.crawl4ai.com/complete-sdk-reference/#6-local-server-mode-legacy)\n  * [Summary](https://docs.crawl4ai.com/complete-sdk-reference/#summary)\n  * [1. Introduction](https://docs.crawl4ai.com/complete-sdk-reference/#1-introduction)\n  * [2. Your First Crawl](https://docs.crawl4ai.com/complete-sdk-reference/#2-your-first-crawl)\n  * [3. Basic Configuration (Light Introduction)](https://docs.crawl4ai.com/complete-sdk-reference/#3-basic-configuration-light-introduction)\n  * [4. Generating Markdown Output](https://docs.crawl4ai.com/complete-sdk-reference/#4-generating-markdown-output)\n  * [Example: Using a Filter with DefaultMarkdownGenerator](https://docs.crawl4ai.com/complete-sdk-reference/#example-using-a-filter-with-defaultmarkdowngenerator)\n  * [5. Simple Data Extraction (CSS-based)](https://docs.crawl4ai.com/complete-sdk-reference/#5-simple-data-extraction-css-based)\n  * [6. Simple Data Extraction (LLM-based)](https://docs.crawl4ai.com/complete-sdk-reference/#6-simple-data-extraction-llm-based)\n  * [7. Adaptive Crawling (New!)](https://docs.crawl4ai.com/complete-sdk-reference/#7-adaptive-crawling-new)\n  * [8. Multi-URL Concurrency (Preview)](https://docs.crawl4ai.com/complete-sdk-reference/#8-multi-url-concurrency-preview)\n  * [8. Dynamic Content Example](https://docs.crawl4ai.com/complete-sdk-reference/#8-dynamic-content-example)\n  * [9. Next Steps](https://docs.crawl4ai.com/complete-sdk-reference/#9-next-steps)\n  * [1. Constructor Overview](https://docs.crawl4ai.com/complete-sdk-reference/#1-constructor-overview)\n  * [2. Lifecycle: Start/Close or Context Manager](https://docs.crawl4ai.com/complete-sdk-reference/#2-lifecycle-startclose-or-context-manager)\n  * [2.1 Context Manager (Recommended)](https://docs.crawl4ai.com/complete-sdk-reference/#21-context-manager-recommended)\n  * [2.2 Manual Start & Close](https://docs.crawl4ai.com/complete-sdk-reference/#22-manual-start-close)\n  * [3. Primary Method: arun()](https://docs.crawl4ai.com/complete-sdk-reference/#3-primary-method-arun)\n  * [3.1 New Approach](https://docs.crawl4ai.com/complete-sdk-reference/#31-new-approach)\n  * [3.2 Legacy Parameters Still Accepted](https://docs.crawl4ai.com/complete-sdk-reference/#32-legacy-parameters-still-accepted)\n  * [4. Batch Processing: arun_many()](https://docs.crawl4ai.com/complete-sdk-reference/#4-batch-processing-arun_many)\n  * [4.1 Resource-Aware Crawling](https://docs.crawl4ai.com/complete-sdk-reference/#41-resource-aware-crawling)\n  * [4.2 Example Usage](https://docs.crawl4ai.com/complete-sdk-reference/#42-example-usage)\n  * [7. Best Practices & Migration Notes](https://docs.crawl4ai.com/complete-sdk-reference/#7-best-practices-migration-notes)\n  * [8. Summary](https://docs.crawl4ai.com/complete-sdk-reference/#8-summary)\n  * [1. Core Usage](https://docs.crawl4ai.com/complete-sdk-reference/#1-core-usage)\n  * [2. Cache Control](https://docs.crawl4ai.com/complete-sdk-reference/#2-cache-control)\n  * [3. Content Processing & Selection](https://docs.crawl4ai.com/complete-sdk-reference/#3-content-processing-selection)\n  * [3.1 Text Processing](https://docs.crawl4ai.com/complete-sdk-reference/#31-text-processing)\n  * [3.2 Content Selection](https://docs.crawl4ai.com/complete-sdk-reference/#32-content-selection)\n  * [3.3 Link Handling](https://docs.crawl4ai.com/complete-sdk-reference/#33-link-handling)\n  * [3.4 Media Filtering](https://docs.crawl4ai.com/complete-sdk-reference/#34-media-filtering)\n  * [4. Page Navigation & Timing](https://docs.crawl4ai.com/complete-sdk-reference/#4-page-navigation-timing)\n  * [4.1 Basic Browser Flow](https://docs.crawl4ai.com/complete-sdk-reference/#41-basic-browser-flow)\n  * [4.2 JavaScript Execution](https://docs.crawl4ai.com/complete-sdk-reference/#42-javascript-execution)\n  * [4.3 Anti-Bot](https://docs.crawl4ai.com/complete-sdk-reference/#43-anti-bot)\n  * [5. Session Management](https://docs.crawl4ai.com/complete-sdk-reference/#5-session-management)\n  * [6. Screenshot, PDF & Media Options](https://docs.crawl4ai.com/complete-sdk-reference/#6-screenshot-pdf-media-options)\n  * [7. Extraction Strategy](https://docs.crawl4ai.com/complete-sdk-reference/#7-extraction-strategy)\n  * [8. Comprehensive Example](https://docs.crawl4ai.com/complete-sdk-reference/#8-comprehensive-example)\n  * [9. Best Practices](https://docs.crawl4ai.com/complete-sdk-reference/#9-best-practices)\n  * [10. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#10-conclusion)\n  * [Function Signature](https://docs.crawl4ai.com/complete-sdk-reference/#function-signature)\n  * [Differences from arun()](https://docs.crawl4ai.com/complete-sdk-reference/#differences-from-arun)\n  * [Basic Example (Batch Mode)](https://docs.crawl4ai.com/complete-sdk-reference/#basic-example-batch-mode)\n  * [Streaming Example](https://docs.crawl4ai.com/complete-sdk-reference/#streaming-example)\n  * [With a Custom Dispatcher](https://docs.crawl4ai.com/complete-sdk-reference/#with-a-custom-dispatcher)\n  * [URL-Specific Configurations](https://docs.crawl4ai.com/complete-sdk-reference/#url-specific-configurations)\n  * [Return Value](https://docs.crawl4ai.com/complete-sdk-reference/#return-value)\n  * [Dispatcher Reference](https://docs.crawl4ai.com/complete-sdk-reference/#dispatcher-reference)\n  * [Common Pitfalls](https://docs.crawl4ai.com/complete-sdk-reference/#common-pitfalls)\n  * [Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#conclusion)\n  * [1. Basic Crawl Info](https://docs.crawl4ai.com/complete-sdk-reference/#1-basic-crawl-info)\n  * [1.1 url (str)](https://docs.crawl4ai.com/complete-sdk-reference/#11-url-str)\n  * [1.2 success (bool)](https://docs.crawl4ai.com/complete-sdk-reference/#12-success-bool)\n  * [1.3 status_code (Optional[int])](https://docs.crawl4ai.com/complete-sdk-reference/#13-status_code-optionalint)\n  * [1.4 error_message (Optional[str])](https://docs.crawl4ai.com/complete-sdk-reference/#14-error_message-optionalstr)\n  * [1.5 session_id (Optional[str])](https://docs.crawl4ai.com/complete-sdk-reference/#15-session_id-optionalstr)\n  * [1.6 response_headers (Optional[dict])](https://docs.crawl4ai.com/complete-sdk-reference/#16-response_headers-optionaldict)\n  * [1.7 ssl_certificate (Optional[SSLCertificate])](https://docs.crawl4ai.com/complete-sdk-reference/#17-ssl_certificate-optionalsslcertificate)\n  * [2. Raw / Cleaned Content](https://docs.crawl4ai.com/complete-sdk-reference/#2-raw-cleaned-content)\n  * [2.1 html (str)](https://docs.crawl4ai.com/complete-sdk-reference/#21-html-str)\n  * [2.2 cleaned_html (Optional[str])](https://docs.crawl4ai.com/complete-sdk-reference/#22-cleaned_html-optionalstr)\n  * [3. Markdown Fields](https://docs.crawl4ai.com/complete-sdk-reference/#3-markdown-fields)\n  * [3.1 The Markdown Generation Approach](https://docs.crawl4ai.com/complete-sdk-reference/#31-the-markdown-generation-approach)\n  * [3.2 markdown (Optional[Union[str, MarkdownGenerationResult]])](https://docs.crawl4ai.com/complete-sdk-reference/#32-markdown-optionalunionstr-markdowngenerationresult)\n  * [4. Media & Links](https://docs.crawl4ai.com/complete-sdk-reference/#4-media-links)\n  * [4.1 media (Dict[str, List[Dict]])](https://docs.crawl4ai.com/complete-sdk-reference/#41-media-dictstr-listdict)\n  * [4.2 links (Dict[str, List[Dict]])](https://docs.crawl4ai.com/complete-sdk-reference/#42-links-dictstr-listdict)\n  * [5. Additional Fields](https://docs.crawl4ai.com/complete-sdk-reference/#5-additional-fields)\n  * [5.1 extracted_content (Optional[str])](https://docs.crawl4ai.com/complete-sdk-reference/#51-extracted_content-optionalstr)\n  * [5.2 downloaded_files (Optional[List[str]])](https://docs.crawl4ai.com/complete-sdk-reference/#52-downloaded_files-optionalliststr)\n  * [5.3 screenshot (Optional[str])](https://docs.crawl4ai.com/complete-sdk-reference/#53-screenshot-optionalstr)\n  * [5.4 pdf (Optional[bytes])](https://docs.crawl4ai.com/complete-sdk-reference/#54-pdf-optionalbytes)\n  * [5.5 mhtml (Optional[str])](https://docs.crawl4ai.com/complete-sdk-reference/#55-mhtml-optionalstr)\n  * [5.6 metadata (Optional[dict])](https://docs.crawl4ai.com/complete-sdk-reference/#56-metadata-optionaldict)\n  * [6. dispatch_result (optional)](https://docs.crawl4ai.com/complete-sdk-reference/#6-dispatch_result-optional)\n  * [7. Network Requests & Console Messages](https://docs.crawl4ai.com/complete-sdk-reference/#7-network-requests-console-messages)\n  * [7.1 network_requests (Optional[List[Dict[str, Any]]])](https://docs.crawl4ai.com/complete-sdk-reference/#71-network_requests-optionallistdictstr-any)\n  * [7.2 console_messages (Optional[List[Dict[str, Any]]])](https://docs.crawl4ai.com/complete-sdk-reference/#72-console_messages-optionallistdictstr-any)\n  * [8. Example: Accessing Everything](https://docs.crawl4ai.com/complete-sdk-reference/#8-example-accessing-everything)\n  * [9. Key Points & Future](https://docs.crawl4ai.com/complete-sdk-reference/#9-key-points-future)\n  * [1. BrowserConfig Essentials](https://docs.crawl4ai.com/complete-sdk-reference/#1-browserconfig-essentials)\n  * [Key Fields to Note](https://docs.crawl4ai.com/complete-sdk-reference/#key-fields-to-note)\n  * [Helper Methods](https://docs.crawl4ai.com/complete-sdk-reference/#helper-methods)\n  * [2. CrawlerRunConfig Essentials](https://docs.crawl4ai.com/complete-sdk-reference/#2-crawlerrunconfig-essentials)\n  * [Key Fields to Note](https://docs.crawl4ai.com/complete-sdk-reference/#key-fields-to-note_1)\n  * [Helper Methods](https://docs.crawl4ai.com/complete-sdk-reference/#helper-methods_1)\n  * [3. LLMConfig Essentials](https://docs.crawl4ai.com/complete-sdk-reference/#3-llmconfig-essentials)\n  * [Key fields to note](https://docs.crawl4ai.com/complete-sdk-reference/#key-fields-to-note_2)\n  * [4. Putting It All Together](https://docs.crawl4ai.com/complete-sdk-reference/#4-putting-it-all-together)\n  * [5. Next Steps](https://docs.crawl4ai.com/complete-sdk-reference/#5-next-steps)\n  * [6. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#6-conclusion)\n  * [1.1 Parameter Highlights](https://docs.crawl4ai.com/complete-sdk-reference/#11-parameter-highlights)\n  * [Basic Usage](https://docs.crawl4ai.com/complete-sdk-reference/#basic-usage)\n  * [Understanding the Response](https://docs.crawl4ai.com/complete-sdk-reference/#understanding-the-response)\n  * [Adding Basic Options](https://docs.crawl4ai.com/complete-sdk-reference/#adding-basic-options)\n  * [Handling Errors](https://docs.crawl4ai.com/complete-sdk-reference/#handling-errors)\n  * [Logging and Debugging](https://docs.crawl4ai.com/complete-sdk-reference/#logging-and-debugging)\n  * [Complete Example](https://docs.crawl4ai.com/complete-sdk-reference/#complete-example)\n  * [1. Quick Example](https://docs.crawl4ai.com/complete-sdk-reference/#1-quick-example)\n  * [2. How Markdown Generation Works](https://docs.crawl4ai.com/complete-sdk-reference/#2-how-markdown-generation-works)\n  * [2.1 HTML-to-Text Conversion (Forked & Modified)](https://docs.crawl4ai.com/complete-sdk-reference/#21-html-to-text-conversion-forked-modified)\n  * [2.2 Link Citations & References](https://docs.crawl4ai.com/complete-sdk-reference/#22-link-citations-references)\n  * [2.3 Optional Content Filters](https://docs.crawl4ai.com/complete-sdk-reference/#23-optional-content-filters)\n  * [3. Configuring the Default Markdown Generator](https://docs.crawl4ai.com/complete-sdk-reference/#3-configuring-the-default-markdown-generator)\n  * [4. Selecting the HTML Source for Markdown Generation](https://docs.crawl4ai.com/complete-sdk-reference/#4-selecting-the-html-source-for-markdown-generation)\n  * [HTML Source Options](https://docs.crawl4ai.com/complete-sdk-reference/#html-source-options)\n  * [When to Use Each Option](https://docs.crawl4ai.com/complete-sdk-reference/#when-to-use-each-option)\n  * [5. Content Filters](https://docs.crawl4ai.com/complete-sdk-reference/#5-content-filters)\n  * [5.1 BM25ContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#51-bm25contentfilter)\n  * [5.2 PruningContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#52-pruningcontentfilter)\n  * [5.3 LLMContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#53-llmcontentfilter)\n  * [6. Using Fit Markdown](https://docs.crawl4ai.com/complete-sdk-reference/#6-using-fit-markdown)\n  * [7. The MarkdownGenerationResult Object](https://docs.crawl4ai.com/complete-sdk-reference/#7-the-markdowngenerationresult-object)\n  * [8. Combining Filters (BM25 + Pruning) in Two Passes](https://docs.crawl4ai.com/complete-sdk-reference/#8-combining-filters-bm25-pruning-in-two-passes)\n  * [Two-Pass Example](https://docs.crawl4ai.com/complete-sdk-reference/#two-pass-example)\n  * [Whatâ€™s Happening?](https://docs.crawl4ai.com/complete-sdk-reference/#whats-happening)\n  * [Tips & Variations](https://docs.crawl4ai.com/complete-sdk-reference/#tips-variations)\n  * [One-Pass Combination?](https://docs.crawl4ai.com/complete-sdk-reference/#one-pass-combination)\n  * [9. Common Pitfalls & Tips](https://docs.crawl4ai.com/complete-sdk-reference/#9-common-pitfalls-tips)\n  * [10. Summary & Next Steps](https://docs.crawl4ai.com/complete-sdk-reference/#10-summary-next-steps)\n  * [1. How â€œFit Markdownâ€ Works](https://docs.crawl4ai.com/complete-sdk-reference/#1-how-fit-markdown-works)\n  * [1.1 The content_filter](https://docs.crawl4ai.com/complete-sdk-reference/#11-the-content_filter)\n  * [1.2 Common Filters](https://docs.crawl4ai.com/complete-sdk-reference/#12-common-filters)\n  * [2. PruningContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#2-pruningcontentfilter)\n  * [2.1 Usage Example](https://docs.crawl4ai.com/complete-sdk-reference/#21-usage-example)\n  * [2.2 Key Parameters](https://docs.crawl4ai.com/complete-sdk-reference/#22-key-parameters)\n  * [3. BM25ContentFilter](https://docs.crawl4ai.com/complete-sdk-reference/#3-bm25contentfilter)\n  * [3.1 Usage Example](https://docs.crawl4ai.com/complete-sdk-reference/#31-usage-example)\n  * [3.2 Parameters](https://docs.crawl4ai.com/complete-sdk-reference/#32-parameters)\n  * [4. Accessing the â€œFitâ€ Output](https://docs.crawl4ai.com/complete-sdk-reference/#4-accessing-the-fit-output)\n  * [5. Code Patterns Recap](https://docs.crawl4ai.com/complete-sdk-reference/#5-code-patterns-recap)\n  * [5.1 Pruning](https://docs.crawl4ai.com/complete-sdk-reference/#51-pruning)\n  * [5.2 BM25](https://docs.crawl4ai.com/complete-sdk-reference/#52-bm25)\n  * [6. Combining with â€œword_count_thresholdâ€ & Exclusions](https://docs.crawl4ai.com/complete-sdk-reference/#6-combining-with-word_count_threshold-exclusions)\n  * [7. Custom Filters](https://docs.crawl4ai.com/complete-sdk-reference/#7-custom-filters)\n  * [8. Final Thoughts](https://docs.crawl4ai.com/complete-sdk-reference/#8-final-thoughts)\n  * [1. CSS-Based Selection](https://docs.crawl4ai.com/complete-sdk-reference/#1-css-based-selection)\n  * [1.1 Using css_selector](https://docs.crawl4ai.com/complete-sdk-reference/#11-using-css_selector)\n  * [1.2 Using target_elements](https://docs.crawl4ai.com/complete-sdk-reference/#12-using-target_elements)\n  * [2. Content Filtering & Exclusions](https://docs.crawl4ai.com/complete-sdk-reference/#2-content-filtering-exclusions)\n  * [2.1 Basic Overview](https://docs.crawl4ai.com/complete-sdk-reference/#21-basic-overview)\n  * [2.2 Example Usage](https://docs.crawl4ai.com/complete-sdk-reference/#22-example-usage)\n  * [3. Handling Iframes](https://docs.crawl4ai.com/complete-sdk-reference/#3-handling-iframes)\n  * [4. Structured Extraction Examples](https://docs.crawl4ai.com/complete-sdk-reference/#4-structured-extraction-examples)\n  * [4.1 Pattern-Based with JsonCssExtractionStrategy](https://docs.crawl4ai.com/complete-sdk-reference/#41-pattern-based-with-jsoncssextractionstrategy)\n  * [4.2 LLM-Based Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#42-llm-based-extraction)\n  * [5. Comprehensive Example](https://docs.crawl4ai.com/complete-sdk-reference/#5-comprehensive-example)\n  * [6. Scraping Modes](https://docs.crawl4ai.com/complete-sdk-reference/#6-scraping-modes)\n  * [Performance Considerations](https://docs.crawl4ai.com/complete-sdk-reference/#performance-considerations)\n  * [Backward Compatibility](https://docs.crawl4ai.com/complete-sdk-reference/#backward-compatibility)\n  * [7. Combining CSS Selection Methods](https://docs.crawl4ai.com/complete-sdk-reference/#7-combining-css-selection-methods)\n  * [8. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#8-conclusion)\n  * [1. JavaScript Execution](https://docs.crawl4ai.com/complete-sdk-reference/#1-javascript-execution)\n  * [Basic Execution](https://docs.crawl4ai.com/complete-sdk-reference/#basic-execution)\n  * [2. Wait Conditions](https://docs.crawl4ai.com/complete-sdk-reference/#2-wait-conditions)\n  * [2.1 CSS-Based Waiting](https://docs.crawl4ai.com/complete-sdk-reference/#21-css-based-waiting)\n  * [2.2 JavaScript-Based Waiting](https://docs.crawl4ai.com/complete-sdk-reference/#22-javascript-based-waiting)\n  * [3. Handling Dynamic Content](https://docs.crawl4ai.com/complete-sdk-reference/#3-handling-dynamic-content)\n  * [3.1 Load More Example (Hacker News â€œMoreâ€ Link)](https://docs.crawl4ai.com/complete-sdk-reference/#31-load-more-example-hacker-news-more-link)\n  * [3.2 Form Interaction](https://docs.crawl4ai.com/complete-sdk-reference/#32-form-interaction)\n  * [4. Timing Control](https://docs.crawl4ai.com/complete-sdk-reference/#4-timing-control)\n  * [5. Multi-Step Interaction Example](https://docs.crawl4ai.com/complete-sdk-reference/#5-multi-step-interaction-example)\n  * [6. Combine Interaction with Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#6-combine-interaction-with-extraction)\n  * [7. Relevant CrawlerRunConfig Parameters](https://docs.crawl4ai.com/complete-sdk-reference/#7-relevant-crawlerrunconfig-parameters)\n  * [8. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#8-conclusion_1)\n  * [9. Virtual Scrolling](https://docs.crawl4ai.com/complete-sdk-reference/#9-virtual-scrolling)\n  * [Virtual Scroll vs JavaScript Scrolling](https://docs.crawl4ai.com/complete-sdk-reference/#virtual-scroll-vs-javascript-scrolling)\n  * [1. Link Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#1-link-extraction)\n  * [1.1 result.links](https://docs.crawl4ai.com/complete-sdk-reference/#11-resultlinks)\n  * [2. Advanced Link Head Extraction & Scoring](https://docs.crawl4ai.com/complete-sdk-reference/#2-advanced-link-head-extraction-scoring)\n  * [2.1 Why Link Head Extraction?](https://docs.crawl4ai.com/complete-sdk-reference/#21-why-link-head-extraction)\n  * [2.2 Complete Working Example](https://docs.crawl4ai.com/complete-sdk-reference/#22-complete-working-example)\n  * [2.3 Configuration Deep Dive](https://docs.crawl4ai.com/complete-sdk-reference/#23-configuration-deep-dive)\n  * [2.4 Understanding the Three Score Types](https://docs.crawl4ai.com/complete-sdk-reference/#24-understanding-the-three-score-types)\n  * [2.5 Practical Use Cases](https://docs.crawl4ai.com/complete-sdk-reference/#25-practical-use-cases)\n  * [2.6 Performance Tips](https://docs.crawl4ai.com/complete-sdk-reference/#26-performance-tips)\n  * [2.7 Troubleshooting](https://docs.crawl4ai.com/complete-sdk-reference/#27-troubleshooting)\n  * [3. Domain Filtering](https://docs.crawl4ai.com/complete-sdk-reference/#3-domain-filtering)\n  * [3.1 Example: Excluding External & Social Media Links](https://docs.crawl4ai.com/complete-sdk-reference/#31-example-excluding-external-social-media-links)\n  * [3.2 Example: Excluding Specific Domains](https://docs.crawl4ai.com/complete-sdk-reference/#32-example-excluding-specific-domains)\n  * [4. Media Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#4-media-extraction)\n  * [4.1 Accessing result.media](https://docs.crawl4ai.com/complete-sdk-reference/#41-accessing-resultmedia)\n  * [4.2 Excluding External Images](https://docs.crawl4ai.com/complete-sdk-reference/#42-excluding-external-images)\n  * [4.3 Additional Media Config](https://docs.crawl4ai.com/complete-sdk-reference/#43-additional-media-config)\n  * [5. Putting It All Together: Link & Media Filtering](https://docs.crawl4ai.com/complete-sdk-reference/#5-putting-it-all-together-link-media-filtering)\n  * [6. Common Pitfalls & Tips](https://docs.crawl4ai.com/complete-sdk-reference/#6-common-pitfalls-tips)\n  * [1. Intro to Schema-Based Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#1-intro-to-schema-based-extraction)\n  * [2. Simple Example: Crypto Prices](https://docs.crawl4ai.com/complete-sdk-reference/#2-simple-example-crypto-prices)\n  * [XPath Example with raw:// HTML](https://docs.crawl4ai.com/complete-sdk-reference/#xpath-example-with-raw-html)\n  * [3. Advanced Schema & Nested Structures](https://docs.crawl4ai.com/complete-sdk-reference/#3-advanced-schema-nested-structures)\n  * [Sample E-Commerce HTML](https://docs.crawl4ai.com/complete-sdk-reference/#sample-e-commerce-html)\n  * [Running the Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#running-the-extraction)\n  * [4. RegexExtractionStrategy - Fast Pattern-Based Extraction](https://docs.crawl4ai.com/complete-sdk-reference/#4-regexextractionstrategy-fast-pattern-based-extraction)\n  * [Key Features](https://docs.crawl4ai.com/complete-sdk-reference/#key-features)\n  * [Simple Example: Extracting Common Entities](https://docs.crawl4ai.com/complete-sdk-reference/#simple-example-extracting-common-entities)\n  * [Available Built-in Patterns](https://docs.crawl4ai.com/complete-sdk-reference/#available-built-in-patterns)\n  * [Custom Pattern Example](https://docs.crawl4ai.com/complete-sdk-reference/#custom-pattern-example)\n  * [LLM-Assisted Pattern Generation](https://docs.crawl4ai.com/complete-sdk-reference/#llm-assisted-pattern-generation)\n  * [Extraction Results Format](https://docs.crawl4ai.com/complete-sdk-reference/#extraction-results-format)\n  * [5. Why \"No LLM\" Is Often Better](https://docs.crawl4ai.com/complete-sdk-reference/#5-why-no-llm-is-often-better)\n  * [6. Base Element Attributes & Additional Fields](https://docs.crawl4ai.com/complete-sdk-reference/#6-base-element-attributes-additional-fields)\n  * [7. Putting It All Together: Larger Example](https://docs.crawl4ai.com/complete-sdk-reference/#7-putting-it-all-together-larger-example)\n  * [8. Tips & Best Practices](https://docs.crawl4ai.com/complete-sdk-reference/#8-tips-best-practices)\n  * [9. Schema Generation Utility](https://docs.crawl4ai.com/complete-sdk-reference/#9-schema-generation-utility)\n  * [Using the Schema Generator](https://docs.crawl4ai.com/complete-sdk-reference/#using-the-schema-generator)\n  * [LLM Provider Options](https://docs.crawl4ai.com/complete-sdk-reference/#llm-provider-options)\n  * [Benefits of Schema Generation](https://docs.crawl4ai.com/complete-sdk-reference/#benefits-of-schema-generation)\n  * [Best Practices](https://docs.crawl4ai.com/complete-sdk-reference/#best-practices)\n  * [10. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#10-conclusion_1)\n  * [1. Why Use an LLM?](https://docs.crawl4ai.com/complete-sdk-reference/#1-why-use-an-llm)\n  * [2. Provider-Agnostic via LiteLLM](https://docs.crawl4ai.com/complete-sdk-reference/#2-provider-agnostic-via-litellm)\n  * [3. How LLM Extraction Works](https://docs.crawl4ai.com/complete-sdk-reference/#3-how-llm-extraction-works)\n  * [3.1 Flow](https://docs.crawl4ai.com/complete-sdk-reference/#31-flow)\n  * [3.2 extraction_type](https://docs.crawl4ai.com/complete-sdk-reference/#32-extraction_type)\n  * [4. Key Parameters](https://docs.crawl4ai.com/complete-sdk-reference/#4-key-parameters)\n  * [5. Putting It in CrawlerRunConfig](https://docs.crawl4ai.com/complete-sdk-reference/#5-putting-it-in-crawlerrunconfig)\n  * [6. Chunking Details](https://docs.crawl4ai.com/complete-sdk-reference/#6-chunking-details)\n  * [6.1 chunk_token_threshold](https://docs.crawl4ai.com/complete-sdk-reference/#61-chunk_token_threshold)\n  * [6.2 overlap_rate](https://docs.crawl4ai.com/complete-sdk-reference/#62-overlap_rate)\n  * [6.3 Performance & Parallelism](https://docs.crawl4ai.com/complete-sdk-reference/#63-performance-parallelism)\n  * [7. Input Format](https://docs.crawl4ai.com/complete-sdk-reference/#7-input-format)\n  * [8. Token Usage & Show Usage](https://docs.crawl4ai.com/complete-sdk-reference/#8-token-usage-show-usage)\n  * [9. Example: Building a Knowledge Graph](https://docs.crawl4ai.com/complete-sdk-reference/#9-example-building-a-knowledge-graph)\n  * [10. Best Practices & Caveats](https://docs.crawl4ai.com/complete-sdk-reference/#10-best-practices-caveats)\n  * [11. Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#11-conclusion)\n  * [Example 1: Basic Session-Based Crawling](https://docs.crawl4ai.com/complete-sdk-reference/#example-1-basic-session-based-crawling)\n  * [Advanced Technique 1: Custom Execution Hooks](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-technique-1-custom-execution-hooks)\n  * [Advanced Technique 2: Integrated JavaScript Execution and Waiting](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-technique-2-integrated-javascript-execution-and-waiting)\n  * [Example: Using Hooks in AsyncWebCrawler](https://docs.crawl4ai.com/complete-sdk-reference/#example-using-hooks-in-asyncwebcrawler)\n  * [Hook Lifecycle Summary](https://docs.crawl4ai.com/complete-sdk-reference/#hook-lifecycle-summary)\n  * [When to Handle Authentication](https://docs.crawl4ai.com/complete-sdk-reference/#when-to-handle-authentication)\n  * [Additional Considerations](https://docs.crawl4ai.com/complete-sdk-reference/#additional-considerations)\n  * [Conclusion](https://docs.crawl4ai.com/complete-sdk-reference/#conclusion_1)\n  * [Core Imports](https://docs.crawl4ai.com/complete-sdk-reference/#core-imports)\n  * [Basic Pattern](https://docs.crawl4ai.com/complete-sdk-reference/#basic-pattern)\n  * [Advanced Pattern](https://docs.crawl4ai.com/complete-sdk-reference/#advanced-pattern)\n  * [Multi-URL Pattern](https://docs.crawl4ai.com/complete-sdk-reference/#multi-url-pattern)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/adaptive-crawling",
    "depth": 1,
    "title": "Adaptive Crawling - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "a0c62172c710f9a6d773581afced19d1",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/adaptive-crawling/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * Adaptive Crawling\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Adaptive Web Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/#adaptive-web-crawling)\n  * [Introduction](https://docs.crawl4ai.com/core/adaptive-crawling/#introduction)\n  * [Key Concepts](https://docs.crawl4ai.com/core/adaptive-crawling/#key-concepts)\n  * [Quick Start](https://docs.crawl4ai.com/core/adaptive-crawling/#quick-start)\n  * [Crawling Strategies](https://docs.crawl4ai.com/core/adaptive-crawling/#crawling-strategies)\n  * [When to Use Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/#when-to-use-adaptive-crawling)\n  * [Understanding the Output](https://docs.crawl4ai.com/core/adaptive-crawling/#understanding-the-output)\n  * [Persistence and Resumption](https://docs.crawl4ai.com/core/adaptive-crawling/#persistence-and-resumption)\n  * [Best Practices](https://docs.crawl4ai.com/core/adaptive-crawling/#best-practices)\n  * [Examples](https://docs.crawl4ai.com/core/adaptive-crawling/#examples)\n  * [Next Steps](https://docs.crawl4ai.com/core/adaptive-crawling/#next-steps)\n  * [FAQ](https://docs.crawl4ai.com/core/adaptive-crawling/#faq)\n\n\n# Adaptive Web Crawling\n## Introduction\nTraditional web crawlers follow predetermined patterns, crawling pages blindly without knowing when they've gathered enough information. **Adaptive Crawling** changes this paradigm by introducing intelligence into the crawling process.\nThink of it like research: when you're looking for information, you don't read every book in the library. You stop when you've found sufficient information to answer your question. That's exactly what Adaptive Crawling does for web scraping.\n## Key Concepts\n### The Problem It Solves\nWhen crawling websites for specific information, you face two challenges: 1. **Under-crawling** : Stopping too early and missing crucial information 2. **Over-crawling** : Wasting resources by crawling irrelevant pages\nAdaptive Crawling solves both by using a three-layer scoring system that determines when you have \"enough\" information.\n### How It Works\nThe AdaptiveCrawler uses three metrics to measure information sufficiency:\n  * **Coverage** : How well your collected pages cover the query terms\n  * **Consistency** : Whether the information is coherent across pages \n  * **Saturation** : Detecting when new pages aren't adding new information\n\n\nWhen these metrics indicate sufficient information has been gathered, crawling stops automatically.\n## Quick Start\n### Basic Usage\n```\nfrom crawl4ai import AsyncWebCrawler, AdaptiveCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        # Create an adaptive crawler (config is optional)\n        adaptive = AdaptiveCrawler(crawler)\n\n        # Start crawling with a query\n        result = await adaptive.digest(\n            start_url=\"https://docs.python.org/3/\",\n            query=\"async context managers\"\n        )\n\n        # View statistics\n        adaptive.print_stats()\n\n        # Get the most relevant content\n        relevant_pages = adaptive.get_relevant_content(top_k=5)\n        for page in relevant_pages:\n            print(f\"- {page['url']} (score: {page['score']:.2f})\")\nCopy\n```\n\n### Configuration Options\n```\nfrom crawl4ai import AdaptiveConfig\n\nconfig = AdaptiveConfig(\n    confidence_threshold=0.8,    # Stop when 80% confident (default: 0.7)\n    max_pages=30,               # Maximum pages to crawl (default: 20)\n    top_k_links=5,              # Links to follow per page (default: 3)\n    min_gain_threshold=0.05     # Minimum expected gain to continue (default: 0.1)\n)\n\nadaptive = AdaptiveCrawler(crawler, config)\nCopy\n```\n\n## Crawling Strategies\nAdaptive Crawling supports two distinct strategies for determining information sufficiency:\n### Statistical Strategy (Default)\nThe statistical strategy uses pure information theory and term-based analysis:\n  * **Fast and efficient** - No API calls or model loading\n  * **Term-based coverage** - Analyzes query term presence and distribution\n  * **No external dependencies** - Works offline\n  * **Best for** : Well-defined queries with specific terminology\n\n\n```\n# Default configuration uses statistical strategy\nconfig = AdaptiveConfig(\n    strategy=\"statistical\",  # This is the default\n    confidence_threshold=0.8\n)\nCopy\n```\n\n### Embedding Strategy\nThe embedding strategy uses semantic embeddings for deeper understanding:\n  * **Semantic understanding** - Captures meaning beyond exact term matches\n  * **Query expansion** - Automatically generates query variations\n  * **Gap-driven selection** - Identifies semantic gaps in knowledge\n  * **Validation-based stopping** - Uses held-out queries to validate coverage\n  * **Best for** : Complex queries, ambiguous topics, conceptual understanding\n\n\n```\n# Configure embedding strategy\nconfig = AdaptiveConfig(\n    strategy=\"embedding\",\n    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",  # Default\n    n_query_variations=10,  # Generate 10 query variations\n    embedding_min_confidence_threshold=0.1  # Stop if completely irrelevant\n)\n\n# With custom LLM provider for query expansion (recommended)\nfrom crawl4ai import LLMConfig\n\nconfig = AdaptiveConfig(\n    strategy=\"embedding\",\n    embedding_llm_config=LLMConfig(\n        provider='openai/text-embedding-3-small',\n        api_token='your-api-key',\n        temperature=0.7\n    )\n)\n\n# Alternative: Dictionary format (backward compatible)\nconfig = AdaptiveConfig(\n    strategy=\"embedding\",\n    embedding_llm_config={\n        'provider': 'openai/text-embedding-3-small',\n        'api_token': 'your-api-key'\n    }\n)\nCopy\n```\n\n### Strategy Comparison\nFeature | Statistical | Embedding  \n---|---|---  \n**Speed** | Very fast | Moderate (API calls)  \n**Cost** | Free | Depends on provider  \n**Accuracy** | Good for exact terms | Excellent for concepts  \n**Dependencies** | None | Embedding model/API  \n**Query Understanding** | Literal | Semantic  \n**Best Use Case** | Technical docs, specific terms | Research, broad topics  \n### Embedding Strategy Configuration\nThe embedding strategy offers fine-tuned control through several parameters:\n```\nconfig = AdaptiveConfig(\n    strategy=\"embedding\",\n\n    # Model configuration\n    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    embedding_llm_config=None,  # Use for API-based embeddings\n\n    # Query expansion\n    n_query_variations=10,  # Number of query variations to generate\n\n    # Coverage parameters\n    embedding_coverage_radius=0.2,  # Distance threshold for coverage\n    embedding_k_exp=3.0,  # Exponential decay factor (higher = stricter)\n\n    # Stopping criteria\n    embedding_min_relative_improvement=0.1,  # Min improvement to continue\n    embedding_validation_min_score=0.3,  # Min validation score\n    embedding_min_confidence_threshold=0.1,  # Below this = irrelevant\n\n    # Link selection\n    embedding_overlap_threshold=0.85,  # Similarity for deduplication\n\n    # Display confidence mapping\n    embedding_quality_min_confidence=0.7,  # Min displayed confidence\n    embedding_quality_max_confidence=0.95  # Max displayed confidence\n)\nCopy\n```\n\n### Handling Irrelevant Queries\nThe embedding strategy can detect when a query is completely unrelated to the content:\n```\n# This will stop quickly with low confidence\nresult = await adaptive.digest(\n    start_url=\"https://docs.python.org/3/\",\n    query=\"how to cook pasta\"  # Irrelevant to Python docs\n)\n\n# Check if query was irrelevant\nif result.metrics.get('is_irrelevant', False):\n    print(\"Query is unrelated to the content!\")\nCopy\n```\n\n## When to Use Adaptive Crawling\n### Perfect For:\n  * **Research Tasks** : Finding comprehensive information about a topic\n  * **Question Answering** : Gathering sufficient context to answer specific queries\n  * **Knowledge Base Building** : Creating focused datasets for AI/ML applications\n  * **Competitive Intelligence** : Collecting complete information about specific products/features\n\n\n### Not Recommended For:\n  * **Full Site Archiving** : When you need every page regardless of content\n  * **Structured Data Extraction** : When targeting specific, known page patterns\n  * **Real-time Monitoring** : When you need continuous updates\n\n\n## Understanding the Output\n### Confidence Score\nThe confidence score (0-1) indicates how sufficient the gathered information is: - **0.0-0.3** : Insufficient information, needs more crawling - **0.3-0.6** : Partial information, may answer basic queries - **0.6-0.7** : Good coverage, can answer most queries - **0.7-1.0** : Excellent coverage, comprehensive information\n### Statistics Display\n```\nadaptive.print_stats(detailed=False)  # Summary table\nadaptive.print_stats(detailed=True)   # Detailed metrics\nCopy\n```\n\nThe summary shows: - Pages crawled vs. confidence achieved - Coverage, consistency, and saturation scores - Crawling efficiency metrics\n## Persistence and Resumption\n### Saving Progress\n```\nconfig = AdaptiveConfig(\n    save_state=True,\n    state_path=\"my_crawl_state.json\"\n)\n\n# Crawl will auto-save progress\nresult = await adaptive.digest(start_url, query)\nCopy\n```\n\n### Resuming a Crawl\n```\n# Resume from saved state\nresult = await adaptive.digest(\n    start_url,\n    query,\n    resume_from=\"my_crawl_state.json\"\n)\nCopy\n```\n\n### Exporting Knowledge Base\n```\n# Export collected pages to JSONL\nadaptive.export_knowledge_base(\"knowledge_base.jsonl\")\n\n# Import into another session\nnew_adaptive = AdaptiveCrawler(crawler)\nnew_adaptive.import_knowledge_base(\"knowledge_base.jsonl\")\nCopy\n```\n\n## Best Practices\n### 1. Query Formulation\n  * Use specific, descriptive queries\n  * Include key terms you expect to find\n  * Avoid overly broad queries\n\n\n### 2. Threshold Tuning\n  * Start with default (0.7) for general use\n  * Lower to 0.5-0.6 for exploratory crawling\n  * Raise to 0.8+ for exhaustive coverage\n\n\n### 3. Performance Optimization\n  * Use appropriate `max_pages` limits\n  * Adjust `top_k_links` based on site structure\n  * Enable caching for repeat crawls\n\n\n### 4. Link Selection\n  * The crawler prioritizes links based on:\n  * Relevance to query\n  * Expected information gain\n  * URL structure and depth\n\n\n## Examples\n### Research Assistant\n```\n# Gather information about a programming concept\nresult = await adaptive.digest(\n    start_url=\"https://realpython.com\",\n    query=\"python decorators implementation patterns\"\n)\n\n# Get the most relevant excerpts\nfor doc in adaptive.get_relevant_content(top_k=3):\n    print(f\"\\nFrom: {doc['url']}\")\n    print(f\"Relevance: {doc['score']:.2%}\")\n    print(doc['content'][:500] + \"...\")\nCopy\n```\n\n### Knowledge Base Builder\n```\n# Build a focused knowledge base about machine learning\nqueries = [\n    \"supervised learning algorithms\",\n    \"neural network architectures\", \n    \"model evaluation metrics\"\n]\n\nfor query in queries:\n    await adaptive.digest(\n        start_url=\"https://scikit-learn.org/stable/\",\n        query=query\n    )\n\n# Export combined knowledge base\nadaptive.export_knowledge_base(\"ml_knowledge.jsonl\")\nCopy\n```\n\n### API Documentation Crawler\n```\n# Intelligently crawl API documentation\nconfig = AdaptiveConfig(\n    confidence_threshold=0.85,  # Higher threshold for completeness\n    max_pages=30\n)\n\nadaptive = AdaptiveCrawler(crawler, config)\nresult = await adaptive.digest(\n    start_url=\"https://api.example.com/docs\",\n    query=\"authentication endpoints rate limits\"\n)\nCopy\n```\n\n## Next Steps\n  * Learn about [Advanced Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n  * Explore the [AdaptiveCrawler API Reference](https://docs.crawl4ai.com/api/adaptive-crawler/)\n  * See more [Examples](https://github.com/unclecode/crawl4ai/tree/main/docs/examples/adaptive_crawling)\n\n\n## FAQ\n**Q: How is this different from traditional crawling?** A: Traditional crawling follows fixed patterns (BFS/DFS). Adaptive crawling makes intelligent decisions about which links to follow and when to stop based on information gain.\n**Q: Can I use this with JavaScript-heavy sites?** A: Yes! AdaptiveCrawler inherits all capabilities from AsyncWebCrawler, including JavaScript execution.\n**Q: How does it handle large websites?** A: The algorithm naturally limits crawling to relevant sections. Use `max_pages` as a safety limit.\n**Q: Can I customize the scoring algorithms?** A: Advanced users can implement custom strategies. See [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/).\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/adaptive-crawling/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/adaptive-crawling/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/adaptive-crawling/)\n\n\nESC to close\n#### On this page\n  * [Introduction](https://docs.crawl4ai.com/core/adaptive-crawling/#introduction)\n  * [Key Concepts](https://docs.crawl4ai.com/core/adaptive-crawling/#key-concepts)\n  * [The Problem It Solves](https://docs.crawl4ai.com/core/adaptive-crawling/#the-problem-it-solves)\n  * [How It Works](https://docs.crawl4ai.com/core/adaptive-crawling/#how-it-works)\n  * [Quick Start](https://docs.crawl4ai.com/core/adaptive-crawling/#quick-start)\n  * [Basic Usage](https://docs.crawl4ai.com/core/adaptive-crawling/#basic-usage)\n  * [Configuration Options](https://docs.crawl4ai.com/core/adaptive-crawling/#configuration-options)\n  * [Crawling Strategies](https://docs.crawl4ai.com/core/adaptive-crawling/#crawling-strategies)\n  * [Statistical Strategy (Default)](https://docs.crawl4ai.com/core/adaptive-crawling/#statistical-strategy-default)\n  * [Embedding Strategy](https://docs.crawl4ai.com/core/adaptive-crawling/#embedding-strategy)\n  * [Strategy Comparison](https://docs.crawl4ai.com/core/adaptive-crawling/#strategy-comparison)\n  * [Embedding Strategy Configuration](https://docs.crawl4ai.com/core/adaptive-crawling/#embedding-strategy-configuration)\n  * [Handling Irrelevant Queries](https://docs.crawl4ai.com/core/adaptive-crawling/#handling-irrelevant-queries)\n  * [When to Use Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/#when-to-use-adaptive-crawling)\n  * [Perfect For:](https://docs.crawl4ai.com/core/adaptive-crawling/#perfect-for)\n  * [Not Recommended For:](https://docs.crawl4ai.com/core/adaptive-crawling/#not-recommended-for)\n  * [Understanding the Output](https://docs.crawl4ai.com/core/adaptive-crawling/#understanding-the-output)\n  * [Confidence Score](https://docs.crawl4ai.com/core/adaptive-crawling/#confidence-score)\n  * [Statistics Display](https://docs.crawl4ai.com/core/adaptive-crawling/#statistics-display)\n  * [Persistence and Resumption](https://docs.crawl4ai.com/core/adaptive-crawling/#persistence-and-resumption)\n  * [Saving Progress](https://docs.crawl4ai.com/core/adaptive-crawling/#saving-progress)\n  * [Resuming a Crawl](https://docs.crawl4ai.com/core/adaptive-crawling/#resuming-a-crawl)\n  * [Exporting Knowledge Base](https://docs.crawl4ai.com/core/adaptive-crawling/#exporting-knowledge-base)\n  * [Best Practices](https://docs.crawl4ai.com/core/adaptive-crawling/#best-practices)\n  * [1. Query Formulation](https://docs.crawl4ai.com/core/adaptive-crawling/#1-query-formulation)\n  * [2. Threshold Tuning](https://docs.crawl4ai.com/core/adaptive-crawling/#2-threshold-tuning)\n  * [3. Performance Optimization](https://docs.crawl4ai.com/core/adaptive-crawling/#3-performance-optimization)\n  * [4. Link Selection](https://docs.crawl4ai.com/core/adaptive-crawling/#4-link-selection)\n  * [Examples](https://docs.crawl4ai.com/core/adaptive-crawling/#examples)\n  * [Research Assistant](https://docs.crawl4ai.com/core/adaptive-crawling/#research-assistant)\n  * [Knowledge Base Builder](https://docs.crawl4ai.com/core/adaptive-crawling/#knowledge-base-builder)\n  * [API Documentation Crawler](https://docs.crawl4ai.com/core/adaptive-crawling/#api-documentation-crawler)\n  * [Next Steps](https://docs.crawl4ai.com/core/adaptive-crawling/#next-steps)\n  * [FAQ](https://docs.crawl4ai.com/core/adaptive-crawling/#faq)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/ask-ai",
    "depth": 1,
    "title": "Ask AI - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "f60ba574430ca8709922db77fdd3e1ef",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/ask-ai/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * Ask AI\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n### History\nNew Chat\n  * [Chat 1/25/2026, 2:32:53 AM](https://docs.crawl4ai.com/core/ask-ai/ \"Chat 1/25/2026, 2:32:53 AM\")âœ•\n\n\nWe will launch this feature very soon.\nSend\n### Citations\n  * No citations available.\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/cache-modes",
    "depth": 1,
    "title": "Cache Modes - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "058ce801de14d0ce65668349efebb316",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/cache-modes/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * Cache Modes\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Crawl4AI Cache System and Migration Guide](https://docs.crawl4ai.com/core/cache-modes/#crawl4ai-cache-system-and-migration-guide)\n  * [Overview](https://docs.crawl4ai.com/core/cache-modes/#overview)\n  * [Old vs New Approach](https://docs.crawl4ai.com/core/cache-modes/#old-vs-new-approach)\n  * [Migration Example](https://docs.crawl4ai.com/core/cache-modes/#migration-example)\n  * [Common Migration Patterns](https://docs.crawl4ai.com/core/cache-modes/#common-migration-patterns)\n\n\n# Crawl4AI Cache System and Migration Guide\n## Overview\nStarting from version 0.5.0, Crawl4AI introduces a new caching system that replaces the old boolean flags with a more intuitive `CacheMode` enum. This change simplifies cache control and makes the behavior more predictable.\n## Old vs New Approach\n### Old Way (Deprecated)\nThe old system used multiple boolean flags: - `bypass_cache`: Skip cache entirely - `disable_cache`: Disable all caching - `no_cache_read`: Don't read from cache - `no_cache_write`: Don't write to cache\n### New Way (Recommended)\nThe new system uses a single `CacheMode` enum: - `CacheMode.ENABLED`: Normal caching (read/write) - `CacheMode.DISABLED`: No caching at all - `CacheMode.READ_ONLY`: Only read from cache - `CacheMode.WRITE_ONLY`: Only write to cache - `CacheMode.BYPASS`: Skip cache for this operation\n## Migration Example\n### Old Code (Deprecated)\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def use_proxy():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True  # Old way\n        )\n        print(len(result.markdown))\n\nasync def main():\n    await use_proxy()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### New Code (Recommended)\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def use_proxy():\n    # Use CacheMode in CrawlerRunConfig\n    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)  \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            config=config  # Pass the configuration object\n        )\n        print(len(result.markdown))\n\nasync def main():\n    await use_proxy()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Common Migration Patterns\nOld Flag | New Mode  \n---|---  \n`bypass_cache=True` | `cache_mode=CacheMode.BYPASS`  \n`disable_cache=True` | `cache_mode=CacheMode.DISABLED`  \n`no_cache_read=True` | `cache_mode=CacheMode.WRITE_ONLY`  \n`no_cache_write=True` | `cache_mode=CacheMode.READ_ONLY`  \nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/cache-modes/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/cache-modes/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/cache-modes/)\n\n\nESC to close\n#### On this page\n  * [Overview](https://docs.crawl4ai.com/core/cache-modes/#overview)\n  * [Old vs New Approach](https://docs.crawl4ai.com/core/cache-modes/#old-vs-new-approach)\n  * [Old Way (Deprecated)](https://docs.crawl4ai.com/core/cache-modes/#old-way-deprecated)\n  * [New Way (Recommended)](https://docs.crawl4ai.com/core/cache-modes/#new-way-recommended)\n  * [Migration Example](https://docs.crawl4ai.com/core/cache-modes/#migration-example)\n  * [Old Code (Deprecated)](https://docs.crawl4ai.com/core/cache-modes/#old-code-deprecated)\n  * [New Code (Recommended)](https://docs.crawl4ai.com/core/cache-modes/#new-code-recommended)\n  * [Common Migration Patterns](https://docs.crawl4ai.com/core/cache-modes/#common-migration-patterns)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/browser-crawler-config",
    "depth": 1,
    "title": "Browser, Crawler & LLM Config - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "7e105acceab08978a6d30f1a61f0d82b",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/browser-crawler-config/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * Browser, Crawler & LLM Config\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Browser, Crawler & LLM Configuration (Quick Overview)](https://docs.crawl4ai.com/core/browser-crawler-config/#browser-crawler-llm-configuration-quick-overview)\n  * [1. BrowserConfig Essentials](https://docs.crawl4ai.com/core/browser-crawler-config/#1-browserconfig-essentials)\n  * [2. CrawlerRunConfig Essentials](https://docs.crawl4ai.com/core/browser-crawler-config/#2-crawlerrunconfig-essentials)\n  * [3. LLMConfig Essentials](https://docs.crawl4ai.com/core/browser-crawler-config/#3-llmconfig-essentials)\n  * [4. Putting It All Together](https://docs.crawl4ai.com/core/browser-crawler-config/#4-putting-it-all-together)\n  * [5. Next Steps](https://docs.crawl4ai.com/core/browser-crawler-config/#5-next-steps)\n  * [6. Conclusion](https://docs.crawl4ai.com/core/browser-crawler-config/#6-conclusion)\n\n\n# Browser, Crawler & LLM Configuration (Quick Overview)\nCrawl4AI's flexibility stems from two key classes:\n  1. **`BrowserConfig`**â€“ Dictates**how** the browser is launched and behaves (e.g., headless or visible, proxy, user agent). \n  2. **`CrawlerRunConfig`**â€“ Dictates**how** each **crawl** operates (e.g., caching, extraction, timeouts, JavaScript code to run, etc.). \n  3. **`LLMConfig`**- Dictates**how** LLM providers are configured. (model, api token, base url, temperature etc.)\n\n\nIn most examples, you create **one** `BrowserConfig` for the entire crawler session, then pass a **fresh** or re-used `CrawlerRunConfig` whenever you call `arun()`. This tutorial shows the most commonly used parameters. If you need advanced or rarely used fields, see the [Configuration Parameters](https://docs.crawl4ai.com/api/parameters/).\n* * *\n## 1. BrowserConfig Essentials\n```\nclass BrowserConfig:\n    def __init__(\n        browser_type=\"chromium\",\n        headless=True,\n        browser_mode=\"dedicated\",\n        use_managed_browser=False,\n        cdp_url=None,\n        debugging_port=9222,\n        host=\"localhost\",\n        proxy_config=None,\n        viewport_width=1080,\n        viewport_height=600,\n        verbose=True,\n        use_persistent_context=False,\n        user_data_dir=None,\n        cookies=None,\n        headers=None,\n        user_agent=(\n            # \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) AppleWebKit/537.36 \"\n            # \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n            # \"(KHTML, like Gecko) Chrome/116.0.5845.187 Safari/604.1 Edg/117.0.2045.47\"\n            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\"\n        ),\n        user_agent_mode=\"\",\n        text_mode=False,\n        light_mode=False,\n        extra_args=None,\n        enable_stealth=False,\n        # ... other advanced parameters omitted here\n    ):\n        ...\nCopy\n```\n\n### Key Fields to Note\n1.â €**`browser_type`**  \n- Options:`\"chromium\"` , `\"firefox\"`, or `\"webkit\"`.  \n- Defaults to `\"chromium\"`.  \n- If you need a different engine, specify it here.\n2.â €**`headless`**  \n-`True` : Runs the browser in headless mode (invisible browser).  \n- `False`: Runs the browser in visible mode, which helps with debugging.\n3.â €**`browser_mode`**  \n- Determines how the browser should be initialized: -`\"dedicated\"` (default): Creates a new browser instance each time - `\"builtin\"`: Uses the builtin CDP browser running in background - `\"custom\"`: Uses explicit CDP settings provided in `cdp_url` - `\"docker\"`: Runs browser in Docker container with isolation\n4.â €**`use_managed_browser`** & **`cdp_url`**  \n-`use_managed_browser=True` : Launch browser using Chrome DevTools Protocol (CDP) for advanced control - `cdp_url`: URL for CDP endpoint (e.g., `\"ws://localhost:9222/devtools/browser/\"`) - Automatically set based on `browser_mode`\n5.â €**`debugging_port`** & **`host`**  \n-`debugging_port` : Port for browser debugging protocol (default: 9222) - `host`: Host for browser connection (default: \"localhost\")\n6.â €**`proxy_config`**  \n- A`ProxyConfig` object or dictionary with fields like:  \n\n```\n{\n    \"server\": \"http://proxy.example.com:8080\", \n    \"username\": \"...\", \n    \"password\": \"...\"\n}\nCopy\n```\n\n- Leave as `None` if a proxy is not required.\n7.â €**`viewport_width` & `viewport_height`**  \n- The initial window size.  \n- Some sites behave differently with smaller or bigger viewports.\n8.â €**`verbose`**  \n- If`True` , prints extra logs.  \n- Handy for debugging.\n9.â €**`use_persistent_context`**  \n- If`True` , uses a **persistent** browser profile, storing cookies/local storage across runs.  \n- Typically also set `user_data_dir` to point to a folder.\n10.â €**`cookies`** & **`headers`**  \n- If you want to start with specific cookies or add universal HTTP headers to the browser context, set them here.  \n- E.g. `cookies=[{\"name\": \"session\", \"value\": \"abc123\", \"domain\": \"example.com\"}]`.\n11.â €**`user_agent`** & **`user_agent_mode`**  \n-`user_agent` : Custom User-Agent string. If `None`, a default is used.  \n- `user_agent_mode`: Set to `\"random\"` for randomization (helps fight bot detection).\n12.â €**`text_mode`** & **`light_mode`**  \n-`text_mode=True` disables images, possibly speeding up text-only crawls.  \n- `light_mode=True` turns off certain background features for performance. \n13.â €**`extra_args`**  \n- Additional flags for the underlying browser.  \n- E.g. `[\"--disable-extensions\"]`.\n14.â €**`enable_stealth`**  \n- If`True` , enables stealth mode using playwright-stealth.  \n- Modifies browser fingerprints to avoid basic bot detection.  \n- Default is `False`. Recommended for sites with bot protection.\n### Helper Methods\nBoth configuration classes provide a `clone()` method to create modified copies:\n```\n# Create a base browser config\nbase_browser = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    text_mode=True\n)\n\n# Create a visible browser config for debugging\ndebug_browser = base_browser.clone(\n    headless=False,\n    verbose=True\n)\nCopy\n```\n\n**Minimal Example** :\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_conf = BrowserConfig(\n    browser_type=\"firefox\",\n    headless=False,\n    text_mode=True\n)\n\nasync with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300])\nCopy\n```\n\n* * *\n## 2. CrawlerRunConfig Essentials\n```\nclass CrawlerRunConfig:\n    def __init__(\n        word_count_threshold=200,\n        extraction_strategy=None,\n        chunking_strategy=RegexChunking(),\n        markdown_generator=None,\n        cache_mode=CacheMode.BYPASS,\n        js_code=None,\n        c4a_script=None,\n        wait_for=None,\n        screenshot=False,\n        pdf=False,\n        capture_mhtml=False,\n        # Location and Identity Parameters\n        locale=None,            # e.g. \"en-US\", \"fr-FR\"\n        timezone_id=None,       # e.g. \"America/New_York\"\n        geolocation=None,       # GeolocationConfig object\n        # Proxy Configuration\n        proxy_config=None,\n        proxy_rotation_strategy=None,\n        # Page Interaction Parameters\n        scan_full_page=False,\n        scroll_delay=0.2,\n        wait_until=\"domcontentloaded\",\n        page_timeout=60000,\n        delay_before_return_html=0.1,\n        # URL Matching Parameters\n        url_matcher=None,       # For URL-specific configurations\n        match_mode=MatchMode.OR,\n        verbose=True,\n        stream=False,  # Enable streaming for arun_many()\n        # ... other advanced parameters omitted\n    ):\n        ...\nCopy\n```\n\n### Key Fields to Note\n1.â €**`word_count_threshold`**:  \n- The minimum word count before a block is considered.  \n- If your site has lots of short paragraphs or items, you can lower it.\n2.â €**`extraction_strategy`**:  \n- Where you plug in JSON-based extraction (CSS, LLM, etc.).  \n- If `None`, no structured extraction is done (only raw/cleaned HTML + markdown).\n3.â €**`chunking_strategy`**:  \n- Strategy to chunk content before extraction.  \n- Defaults to `RegexChunking()`. Can be customized for different chunking approaches.\n4.â €**`markdown_generator`**:  \n- E.g., `DefaultMarkdownGenerator(...)`, controlling how HTMLâ†’Markdown conversion is done.  \n- If `None`, a default approach is used.\n5.â €**`cache_mode`**:  \n- Controls caching behavior (`ENABLED`, `BYPASS`, `DISABLED`, etc.).  \n- Defaults to `CacheMode.BYPASS`.\n6.â €**`js_code`** & **`c4a_script`**:  \n- `js_code`: A string or list of JavaScript strings to execute.  \n- `c4a_script`: C4A script that compiles to JavaScript. - Great for \"Load More\" buttons or user interactions. \n7.â €**`wait_for`**:  \n- A CSS or JS expression to wait for before extracting content.  \n- Common usage: `wait_for=\"css:.main-loaded\"` or `wait_for=\"js:() => window.loaded === true\"`.\n8.â €**`screenshot`**,**`pdf`**, & **`capture_mhtml`**:  \n- If `True`, captures a screenshot, PDF, or MHTML snapshot after the page is fully loaded.  \n- The results go to `result.screenshot` (base64), `result.pdf` (bytes), or `result.mhtml` (string).\n9.â €**Location Parameters** :  \n- **`locale`**: Browser's locale (e.g.,`\"en-US\"` , `\"fr-FR\"`) for language preferences - **`timezone_id`**: Browser's timezone (e.g.,`\"America/New_York\"` , `\"Europe/Paris\"`) - **`geolocation`**: GPS coordinates via`GeolocationConfig(latitude=48.8566, longitude=2.3522)` - See [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/#7-locale-timezone-and-geolocation-control)\n10.â €**Proxy Configuration** :  \n- **`proxy_config`**: Proxy server configuration (ProxyConfig object or dict) e.g. {\"server\": \"...\", \"username\": \"...\", \"password\"} -**`proxy_rotation_strategy`**: Strategy for rotating proxies during crawls\n11.â €**Page Interaction Parameters** :  \n- **`scan_full_page`**: If`True` , scroll through the entire page to load all content - **`wait_until`**: Condition to wait for when navigating (e.g., \"domcontentloaded\", \"networkidle\") -**`page_timeout`**: Timeout in milliseconds for page operations (default: 60000) -**`delay_before_return_html`**: Delay in seconds before retrieving final HTML.\n12.â €**`url_matcher`** & **`match_mode`**:  \n- Enable URL-specific configurations when used with `arun_many()`. - Set `url_matcher` to patterns (glob, function, or list) to match specific URLs. - Use `match_mode` (OR/AND) to control how multiple patterns combine. - See [URL-Specific Configurations](https://docs.crawl4ai.com/api/arun_many/#url-specific-configurations) for examples.\n13.â €**`verbose`**:  \n- Logs additional runtime details.  \n- Overlaps with the browser's verbosity if also set to `True` in `BrowserConfig`.\n14.â €**`stream`**:  \n- If `True`, enables streaming mode for `arun_many()` to process URLs as they complete. - Allows handling results incrementally instead of waiting for all URLs to finish.\n### Helper Methods\nThe `clone()` method is particularly useful for creating variations of your crawler configuration:\n```\n# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200,\n    wait_until=\"networkidle\"\n)\n\n# Create variations for different use cases\nstream_config = base_config.clone(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\ndebug_config = base_config.clone(\n    page_timeout=120000,  # Longer timeout for debugging\n    verbose=True\n)\nCopy\n```\n\nThe `clone()` method: - Creates a new instance with all the same settings - Updates only the specified parameters - Leaves the original configuration unchanged - Perfect for creating variations without repeating all parameters\n* * *\n## 3. LLMConfig Essentials\n### Key fields to note\n1.â €**`provider`**:  \n- Which LLM provider to use. - Possible values are `\"ollama/llama3\",\"groq/llama3-70b-8192\",\"groq/llama3-8b-8192\", \"openai/gpt-4o-mini\" ,\"openai/gpt-4o\",\"openai/o1-mini\",\"openai/o1-preview\",\"openai/o3-mini\",\"openai/o3-mini-high\",\"anthropic/claude-3-haiku-20240307\",\"anthropic/claude-3-opus-20240229\",\"anthropic/claude-3-sonnet-20240229\",\"anthropic/claude-3-5-sonnet-20240620\",\"gemini/gemini-pro\",\"gemini/gemini-1.5-pro\",\"gemini/gemini-2.0-flash\",\"gemini/gemini-2.0-flash-exp\",\"gemini/gemini-2.0-flash-lite-preview-02-05\",\"deepseek/deepseek-chat\"`  \n_(default:`\"openai/gpt-4o-mini\"`)_\n2.â €**`api_token`**:  \n- Optional. When not provided explicitly, api_token will be read from environment variables based on provider. For example: If a gemini model is passed as provider then,`\"GEMINI_API_KEY\"` will be read from environment variables  \n- API token of LLM provider   \neg: `api_token = \"gsk_1ClHGGJ7Lpn4WGybR7vNWGdyb3FY7zXEw3SCiy0BAVM9lL8CQv\"` - Environment variable - use with prefix \"env:\"   \neg:`api_token = \"env: GROQ_API_KEY\"`\n3.â €**`base_url`**:  \n- If your provider has a custom endpoint\n4.â €**Retry/backoff controls** _(optional)_ :  \n- `backoff_base_delay` _(default`2` seconds)_ â€“ base delay inserted before the first retry when the provider returns a rate-limit response.  \n- `backoff_max_attempts` _(default`3`)_ â€“ total number of attempts (initial call plus retries) before the request is surfaced as an error.  \n- `backoff_exponential_factor` _(default`2`)_ â€“ growth rate for the retry delay (`delay = base_delay * factor^attempt`).  \n- These values are forwarded to the shared `perform_completion_with_backoff` helper, ensuring every strategy that consumes your `LLMConfig` honors the same throttling policy.\n```\nllm_config = LLMConfig(\n    provider=\"openai/gpt-4o-mini\",\n    api_token=os.getenv(\"OPENAI_API_KEY\"),\n    backoff_base_delay=1, # optional\n    backoff_max_attempts=5, # optional\n    backoff_exponential_factor=3, #optional\n)\nCopy\n```\n\n## 4. Putting It All Together\nIn a typical scenario, you define **one** `BrowserConfig` for your crawler session, then create **one or more** `CrawlerRunConfig` & `LLMConfig` depending on each call's needs:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig, LLMContentFilter, DefaultMarkdownGenerator\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    # 1) Browser config: headless, bigger viewport, no proxy\n    browser_conf = BrowserConfig(\n        headless=True,\n        viewport_width=1280,\n        viewport_height=720\n    )\n\n    # 2) Example extraction strategy\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"div.article\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    extraction = JsonCssExtractionStrategy(schema)\n\n    # 3) Example LLM content filtering\n\n    gemini_config = LLMConfig(\n        provider=\"gemini/gemini-1.5-pro\", \n        api_token = \"env:GEMINI_API_TOKEN\"\n    )\n\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config=gemini_config,  # or your preferred provider\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=500,  # Adjust based on your needs\n        verbose=True\n    )\n\n    md_generator = DefaultMarkdownGenerator(\n        content_filter=filter,\n        options={\"ignore_links\": True}\n    )\n\n    # 4) Crawler run config: skip cache, use extraction\n    run_conf = CrawlerRunConfig(\n        markdown_generator=md_generator,\n        extraction_strategy=extraction,\n        cache_mode=CacheMode.BYPASS,\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        # 4) Execute the crawl\n        result = await crawler.arun(url=\"https://example.com/news\", config=run_conf)\n\n        if result.success:\n            print(\"Extracted content:\", result.extracted_content)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## 5. Next Steps\nFor a **detailed list** of available parameters (including advanced ones), see:\n  * [BrowserConfig, CrawlerRunConfig & LLMConfig Reference](https://docs.crawl4ai.com/api/parameters/)\n\n\nYou can explore topics like:\n  * **Custom Hooks & Auth** (Inject JavaScript or handle login forms). \n  * **Session Management** (Re-use pages, preserve state across multiple calls). \n  * **Magic Mode** or **Identity-based Crawling** (Fight bot detection by simulating user behavior). \n  * **Advanced Caching** (Fine-tune read/write cache modes). \n\n\n* * *\n## 6. Conclusion\n**BrowserConfig** , **CrawlerRunConfig** and **LLMConfig** give you straightforward ways to define:\n  * **Which** browser to launch, how it should run, and any proxy or user agent needs. \n  * **How** each crawl should behaveâ€”caching, timeouts, JavaScript code, extraction strategies, etc.\n  * **Which** LLM provider to use, api token, temperature and base url for custom endpoints\n\n\nUse them together for **clear, maintainable** code, and when you need more specialized behavior, check out the advanced parameters in the [reference docs](https://docs.crawl4ai.com/api/parameters/). Happy crawling!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/browser-crawler-config/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/browser-crawler-config/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/browser-crawler-config/)\n\n\nESC to close\n#### On this page\n  * [1. BrowserConfig Essentials](https://docs.crawl4ai.com/core/browser-crawler-config/#1-browserconfig-essentials)\n  * [Key Fields to Note](https://docs.crawl4ai.com/core/browser-crawler-config/#key-fields-to-note)\n  * [Helper Methods](https://docs.crawl4ai.com/core/browser-crawler-config/#helper-methods)\n  * [2. CrawlerRunConfig Essentials](https://docs.crawl4ai.com/core/browser-crawler-config/#2-crawlerrunconfig-essentials)\n  * [Key Fields to Note](https://docs.crawl4ai.com/core/browser-crawler-config/#key-fields-to-note_1)\n  * [Helper Methods](https://docs.crawl4ai.com/core/browser-crawler-config/#helper-methods_1)\n  * [3. LLMConfig Essentials](https://docs.crawl4ai.com/core/browser-crawler-config/#3-llmconfig-essentials)\n  * [Key fields to note](https://docs.crawl4ai.com/core/browser-crawler-config/#key-fields-to-note_2)\n  * [4. Putting It All Together](https://docs.crawl4ai.com/core/browser-crawler-config/#4-putting-it-all-together)\n  * [5. Next Steps](https://docs.crawl4ai.com/core/browser-crawler-config/#5-next-steps)\n  * [6. Conclusion](https://docs.crawl4ai.com/core/browser-crawler-config/#6-conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/cli",
    "depth": 1,
    "title": "Command Line Interface - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "8292b213559fae0d737cab93e878a8da",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/cli/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * Command Line Interface\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Crawl4AI CLI Guide](https://docs.crawl4ai.com/core/cli/#crawl4ai-cli-guide)\n  * [Table of Contents](https://docs.crawl4ai.com/core/cli/#table-of-contents)\n  * [Installation](https://docs.crawl4ai.com/core/cli/#installation)\n  * [Basic Usage](https://docs.crawl4ai.com/core/cli/#basic-usage)\n  * [Quick Example of Advanced Usage](https://docs.crawl4ai.com/core/cli/#quick-example-of-advanced-usage)\n  * [Configuration](https://docs.crawl4ai.com/core/cli/#configuration)\n  * [Advanced Features](https://docs.crawl4ai.com/core/cli/#advanced-features)\n  * [Output Formats](https://docs.crawl4ai.com/core/cli/#output-formats)\n  * [Complete Examples](https://docs.crawl4ai.com/core/cli/#complete-examples)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/cli/#best-practices-tips)\n  * [Recap](https://docs.crawl4ai.com/core/cli/#recap)\n\n\n# Crawl4AI CLI Guide\n## Table of Contents\n  * [Installation](https://docs.crawl4ai.com/core/cli/#installation)\n  * [Basic Usage](https://docs.crawl4ai.com/core/cli/#basic-usage)\n  * [Configuration](https://docs.crawl4ai.com/core/cli/#configuration)\n  * [Browser Configuration](https://docs.crawl4ai.com/core/cli/#browser-configuration)\n  * [Crawler Configuration](https://docs.crawl4ai.com/core/cli/#crawler-configuration)\n  * [Extraction Configuration](https://docs.crawl4ai.com/core/cli/#extraction-configuration)\n  * [Content Filtering](https://docs.crawl4ai.com/core/cli/#content-filtering)\n  * [Advanced Features](https://docs.crawl4ai.com/core/cli/#advanced-features)\n  * [LLM Q&A](https://docs.crawl4ai.com/core/cli/#llm-qa)\n  * [Structured Data Extraction](https://docs.crawl4ai.com/core/cli/#structured-data-extraction)\n  * [Content Filtering](https://docs.crawl4ai.com/core/cli/#content-filtering-1)\n  * [Output Formats](https://docs.crawl4ai.com/core/cli/#output-formats)\n  * [Examples](https://docs.crawl4ai.com/core/cli/#examples)\n  * [Configuration Reference](https://docs.crawl4ai.com/core/cli/#configuration-reference)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/cli/#best-practices--tips)\n\n\n## Installation\nThe Crawl4AI CLI will be installed automatically when you install the library.\n## Basic Usage\nThe Crawl4AI CLI (`crwl`) provides a simple interface to the Crawl4AI library:\n```\n# Basic crawling\ncrwl https://example.com\n\n# Get markdown output\ncrwl https://example.com -o markdown\n\n# Verbose JSON output with cache bypass\ncrwl https://example.com -o json -v --bypass-cache\n\n# See usage examples\ncrwl --example\nCopy\n```\n\n## Quick Example of Advanced Usage\nIf you clone the repository and run the following command, you will receive the content of the page in JSON format according to a JSON-CSS schema:\n```\ncrwl \"https://www.infoq.com/ai-ml-data-eng/\" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;\nCopy\n```\n\n## Configuration\n### Browser Configuration\nBrowser settings can be configured via YAML file or command line parameters:\n```\n# browser.yml\nheadless: true\nviewport_width: 1280\nuser_agent_mode: \"random\"\nverbose: true\nignore_https_errors: true\nCopy\n```\n\n```\n# Using config file\ncrwl https://example.com -B browser.yml\n\n# Using direct parameters\ncrwl https://example.com -b \"headless=true,viewport_width=1280,user_agent_mode=random\"\nCopy\n```\n\n### Crawler Configuration\nControl crawling behavior:\n```\n# crawler.yml\ncache_mode: \"bypass\"\nwait_until: \"networkidle\"\npage_timeout: 30000\ndelay_before_return_html: 0.5\nword_count_threshold: 100\nscan_full_page: true\nscroll_delay: 0.3\nprocess_iframes: false\nremove_overlay_elements: true\nmagic: true\nverbose: true\nCopy\n```\n\n```\n# Using config file\ncrwl https://example.com -C crawler.yml\n\n# Using direct parameters\ncrwl https://example.com -c \"css_selector=#main,delay_before_return_html=2,scan_full_page=true\"\nCopy\n```\n\n### Extraction Configuration\nTwo types of extraction are supported:\n  1. CSS/XPath-based extraction: \n```\n# extract_css.yml\ntype: \"json-css\"\nparams:\n  verbose: true\nCopy\n```\n\n\n\n```\n// css_schema.json\n{\n  \"name\": \"ArticleExtractor\",\n  \"baseSelector\": \".article\",\n  \"fields\": [\n    {\n      \"name\": \"title\",\n      \"selector\": \"h1.title\",\n      \"type\": \"text\"\n    },\n    {\n      \"name\": \"link\",\n      \"selector\": \"a.read-more\",\n      \"type\": \"attribute\",\n      \"attribute\": \"href\"\n    }\n  ]\n}\nCopy\n```\n\n  1. LLM-based extraction: \n```\n# extract_llm.yml\ntype: \"llm\"\nprovider: \"openai/gpt-4\"\ninstruction: \"Extract all articles with their titles and links\"\napi_token: \"your-token\"\nparams:\n  temperature: 0.3\n  max_tokens: 1000\nCopy\n```\n\n\n\n```\n// llm_schema.json\n{\n  \"title\": \"Article\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"title\": {\n      \"type\": \"string\",\n      \"description\": \"The title of the article\"\n    },\n    \"link\": {\n      \"type\": \"string\",\n      \"description\": \"URL to the full article\"\n    }\n  }\n}\nCopy\n```\n\n## Advanced Features\n### LLM Q&A\nAsk questions about crawled content:\n```\n# Simple question\ncrwl https://example.com -q \"What is the main topic discussed?\"\n\n# View content then ask questions\ncrwl https://example.com -o markdown  # See content first\ncrwl https://example.com -q \"Summarize the key points\"\ncrwl https://example.com -q \"What are the conclusions?\"\n\n# Combined with advanced crawling\ncrwl https://example.com \\\n    -B browser.yml \\\n    -c \"css_selector=article,scan_full_page=true\" \\\n    -q \"What are the pros and cons mentioned?\"\nCopy\n```\n\nFirst-time setup: - Prompts for LLM provider and API token - Saves configuration in `~/.crawl4ai/global.yml` - Supports various providers (openai/gpt-4, anthropic/claude-3-sonnet, etc.) - For case of `ollama` you do not need to provide API token. - See [LiteLLM Providers](https://docs.litellm.ai/docs/providers) for full list\n### Structured Data Extraction\nExtract structured data using CSS selectors:\n```\ncrwl https://example.com \\\n    -e extract_css.yml \\\n    -s css_schema.json \\\n    -o json\nCopy\n```\n\nOr using LLM-based extraction:\n```\ncrwl https://example.com \\\n    -e extract_llm.yml \\\n    -s llm_schema.json \\\n    -o json\nCopy\n```\n\n### Content Filtering\nFilter content for relevance:\n```\n# filter_bm25.yml\ntype: \"bm25\"\nquery: \"target content\"\nthreshold: 1.0\n\n# filter_pruning.yml\ntype: \"pruning\"\nquery: \"focus topic\"\nthreshold: 0.48\nCopy\n```\n\n```\ncrwl https://example.com -f filter_bm25.yml -o markdown-fit\nCopy\n```\n\n## Output Formats\n  * `all` - Full crawl result including metadata\n  * `json` - Extracted structured data (when using extraction)\n  * `markdown` / `md` - Raw markdown output\n  * `markdown-fit` / `md-fit` - Filtered markdown for better readability\n\n\n## Complete Examples\n  1. Basic Extraction: \n```\ncrwl https://example.com \\\n    -B browser.yml \\\n    -C crawler.yml \\\n    -o json\nCopy\n```\n\n  2. Structured Data Extraction: \n```\ncrwl https://example.com \\\n    -e extract_css.yml \\\n    -s css_schema.json \\\n    -o json \\\n    -v\nCopy\n```\n\n  3. LLM Extraction with Filtering: \n```\ncrwl https://example.com \\\n    -B browser.yml \\\n    -e extract_llm.yml \\\n    -s llm_schema.json \\\n    -f filter_bm25.yml \\\n    -o json\nCopy\n```\n\n  4. Interactive Q&A: \n```\n# First crawl and view\ncrwl https://example.com -o markdown\n\n# Then ask questions\ncrwl https://example.com -q \"What are the main points?\"\ncrwl https://example.com -q \"Summarize the conclusions\"\nCopy\n```\n\n\n\n## Best Practices & Tips\n  1. **Configuration Management** :\n  2. Keep common configurations in YAML files\n  3. Use CLI parameters for quick overrides\n  4. Store sensitive data (API tokens) in `~/.crawl4ai/global.yml`\n  5. **Performance Optimization** :\n  6. Use `--bypass-cache` for fresh content\n  7. Enable `scan_full_page` for infinite scroll pages\n  8. Adjust `delay_before_return_html` for dynamic content\n  9. **Content Extraction** :\n  10. Use CSS extraction for structured content\n  11. Use LLM extraction for unstructured content\n  12. Combine with filters for focused results\n  13. **Q &A Workflow**:\n  14. View content first with `-o markdown`\n  15. Ask specific questions\n  16. Use broader context with appropriate selectors\n\n\n## Recap\nThe Crawl4AI CLI provides: - Flexible configuration via files and parameters - Multiple extraction strategies (CSS, XPath, LLM) - Content filtering and optimization - Interactive Q&A capabilities - Various output formats\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/cli/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/cli/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/cli/)\n\n\nESC to close\n#### On this page\n  * [Table of Contents](https://docs.crawl4ai.com/core/cli/#table-of-contents)\n  * [Installation](https://docs.crawl4ai.com/core/cli/#installation)\n  * [Basic Usage](https://docs.crawl4ai.com/core/cli/#basic-usage)\n  * [Quick Example of Advanced Usage](https://docs.crawl4ai.com/core/cli/#quick-example-of-advanced-usage)\n  * [Configuration](https://docs.crawl4ai.com/core/cli/#configuration)\n  * [Browser Configuration](https://docs.crawl4ai.com/core/cli/#browser-configuration)\n  * [Crawler Configuration](https://docs.crawl4ai.com/core/cli/#crawler-configuration)\n  * [Extraction Configuration](https://docs.crawl4ai.com/core/cli/#extraction-configuration)\n  * [Advanced Features](https://docs.crawl4ai.com/core/cli/#advanced-features)\n  * [LLM Q&A](https://docs.crawl4ai.com/core/cli/#llm-qa)\n  * [Structured Data Extraction](https://docs.crawl4ai.com/core/cli/#structured-data-extraction)\n  * [Content Filtering](https://docs.crawl4ai.com/core/cli/#content-filtering)\n  * [Output Formats](https://docs.crawl4ai.com/core/cli/#output-formats)\n  * [Complete Examples](https://docs.crawl4ai.com/core/cli/#complete-examples)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/cli/#best-practices-tips)\n  * [Recap](https://docs.crawl4ai.com/core/cli/#recap)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/c4a-script",
    "depth": 1,
    "title": "C4A-Script - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "3bf4b14b3d3cf4285c318bb4f5de9164",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/c4a-script/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * C4A-Script\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [C4A-Script: Visual Web Automation Made Simple](https://docs.crawl4ai.com/core/c4a-script/#c4a-script-visual-web-automation-made-simple)\n  * [What is C4A-Script?](https://docs.crawl4ai.com/core/c4a-script/#what-is-c4a-script)\n  * [Getting Started: Your First Script](https://docs.crawl4ai.com/core/c4a-script/#getting-started-your-first-script)\n  * [Interactive Tutorial & Live Demo](https://docs.crawl4ai.com/core/c4a-script/#interactive-tutorial-live-demo)\n  * [Core Concepts](https://docs.crawl4ai.com/core/c4a-script/#core-concepts)\n  * [Command Categories](https://docs.crawl4ai.com/core/c4a-script/#command-categories)\n  * [Real-World Examples](https://docs.crawl4ai.com/core/c4a-script/#real-world-examples)\n  * [Visual Programming with Blockly](https://docs.crawl4ai.com/core/c4a-script/#visual-programming-with-blockly)\n  * [Advanced Features](https://docs.crawl4ai.com/core/c4a-script/#advanced-features)\n  * [Best Practices](https://docs.crawl4ai.com/core/c4a-script/#best-practices)\n  * [Getting Help](https://docs.crawl4ai.com/core/c4a-script/#getting-help)\n  * [What's Next?](https://docs.crawl4ai.com/core/c4a-script/#whats-next)\n\n\n# C4A-Script: Visual Web Automation Made Simple\n## What is C4A-Script?\nC4A-Script is a powerful, human-readable domain-specific language (DSL) designed for web automation and interaction. Think of it as a simplified programming language that anyone can read and write, perfect for automating repetitive web tasks, testing user interfaces, or creating interactive demos.\n### Why C4A-Script?\n**Simple Syntax, Powerful Results**\n```\n# Navigate and interact in plain English\nGO https://example.com\nWAIT `#search-box` 5\nTYPE \"Hello World\"\nCLICK `button[type=\"submit\"]`\nCopy\n```\n\n**Visual Programming Support** C4A-Script comes with a built-in Blockly visual editor, allowing you to create scripts by dragging and dropping blocks - no coding experience required!\n**Perfect for:** - **UI Testing** : Automate user interaction flows - **Demo Creation** : Build interactive product demonstrations  \n- **Data Entry** : Automate form filling and submissions - **Testing Workflows** : Validate complex user journeys - **Training** : Teach web automation without code complexity\n## Getting Started: Your First Script\nLet's create a simple script that searches for something on a website:\n```\n# My first C4A-Script\nGO https://duckduckgo.com\n\n# Wait for the search box to appear\nWAIT `input[name=\"q\"]` 10\n\n# Type our search query\nTYPE \"Crawl4AI\"\n\n# Press Enter to search\nPRESS Enter\n\n# Wait for results\nWAIT `.results` 5\nCopy\n```\n\nThat's it! In just a few lines, you've automated a complete search workflow.\n## Interactive Tutorial & Live Demo\nWant to learn by doing? We've got you covered:\n**ðŸš€[Live Demo](https://docs.crawl4ai.com/apps/c4a-script/)** - Try C4A-Script in your browser right now!\n**ðŸ“[Tutorial Examples](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/c4a_script/)** - Complete examples with source code\n### Running the Tutorial Locally\nThe tutorial includes a Flask-based web interface with: - **Live Code Editor** with syntax highlighting - **Visual Blockly Editor** for drag-and-drop programming - **Recording Mode** to capture your actions and generate scripts - **Timeline View** to see and edit your automation steps\n```\n# Clone and navigate to the tutorial\ncd docs/examples/c4a_script/tutorial/\n\n# Install dependencies\npip install -r requirements.txt\n\n# Launch the tutorial server\npython server.py\n\n# Open http://localhost:8000 in your browser\nCopy\n```\n\n## Core Concepts\n### Commands and Syntax\nC4A-Script uses simple, English-like commands. Each command does one specific thing:\n```\n# Comments start with #\nCOMMAND parameter1 parameter2\n\n# Most commands use CSS selectors in backticks\nCLICK `#submit-button`\n\n# Text content goes in quotes\nTYPE \"Hello, World!\"\n\n# Numbers are used directly\nWAIT 3\nCopy\n```\n\n### Selectors: Finding Elements\nC4A-Script uses CSS selectors to identify elements on the page:\n```\n# By ID\nCLICK `#login-button`\n\n# By class\nCLICK `.submit-btn`\n\n# By attribute\nCLICK `button[type=\"submit\"]`\n\n# By accessible attributes\nCLICK `button[aria-label=\"Search\"][title=\"Search\"]`\n\n# Complex selectors\nCLICK `.form-container input[name=\"email\"]`\nCopy\n```\n\n### Variables and Dynamic Content\nStore and reuse values with variables:\n```\n# Set a variable\nSETVAR username = \"john@example.com\"\nSETVAR password = \"secret123\"\n\n# Use variables (prefix with $)\nTYPE $username\nPRESS Tab\nTYPE $password\nCopy\n```\n\n## Command Categories\n### ðŸ§­ Navigation Commands\nMove around the web like a user would:\nCommand | Purpose | Example  \n---|---|---  \n`GO` | Navigate to URL | `GO https://example.com`  \n`RELOAD` | Refresh current page | `RELOAD`  \n`BACK` | Go back in history | `BACK`  \n`FORWARD` | Go forward in history | `FORWARD`  \n### â±ï¸ Wait Commands\nEnsure elements are ready before interacting:\nCommand | Purpose | Example  \n---|---|---  \n`WAIT` | Wait for time/element/text |  `WAIT 3` or `WAIT \\`#element` 10`  \n### ðŸ–±ï¸ Mouse Commands\nClick, drag, and move like a human:\nCommand | Purpose | Example  \n---|---|---  \n`CLICK` | Click element or coordinates |  `CLICK \\`button``or`CLICK 100 200`  \n`DOUBLE_CLICK` | Double-click element |  `DOUBLE_CLICK \\`.item``  \n`RIGHT_CLICK` | Right-click element |  `RIGHT_CLICK \\`#menu``  \n`SCROLL` | Scroll in direction | `SCROLL DOWN 500`  \n`DRAG` | Drag from point to point | `DRAG 100 100 500 300`  \n### âŒ¨ï¸ Keyboard Commands\nType text and press keys naturally:\nCommand | Purpose | Example  \n---|---|---  \n`TYPE` | Type text or variable |  `TYPE \"Hello\"` or `TYPE $username`  \n`PRESS` | Press special keys |  `PRESS Tab` or `PRESS Enter`  \n`CLEAR` | Clear input field |  `CLEAR \\`#search``  \n`SET` | Set input value directly |  `SET \\`#email` \"user@example.com\"`  \n### ðŸ”€ Control Flow\nAdd logic and repetition to your scripts:\nCommand | Purpose | Example  \n---|---|---  \n`IF` | Conditional execution |  `IF (EXISTS \\`#popup`) THEN CLICK `#close``  \n`REPEAT` | Loop commands | `REPEAT (SCROLL DOWN 300, 5)`  \n### ðŸ’¾ Variables & Advanced\nStore data and execute custom code:\nCommand | Purpose | Example  \n---|---|---  \n`SETVAR` | Create variable | `SETVAR email = \"test@example.com\"`  \n`EVAL` | Execute JavaScript |  `EVAL \\`console.log('Hello')``  \n## Real-World Examples\n### Example 1: Login Flow\n```\n# Complete login automation\nGO https://myapp.com/login\n\n# Wait for page to load\nWAIT `#login-form` 5\n\n# Fill credentials\nCLICK `#email`\nTYPE \"user@example.com\"\nPRESS Tab\nTYPE \"mypassword\"\n\n# Submit form\nCLICK `button[type=\"submit\"]`\n\n# Wait for dashboard\nWAIT `.dashboard` 10\nCopy\n```\n\n### Example 2: E-commerce Shopping\n```\n# Shopping automation with variables\nSETVAR product = \"laptop\"\nSETVAR budget = \"1000\"\n\nGO https://shop.example.com\nWAIT `#search-box` 3\n\n# Search for product\nTYPE $product\nPRESS Enter\nWAIT `.product-list` 5\n\n# Filter by price\nCLICK `.price-filter`\nSET `#max-price` $budget\nCLICK `.apply-filters`\n\n# Select first result\nWAIT `.product-item` 3\nCLICK `.product-item:first-child`\nCopy\n```\n\n### Example 3: Form Automation with Conditions\n```\n# Smart form filling with error handling\nGO https://forms.example.com\n\n# Check if user is already logged in\nIF (EXISTS `.user-menu`) THEN GO https://forms.example.com/new\nIF (NOT EXISTS `.user-menu`) THEN CLICK `#login-link`\n\n# Fill form\nWAIT `#contact-form` 5\nSET `#name` \"John Doe\"\nSET `#email` \"john@example.com\"\nSET `#message` \"Hello from C4A-Script!\"\n\n# Handle popup if it appears\nIF (EXISTS `.cookie-banner`) THEN CLICK `.accept-cookies`\n\n# Submit\nCLICK `#submit-button`\nWAIT `.success-message` 10\nCopy\n```\n\n## Visual Programming with Blockly\nC4A-Script includes a powerful visual programming interface built on Google Blockly. Perfect for:\n  * **Non-programmers** who want to create automation\n  * **Rapid prototyping** of automation workflows \n  * **Educational environments** for teaching automation concepts\n  * **Collaborative development** where visual representation helps communication\n\n\n### Features:\n  * **Drag & Drop Interface**: Build scripts by connecting blocks\n  * **Real-time Sync** : Changes in visual mode instantly update the text script\n  * **Smart Block Types** : Blocks are categorized by function (Navigation, Actions, etc.)\n  * **Error Prevention** : Visual connections prevent syntax errors\n  * **Comment Support** : Add visual comment blocks for documentation\n\n\nTry the visual editor in our [live demo](https://docs.crawl4ai.com/c4a-script/demo) or [local tutorial](https://docs.crawl4ai.com/examples/c4a_script/tutorial/).\n## Advanced Features\n### Recording Mode\nThe tutorial interface includes a recording feature that watches your browser interactions and automatically generates C4A-Script commands:\n  1. Click \"Record\" in the tutorial interface\n  2. Perform actions in the browser preview\n  3. Watch as C4A-Script commands are generated in real-time\n  4. Edit and refine the generated script\n\n\n### Error Handling and Debugging\nC4A-Script provides clear error messages and debugging information:\n```\n# Use comments for debugging\n# This will wait up to 10 seconds for the element\nWAIT `#slow-loading-element` 10\n\n# Check if element exists before clicking\nIF (EXISTS `#optional-button`) THEN CLICK `#optional-button`\n\n# Use EVAL for custom debugging\nEVAL `console.log(\"Current page title:\", document.title)`\nCopy\n```\n\n### Integration with Crawl4AI\nC4A-Script integrates seamlessly with Crawl4AI's web crawling capabilities:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Use C4A-Script for interaction before crawling\nscript = \"\"\"\nGO https://example.com\nCLICK `#load-more-content`\nWAIT `.dynamic-content` 5\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    js_code=script,\n    wait_for=\".dynamic-content\"\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://example.com\", config=config)\n    print(result.markdown)\nCopy\n```\n\n## Best Practices\n### 1. Always Wait for Elements\n```\n# Bad: Clicking immediately\nCLICK `#button`\n\n# Good: Wait for element to appear\nWAIT `#button` 5\nCLICK `#button`\nCopy\n```\n\n### 2. Use Descriptive Comments\n```\n# Login to user account\nGO https://myapp.com/login\nWAIT `#login-form` 5\n\n# Enter credentials\nTYPE \"user@example.com\"\nPRESS Tab\nTYPE \"password123\"\n\n# Submit and wait for redirect\nCLICK `#submit-button`\nWAIT `.dashboard` 10\nCopy\n```\n\n### 3. Handle Variable Conditions\n```\n# Handle different page states\nIF (EXISTS `.cookie-banner`) THEN CLICK `.accept-cookies`\nIF (EXISTS `.popup-modal`) THEN CLICK `.close-modal`\n\n# Proceed with main workflow\nCLICK `#main-action`\nCopy\n```\n\n### 4. Use Variables for Reusability\n```\n# Define once, use everywhere\nSETVAR base_url = \"https://myapp.com\"\nSETVAR test_email = \"test@example.com\"\n\nGO $base_url/login\nSET `#email` $test_email\nCopy\n```\n\n## Getting Help\n  * **ðŸ“–[Complete Examples](https://docs.crawl4ai.com/examples/c4a_script/)** - Real-world automation scripts\n  * **ðŸŽ®[Interactive Tutorial](https://docs.crawl4ai.com/examples/c4a_script/tutorial/)** - Hands-on learning environment \n  * **ðŸ“‹[API Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)** - Detailed command documentation\n  * **ðŸŒ[Live Demo](https://docs.crawl4ai.com/c4a-script/demo)** - Try it in your browser\n\n\n## What's Next?\nReady to dive deeper? Check out:\n  1. **[API Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)** - Complete command documentation\n  2. **[Tutorial Examples](https://docs.crawl4ai.com/examples/c4a_script/)** - Copy-paste ready scripts\n  3. **[Local Tutorial Setup](https://docs.crawl4ai.com/examples/c4a_script/tutorial/)** - Run the full development environment\n\n\nC4A-Script makes web automation accessible to everyone. Whether you're a developer automating tests, a designer creating interactive demos, or a business user streamlining repetitive tasks, C4A-Script has the tools you need.\n_Start automating today - your future self will thank you!_ ðŸš€\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/c4a-script/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/c4a-script/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/c4a-script/)\n\n\nESC to close\n#### On this page\n  * [What is C4A-Script?](https://docs.crawl4ai.com/core/c4a-script/#what-is-c4a-script)\n  * [Why C4A-Script?](https://docs.crawl4ai.com/core/c4a-script/#why-c4a-script)\n  * [Getting Started: Your First Script](https://docs.crawl4ai.com/core/c4a-script/#getting-started-your-first-script)\n  * [Interactive Tutorial & Live Demo](https://docs.crawl4ai.com/core/c4a-script/#interactive-tutorial-live-demo)\n  * [Running the Tutorial Locally](https://docs.crawl4ai.com/core/c4a-script/#running-the-tutorial-locally)\n  * [Core Concepts](https://docs.crawl4ai.com/core/c4a-script/#core-concepts)\n  * [Commands and Syntax](https://docs.crawl4ai.com/core/c4a-script/#commands-and-syntax)\n  * [Selectors: Finding Elements](https://docs.crawl4ai.com/core/c4a-script/#selectors-finding-elements)\n  * [Variables and Dynamic Content](https://docs.crawl4ai.com/core/c4a-script/#variables-and-dynamic-content)\n  * [Command Categories](https://docs.crawl4ai.com/core/c4a-script/#command-categories)\n  * [ðŸ§­ Navigation Commands](https://docs.crawl4ai.com/core/c4a-script/#navigation-commands)\n  * [â±ï¸ Wait Commands](https://docs.crawl4ai.com/core/c4a-script/#wait-commands)\n  * [ðŸ–±ï¸ Mouse Commands](https://docs.crawl4ai.com/core/c4a-script/#mouse-commands)\n  * [âŒ¨ï¸ Keyboard Commands](https://docs.crawl4ai.com/core/c4a-script/#keyboard-commands)\n  * [ðŸ”€ Control Flow](https://docs.crawl4ai.com/core/c4a-script/#control-flow)\n  * [ðŸ’¾ Variables & Advanced](https://docs.crawl4ai.com/core/c4a-script/#variables-advanced)\n  * [Real-World Examples](https://docs.crawl4ai.com/core/c4a-script/#real-world-examples)\n  * [Example 1: Login Flow](https://docs.crawl4ai.com/core/c4a-script/#example-1-login-flow)\n  * [Example 2: E-commerce Shopping](https://docs.crawl4ai.com/core/c4a-script/#example-2-e-commerce-shopping)\n  * [Example 3: Form Automation with Conditions](https://docs.crawl4ai.com/core/c4a-script/#example-3-form-automation-with-conditions)\n  * [Visual Programming with Blockly](https://docs.crawl4ai.com/core/c4a-script/#visual-programming-with-blockly)\n  * [Features:](https://docs.crawl4ai.com/core/c4a-script/#features)\n  * [Advanced Features](https://docs.crawl4ai.com/core/c4a-script/#advanced-features)\n  * [Recording Mode](https://docs.crawl4ai.com/core/c4a-script/#recording-mode)\n  * [Error Handling and Debugging](https://docs.crawl4ai.com/core/c4a-script/#error-handling-and-debugging)\n  * [Integration with Crawl4AI](https://docs.crawl4ai.com/core/c4a-script/#integration-with-crawl4ai)\n  * [Best Practices](https://docs.crawl4ai.com/core/c4a-script/#best-practices)\n  * [1. Always Wait for Elements](https://docs.crawl4ai.com/core/c4a-script/#1-always-wait-for-elements)\n  * [2. Use Descriptive Comments](https://docs.crawl4ai.com/core/c4a-script/#2-use-descriptive-comments)\n  * [3. Handle Variable Conditions](https://docs.crawl4ai.com/core/c4a-script/#3-handle-variable-conditions)\n  * [4. Use Variables for Reusability](https://docs.crawl4ai.com/core/c4a-script/#4-use-variables-for-reusability)\n  * [Getting Help](https://docs.crawl4ai.com/core/c4a-script/#getting-help)\n  * [What's Next?](https://docs.crawl4ai.com/core/c4a-script/#whats-next)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/deep-crawling",
    "depth": 1,
    "title": "Deep Crawling - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "e73c4bd99fd57d44ffb7f4cfc12d30f1",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/deep-crawling/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * Deep Crawling\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/#deep-crawling)\n  * [1. Quick Example](https://docs.crawl4ai.com/core/deep-crawling/#1-quick-example)\n  * [2. Understanding Deep Crawling Strategy Options](https://docs.crawl4ai.com/core/deep-crawling/#2-understanding-deep-crawling-strategy-options)\n  * [3. Streaming vs. Non-Streaming Results](https://docs.crawl4ai.com/core/deep-crawling/#3-streaming-vs-non-streaming-results)\n  * [4. Filtering Content with Filter Chains](https://docs.crawl4ai.com/core/deep-crawling/#4-filtering-content-with-filter-chains)\n  * [5. Using Scorers for Prioritized Crawling](https://docs.crawl4ai.com/core/deep-crawling/#5-using-scorers-for-prioritized-crawling)\n  * [6. Advanced Filtering Techniques](https://docs.crawl4ai.com/core/deep-crawling/#6-advanced-filtering-techniques)\n  * [7. Building a Complete Advanced Crawler](https://docs.crawl4ai.com/core/deep-crawling/#7-building-a-complete-advanced-crawler)\n  * [8. Limiting and Controlling Crawl Size](https://docs.crawl4ai.com/core/deep-crawling/#8-limiting-and-controlling-crawl-size)\n  * [9. Common Pitfalls & Tips](https://docs.crawl4ai.com/core/deep-crawling/#9-common-pitfalls-tips)\n  * [10. Summary & Next Steps](https://docs.crawl4ai.com/core/deep-crawling/#10-summary-next-steps)\n\n\n# Deep Crawling\nOne of Crawl4AI's most powerful features is its ability to perform **configurable deep crawling** that can explore websites beyond a single page. With fine-tuned control over crawl depth, domain boundaries, and content filtering, Crawl4AI gives you the tools to extract precisely the content you need.\nIn this tutorial, you'll learn:\n  1. How to set up a **Basic Deep Crawler** with BFS strategy \n  2. Understanding the difference between **streamed and non-streamed** output \n  3. Implementing **filters and scorers** to target specific content \n  4. Creating **advanced filtering chains** for sophisticated crawls \n  5. Using **BestFirstCrawling** for intelligent exploration prioritization \n\n\n> **Prerequisites**  \n>  - Youâ€™ve completed or read [AsyncWebCrawler Basics](https://docs.crawl4ai.com/core/simple-crawling/) to understand how to run a simple crawl.  \n>  - You know how to configure `CrawlerRunConfig`.\n* * *\n## 1. Quick Example\nHere's a minimal code snippet that implements a basic deep crawl using the **BFSDeepCrawlStrategy** :\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n\nasync def main():\n    # Configure a 2-level deep crawl\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(\n            max_depth=2, \n            include_external=False\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n\n        print(f\"Crawled {len(results)} pages in total\")\n\n        # Access individual results\n        for result in results[:3]:  # Show first 3 results\n            print(f\"URL: {result.url}\")\n            print(f\"Depth: {result.metadata.get('depth', 0)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**What's happening?**  \n- `BFSDeepCrawlStrategy(max_depth=2, include_external=False)` instructs Crawl4AI to: - Crawl the starting page (depth 0) plus 2 more levels - Stay within the same domain (don't follow external links) - Each result contains metadata like the crawl depth - Results are returned as a list after all crawling is complete\n* * *\n## 2. Understanding Deep Crawling Strategy Options\n### 2.1 BFSDeepCrawlStrategy (Breadth-First Search)\nThe **BFSDeepCrawlStrategy** uses a breadth-first approach, exploring all links at one depth before moving deeper:\n```\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=50,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n)\nCopy\n```\n\n**Key parameters:** - **`max_depth`**: Number of levels to crawl beyond the starting page -**`include_external`**: Whether to follow links to other domains -**`max_pages`**: Maximum number of pages to crawl (default: infinite) -**`score_threshold`**: Minimum score for URLs to be crawled (default: -inf) -**`filter_chain`**: FilterChain instance for URL filtering -**`url_scorer`**: Scorer instance for evaluating URLs\n### 2.2 DFSDeepCrawlStrategy (Depth-First Search)\nThe **DFSDeepCrawlStrategy** uses a depth-first approach, explores as far down a branch as possible before backtracking.\n```\nfrom crawl4ai.deep_crawling import DFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=30,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)\n)\nCopy\n```\n\n**Key parameters:** - **`max_depth`**: Number of levels to crawl beyond the starting page -**`include_external`**: Whether to follow links to other domains -**`max_pages`**: Maximum number of pages to crawl (default: infinite) -**`score_threshold`**: Minimum score for URLs to be crawled (default: -inf) -**`filter_chain`**: FilterChain instance for URL filtering -**`url_scorer`**: Scorer instance for evaluating URLs\n### 2.3 BestFirstCrawlingStrategy (â­ï¸ - Recommended Deep crawl strategy)\nFor more intelligent crawling, use **BestFirstCrawlingStrategy** with scorers to prioritize the most relevant pages:\n```\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\n# Create a scorer\nscorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7\n)\n\n# Configure the strategy\nstrategy = BestFirstCrawlingStrategy(\n    max_depth=2,\n    include_external=False,\n    url_scorer=scorer,\n    max_pages=25,              # Maximum number of pages to crawl (optional)\n)\nCopy\n```\n\nThis crawling approach: - Evaluates each discovered URL based on scorer criteria - Visits higher-scoring pages first - Helps focus crawl resources on the most relevant content - Can limit total pages crawled with `max_pages` - Does not need `score_threshold` as it naturally prioritizes by score\n* * *\n## 3. Streaming vs. Non-Streaming Results\nCrawl4AI can return results in two modes:\n### 3.1 Non-Streaming Mode (Default)\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=False  # Default behavior\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Wait for ALL results to be collected before returning\n    results = await crawler.arun(\"https://example.com\", config=config)\n\n    for result in results:\n        process_result(result)\nCopy\n```\n\n**When to use non-streaming mode:** - You need the complete dataset before processing - You're performing batch operations on all results together - Crawl time isn't a critical factor\n### 3.2 Streaming Mode\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=True  # Enable streaming\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Returns an async iterator\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        # Process each result as it becomes available\n        process_result(result)\nCopy\n```\n\n**Benefits of streaming mode:** - Process results immediately as they're discovered - Start working with early results while crawling continues - Better for real-time applications or progressive display - Reduces memory pressure when handling many pages\n* * *\n## 4. Filtering Content with Filter Chains\nFilters help you narrow down which pages to crawl. Combine multiple filters using **FilterChain** for powerful targeting.\n### 4.1 Basic URL Pattern Filter\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n\n# Only follow URLs containing \"blog\" or \"docs\"\nurl_filter = URLPatternFilter(patterns=[\"*blog*\", \"*docs*\"])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([url_filter])\n    )\n)\nCopy\n```\n\n### 4.2 Combining Multiple Filters\n```\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    URLPatternFilter,\n    DomainFilter,\n    ContentTypeFilter\n)\n\n# Create a chain of filters\nfilter_chain = FilterChain([\n    # Only follow URLs with specific patterns\n    URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\"]),\n\n    # Only crawl specific domains\n    DomainFilter(\n        allowed_domains=[\"docs.example.com\"],\n        blocked_domains=[\"old.docs.example.com\"]\n    ),\n\n    # Only include specific content types\n    ContentTypeFilter(allowed_types=[\"text/html\"])\n])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=2,\n        filter_chain=filter_chain\n    )\n)\nCopy\n```\n\n### 4.3 Available Filter Types\nCrawl4AI includes several specialized filters:\n  * **`URLPatternFilter`**: Matches URL patterns using wildcard syntax\n  * **`DomainFilter`**: Controls which domains to include or exclude\n  * **`ContentTypeFilter`**: Filters based on HTTP Content-Type\n  * **`ContentRelevanceFilter`**: Uses similarity to a text query\n  * **`SEOFilter`**: Evaluates SEO elements (meta tags, headers, etc.)\n\n\n* * *\n## 5. Using Scorers for Prioritized Crawling\nScorers assign priority values to discovered URLs, helping the crawler focus on the most relevant content first.\n### 5.1 KeywordRelevanceScorer\n```\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n\n# Create a keyword relevance scorer\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7  # Importance of this scorer (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        url_scorer=keyword_scorer\n    ),\n    stream=True  # Recommended with BestFirstCrawling\n)\n\n# Results will come in order of relevance score\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        score = result.metadata.get(\"score\", 0)\n        print(f\"Score: {score:.2f} | {result.url}\")\nCopy\n```\n\n**How scorers work:** - Evaluate each discovered URL before crawling - Calculate relevance based on various signals - Help the crawler make intelligent choices about traversal order\n* * *\n## 6. Advanced Filtering Techniques\n### 6.1 SEO Filter for Quality Assessment\nThe **SEOFilter** helps you identify pages with strong SEO characteristics:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, SEOFilter\n\n# Create an SEO filter that looks for specific keywords in page metadata\nseo_filter = SEOFilter(\n    threshold=0.5,  # Minimum score (0.0 to 1.0)\n    keywords=[\"tutorial\", \"guide\", \"documentation\"]\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([seo_filter])\n    )\n)\nCopy\n```\n\n### 6.2 Content Relevance Filter\nThe **ContentRelevanceFilter** analyzes the actual content of pages:\n```\nfrom crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n\n# Create a content relevance filter\nrelevance_filter = ContentRelevanceFilter(\n    query=\"Web crawling and data extraction with Python\",\n    threshold=0.7  # Minimum similarity score (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([relevance_filter])\n    )\n)\nCopy\n```\n\nThis filter: - Measures semantic similarity between query and page content - It's a BM25-based relevance filter using head section content\n* * *\n## 7. Building a Complete Advanced Crawler\nThis example combines multiple techniques for a sophisticated crawl:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    DomainFilter,\n    URLPatternFilter,\n    ContentTypeFilter\n)\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\nasync def run_advanced_crawler():\n    # Create a sophisticated filter chain\n    filter_chain = FilterChain([\n        # Domain boundaries\n        DomainFilter(\n            allowed_domains=[\"docs.example.com\"],\n            blocked_domains=[\"old.docs.example.com\"]\n        ),\n\n        # URL patterns to include\n        URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\n\n        # Content type filtering\n        ContentTypeFilter(allowed_types=[\"text/html\"])\n    ])\n\n    # Create a relevance scorer\n    keyword_scorer = KeywordRelevanceScorer(\n        keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n        weight=0.7\n    )\n\n    # Set up the configuration\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\n            max_depth=2,\n            include_external=False,\n            filter_chain=filter_chain,\n            url_scorer=keyword_scorer\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        stream=True,\n        verbose=True\n    )\n\n    # Execute the crawl\n    results = []\n    async with AsyncWebCrawler() as crawler:\n        async for result in await crawler.arun(\"https://docs.example.com\", config=config):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n\n    # Analyze the results\n    print(f\"Crawled {len(results)} high-value pages\")\n    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n\n    # Group by depth\n    depth_counts = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n\n    print(\"Pages crawled by depth:\")\n    for depth, count in sorted(depth_counts.items()):\n        print(f\"  Depth {depth}: {count} pages\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_advanced_crawler())\nCopy\n```\n\n* * *\n## 8. Limiting and Controlling Crawl Size\n### 8.1 Using max_pages\nYou can limit the total number of pages crawled with the `max_pages` parameter:\n```\n# Limit to exactly 20 pages regardless of depth\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=3,\n    max_pages=20\n)\nCopy\n```\n\nThis feature is useful for: - Controlling API costs - Setting predictable execution times - Focusing on the most important content - Testing crawl configurations before full execution\n### 8.2 Using score_threshold\nFor BFS and DFS strategies, you can set a minimum score threshold to only crawl high-quality pages:\n```\n# Only follow links with scores above 0.4\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,\n    url_scorer=KeywordRelevanceScorer(keywords=[\"api\", \"guide\", \"reference\"]),\n    score_threshold=0.4  # Skip URLs with scores below this value\n)\nCopy\n```\n\nNote that for BestFirstCrawlingStrategy, score_threshold is not needed since pages are already processed in order of highest score first.\n## 9. Common Pitfalls & Tips\n1.**Set realistic limits.** Be cautious with `max_depth` values > 3, which can exponentially increase crawl size. Use `max_pages` to set hard limits.\n2.**Don't neglect the scoring component.** BestFirstCrawling works best with well-tuned scorers. Experiment with keyword weights for optimal prioritization.\n3.**Be a good web citizen.** Respect robots.txt. (disabled by default)\n4.**Handle page errors gracefully.** Not all pages will be accessible. Check `result.status` when processing results.\n5.**Balance breadth vs. depth.** Choose your strategy wisely - BFS for comprehensive coverage, DFS for deep exploration, BestFirst for focused relevance-based crawling.\n6.**Preserve HTTPS for security.** If crawling HTTPS sites that redirect to HTTP, use `preserve_https_for_internal_links=True` to maintain secure connections:\n```\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=2),\n    preserve_https_for_internal_links=True  # Keep HTTPS even if server redirects to HTTP\n)\nCopy\n```\n\nThis is especially useful for security-conscious crawling or when dealing with sites that support both protocols.\n* * *\n## 10. Summary & Next Steps\nIn this **Deep Crawling with Crawl4AI** tutorial, you learned to:\n  * Configure **BFSDeepCrawlStrategy** , **DFSDeepCrawlStrategy** , and **BestFirstCrawlingStrategy**\n  * Process results in streaming or non-streaming mode\n  * Apply filters to target specific content\n  * Use scorers to prioritize the most relevant pages\n  * Limit crawls with `max_pages` and `score_threshold` parameters\n  * Build a complete advanced crawler with combined techniques\n\n\nWith these tools, you can efficiently extract structured data from websites at scale, focusing precisely on the content you need for your specific use case.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/deep-crawling/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/deep-crawling/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/deep-crawling/)\n\n\nESC to close\n#### On this page\n  * [1. Quick Example](https://docs.crawl4ai.com/core/deep-crawling/#1-quick-example)\n  * [2. Understanding Deep Crawling Strategy Options](https://docs.crawl4ai.com/core/deep-crawling/#2-understanding-deep-crawling-strategy-options)\n  * [2.1 BFSDeepCrawlStrategy (Breadth-First Search)](https://docs.crawl4ai.com/core/deep-crawling/#21-bfsdeepcrawlstrategy-breadth-first-search)\n  * [2.2 DFSDeepCrawlStrategy (Depth-First Search)](https://docs.crawl4ai.com/core/deep-crawling/#22-dfsdeepcrawlstrategy-depth-first-search)\n  * [2.3 BestFirstCrawlingStrategy (â­ï¸ - Recommended Deep crawl strategy)](https://docs.crawl4ai.com/core/deep-crawling/#23-bestfirstcrawlingstrategy-recommended-deep-crawl-strategy)\n  * [3. Streaming vs. Non-Streaming Results](https://docs.crawl4ai.com/core/deep-crawling/#3-streaming-vs-non-streaming-results)\n  * [3.1 Non-Streaming Mode (Default)](https://docs.crawl4ai.com/core/deep-crawling/#31-non-streaming-mode-default)\n  * [3.2 Streaming Mode](https://docs.crawl4ai.com/core/deep-crawling/#32-streaming-mode)\n  * [4. Filtering Content with Filter Chains](https://docs.crawl4ai.com/core/deep-crawling/#4-filtering-content-with-filter-chains)\n  * [4.1 Basic URL Pattern Filter](https://docs.crawl4ai.com/core/deep-crawling/#41-basic-url-pattern-filter)\n  * [4.2 Combining Multiple Filters](https://docs.crawl4ai.com/core/deep-crawling/#42-combining-multiple-filters)\n  * [4.3 Available Filter Types](https://docs.crawl4ai.com/core/deep-crawling/#43-available-filter-types)\n  * [5. Using Scorers for Prioritized Crawling](https://docs.crawl4ai.com/core/deep-crawling/#5-using-scorers-for-prioritized-crawling)\n  * [5.1 KeywordRelevanceScorer](https://docs.crawl4ai.com/core/deep-crawling/#51-keywordrelevancescorer)\n  * [6. Advanced Filtering Techniques](https://docs.crawl4ai.com/core/deep-crawling/#6-advanced-filtering-techniques)\n  * [6.1 SEO Filter for Quality Assessment](https://docs.crawl4ai.com/core/deep-crawling/#61-seo-filter-for-quality-assessment)\n  * [6.2 Content Relevance Filter](https://docs.crawl4ai.com/core/deep-crawling/#62-content-relevance-filter)\n  * [7. Building a Complete Advanced Crawler](https://docs.crawl4ai.com/core/deep-crawling/#7-building-a-complete-advanced-crawler)\n  * [8. Limiting and Controlling Crawl Size](https://docs.crawl4ai.com/core/deep-crawling/#8-limiting-and-controlling-crawl-size)\n  * [8.1 Using max_pages](https://docs.crawl4ai.com/core/deep-crawling/#81-using-max_pages)\n  * [8.2 Using score_threshold](https://docs.crawl4ai.com/core/deep-crawling/#82-using-score_threshold)\n  * [9. Common Pitfalls & Tips](https://docs.crawl4ai.com/core/deep-crawling/#9-common-pitfalls-tips)\n  * [10. Summary & Next Steps](https://docs.crawl4ai.com/core/deep-crawling/#10-summary-next-steps)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/crawler-result",
    "depth": 1,
    "title": "Crawler Result - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "ce45dd6346758c7411cb19b2e9393462",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/crawler-result/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * Crawler Result\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Crawl Result and Output](https://docs.crawl4ai.com/core/crawler-result/#crawl-result-and-output)\n  * [1. The CrawlResult Model](https://docs.crawl4ai.com/core/crawler-result/#1-the-crawlresult-model)\n  * [2. HTML Variants](https://docs.crawl4ai.com/core/crawler-result/#2-html-variants)\n  * [3. Markdown Generation](https://docs.crawl4ai.com/core/crawler-result/#3-markdown-generation)\n  * [4. Structured Extraction: extracted_content](https://docs.crawl4ai.com/core/crawler-result/#4-structured-extraction-extracted_content)\n  * [5. More Fields: Links, Media, Tables and More](https://docs.crawl4ai.com/core/crawler-result/#5-more-fields-links-media-tables-and-more)\n  * [6. Accessing These Fields](https://docs.crawl4ai.com/core/crawler-result/#6-accessing-these-fields)\n  * [7. Next Steps](https://docs.crawl4ai.com/core/crawler-result/#7-next-steps)\n\n\n# Crawl Result and Output\nWhen you call `arun()` on a page, Crawl4AI returns a **`CrawlResult`**object containing everything you might needâ€”raw HTML, a cleaned version, optional screenshots or PDFs, structured extraction results, and more. This document explains those fields and how they map to different output types.\n* * *\n## 1. The `CrawlResult` Model\nBelow is the core schema. Each field captures a different aspect of the crawlâ€™s result:\n```\nclass MarkdownGenerationResult(BaseModel):\n    raw_markdown: str\n    markdown_with_citations: str\n    references_markdown: str\n    fit_markdown: Optional[str] = None\n    fit_html: Optional[str] = None\n\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    fit_html: Optional[str] = None\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    js_execution_result: Optional[Dict[str, Any]] = None\n    screenshot: Optional[str] = None\n    pdf: Optional[bytes] = None\n    mhtml: Optional[str] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    dispatch_result: Optional[DispatchResult] = None\n    redirected_url: Optional[str] = None\n    network_requests: Optional[List[Dict[str, Any]]] = None\n    console_messages: Optional[List[Dict[str, Any]]] = None\n    tables: List[Dict] = Field(default_factory=list)\n\n    class Config:\n        arbitrary_types_allowed = True\nCopy\n```\n\n### Table: Key Fields in `CrawlResult`\nField (Name & Type) | Description  \n---|---  \n**url (`str`)** | The final or actual URL crawled (in case of redirects).  \n**html (`str`)** | Original, unmodified page HTML. Good for debugging or custom processing.  \n**fit_html (`Optional[str]`)** | Preprocessed HTML optimized for extraction and content filtering.  \n**success (`bool`)** |  `True` if the crawl completed without major errors, else `False`.  \n**cleaned_html (`Optional[str]`)** | Sanitized HTML with scripts/styles removed; can exclude tags if configured via `excluded_tags` etc.  \n**media (`Dict[str, List[Dict]]`)** | Extracted media info (images, audio, etc.), each with attributes like `src`, `alt`, `score`, etc.  \n**links (`Dict[str, List[Dict]]`)** | Extracted link data, split by `internal` and `external`. Each link usually has `href`, `text`, etc.  \n**downloaded_files (`Optional[List[str]]`)** | If `accept_downloads=True` in `BrowserConfig`, this lists the filepaths of saved downloads.  \n**js_execution_result (`Optional[Dict[str, Any]]`)** | Results from JavaScript execution during crawling.  \n**screenshot (`Optional[str]`)** | Screenshot of the page (base64-encoded) if `screenshot=True`.  \n**pdf (`Optional[bytes]`)** | PDF of the page if `pdf=True`.  \n**mhtml (`Optional[str]`)** | MHTML snapshot of the page if `capture_mhtml=True`. Contains the full page with all resources.  \n**markdown (`Optional[str or MarkdownGenerationResult]`)** | It holds a `MarkdownGenerationResult`. Over time, this will be consolidated into `markdown`. The generator can provide raw markdown, citations, references, and optionally `fit_markdown`.  \n**extracted_content (`Optional[str]`)** | The output of a structured extraction (CSS/LLM-based) stored as JSON string or other text.  \n**metadata (`Optional[dict]`)** | Additional info about the crawl or extracted data.  \n**error_message (`Optional[str]`)** | If `success=False`, contains a short description of what went wrong.  \n**session_id (`Optional[str]`)** | The ID of the session used for multi-page or persistent crawling.  \n**response_headers (`Optional[dict]`)** | HTTP response headers, if captured.  \n**status_code (`Optional[int]`)** | HTTP status code (e.g., 200 for OK).  \n**ssl_certificate (`Optional[SSLCertificate]`)** | SSL certificate info if `fetch_ssl_certificate=True`.  \n**dispatch_result (`Optional[DispatchResult]`)** | Additional concurrency and resource usage information when crawling URLs in parallel.  \n**redirected_url (`Optional[str]`)** | The URL after any redirects (different from `url` which is the final URL).  \n**network_requests (`Optional[List[Dict[str, Any]]]`)** | List of network requests, responses, and failures captured during the crawl if `capture_network_requests=True`.  \n**console_messages (`Optional[List[Dict[str, Any]]]`)** | List of browser console messages captured during the crawl if `capture_console_messages=True`.  \n**tables (`List[Dict]`)** | Table data extracted from HTML tables with structure `[{headers, rows, caption, summary}]`.  \n* * *\n## 2. HTML Variants\n###  `html`: Raw HTML\nCrawl4AI preserves the exact HTML as `result.html`. Useful for:\n  * Debugging page issues or checking the original content.\n  * Performing your own specialized parse if needed.\n\n\n###  `cleaned_html`: Sanitized\nIf you specify any cleanup or exclusion parameters in `CrawlerRunConfig` (like `excluded_tags`, `remove_forms`, etc.), youâ€™ll see the result here:\n```\nconfig = CrawlerRunConfig(\n    excluded_tags=[\"form\", \"header\", \"footer\"],\n    keep_data_attributes=False\n)\nresult = await crawler.arun(\"https://example.com\", config=config)\nprint(result.cleaned_html)  # Freed of forms, header, footer, data-* attributes\nCopy\n```\n\n* * *\n## 3. Markdown Generation\n### 3.1 `markdown`\n  * **`markdown`**: The current location for detailed markdown output, returning a**`MarkdownGenerationResult`**object.\n  * **`markdown_v2`**: Deprecated since v0.5.\n\n\n**`MarkdownGenerationResult`**Fields:\nField | Description  \n---|---  \n**raw_markdown** | The basic HTMLâ†’Markdown conversion.  \n**markdown_with_citations** | Markdown including inline citations that reference links at the end.  \n**references_markdown** | The references/citations themselves (if `citations=True`).  \n**fit_markdown** | The filtered/â€œfitâ€ markdown if a content filter was used.  \n**fit_html** | The filtered HTML that generated `fit_markdown`.  \n### 3.2 Basic Example with a Markdown Generator\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        options={\"citations\": True, \"body_width\": 80}  # e.g. pass html2text style options\n    )\n)\nresult = await crawler.arun(url=\"https://example.com\", config=config)\n\nmd_res = result.markdown  # or eventually 'result.markdown'\nprint(md_res.raw_markdown[:500])\nprint(md_res.markdown_with_citations)\nprint(md_res.references_markdown)\nCopy\n```\n\n**Note** : If you use a filter like `PruningContentFilter`, youâ€™ll get `fit_markdown` and `fit_html` as well.\n* * *\n## 4. Structured Extraction: `extracted_content`\nIf you run a JSON-based extraction strategy (CSS, XPath, LLM, etc.), the structured data is **not** stored in `markdown`â€”itâ€™s placed in **`result.extracted_content`**as a JSON string (or sometimes plain text).\n### Example: CSS Extraction with `raw://` HTML\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\nHere: - `url=\"raw://...\"` passes the HTML content directly, no network requests.  \n- The **CSS** extraction strategy populates `result.extracted_content` with the JSON array `[{\"title\": \"...\", \"link\": \"...\"}]`.\n* * *\n## 5. More Fields: Links, Media, Tables and More\n### 5.1 `links`\nA dictionary, typically with `\"internal\"` and `\"external\"` lists. Each entry might have `href`, `text`, `title`, etc. This is automatically captured if you havenâ€™t disabled link extraction.\n```\nprint(result.links[\"internal\"][:3])  # Show first 3 internal links\nCopy\n```\n\n### 5.2 `media`\nSimilarly, a dictionary with `\"images\"`, `\"audio\"`, `\"video\"`, etc. Each item could include `src`, `alt`, `score`, and more, if your crawler is set to gather them.\n```\nimages = result.media.get(\"images\", [])\nfor img in images:\n    print(\"Image URL:\", img[\"src\"], \"Alt:\", img.get(\"alt\"))\nCopy\n```\n\n### 5.3 `tables`\nThe `tables` field contains structured data extracted from HTML tables found on the crawled page. Tables are analyzed based on various criteria to determine if they are actual data tables (as opposed to layout tables), including:\n  * Presence of thead and tbody sections\n  * Use of th elements for headers\n  * Column consistency\n  * Text density\n  * And other factors\n\n\nTables that score above the threshold (default: 7) are extracted and stored in result.tables.\n### Accessing Table data:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.w3schools.com/html/html_tables.asp\",\n            config=CrawlerRunConfig(\n                table_score_threshold=7  # Minimum score for table detection\n            )\n        )\n\n        if result.success and result.tables:\n            print(f\"Found {len(result.tables)} tables\")\n\n            for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}:\")\n                print(f\"Caption: {table.get('caption', 'No caption')}\")\n                print(f\"Headers: {table['headers']}\")\n                print(f\"Rows: {len(table['rows'])}\")\n\n                # Print first few rows as example\n                for j, row in enumerate(table['rows'][:3]):\n                    print(f\"  Row {j+1}: {row}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### Configuring Table Extraction:\nYou can adjust the sensitivity of the table detection algorithm with:\n```\nconfig = CrawlerRunConfig(\n    table_score_threshold=5  # Lower value = more tables detected (default: 7)\n)\nCopy\n```\n\nEach extracted table contains: \n  * `headers`: Column header names \n  * `rows`: List of rows, each containing cell values\n  * `caption`: Table caption text (if available) \n  * `summary`: Table summary attribute (if specified)\n\n\n### Table Extraction Tips\n  * Not all HTML tables are extracted - only those detected as \"data tables\" vs. layout tables.\n  * Tables with inconsistent cell counts, nested tables, or those used purely for layout may be skipped.\n  * If you're missing tables, try adjusting the `table_score_threshold` to a lower value (default is 7).\n\n\nThe table detection algorithm scores tables based on features like consistent columns, presence of headers, text density, and more. Tables scoring above the threshold are considered data tables worth extracting.\n### 5.4 `screenshot`, `pdf`, and `mhtml`\nIf you set `screenshot=True`, `pdf=True`, or `capture_mhtml=True` in **`CrawlerRunConfig`**, then:\n  * `result.screenshot` contains a base64-encoded PNG string.\n  * `result.pdf` contains raw PDF bytes (you can write them to a file).\n  * `result.mhtml` contains the MHTML snapshot of the page as a string (you can write it to a .mhtml file).\n\n\n```\n# Save the PDF\nwith open(\"page.pdf\", \"wb\") as f:\n    f.write(result.pdf)\n\n# Save the MHTML\nif result.mhtml:\n    with open(\"page.mhtml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(result.mhtml)\nCopy\n```\n\nThe MHTML (MIME HTML) format is particularly useful as it captures the entire web page including all of its resources (CSS, images, scripts, etc.) in a single file, making it perfect for archiving or offline viewing.\n### 5.5 `ssl_certificate`\nIf `fetch_ssl_certificate=True`, `result.ssl_certificate` holds details about the siteâ€™s SSL cert, such as issuer, validity dates, etc.\n* * *\n## 6. Accessing These Fields\nAfter you run:\n```\nresult = await crawler.arun(url=\"https://example.com\", config=some_config)\nCopy\n```\n\nCheck any field:\n```\nif result.success:\n    print(result.status_code, result.response_headers)\n    print(\"Links found:\", len(result.links.get(\"internal\", [])))\n    if result.markdown:\n        print(\"Markdown snippet:\", result.markdown.raw_markdown[:200])\n    if result.extracted_content:\n        print(\"Structured JSON:\", result.extracted_content)\nelse:\n    print(\"Error:\", result.error_message)\nCopy\n```\n\n**Deprecation** : Since v0.5 `result.markdown_v2`, `result.fit_html`,`result.fit_markdown` are deprecated. Use `result.markdown` instead! It holds `MarkdownGenerationResult`, which includes `fit_html` and `fit_markdown` as it's properties.\n* * *\n## 7. Next Steps\n  * **Markdown Generation** : Dive deeper into how to configure `DefaultMarkdownGenerator` and various filters. \n  * **Content Filtering** : Learn how to use `BM25ContentFilter` and `PruningContentFilter`.\n  * **Session & Hooks**: If you want to manipulate the page or preserve state across multiple `arun()` calls, see the hooking or session docs. \n  * **LLM Extraction** : For complex or unstructured content requiring AI-driven parsing, check the LLM-based strategies doc.\n\n\n**Enjoy** exploring all that `CrawlResult` offersâ€”whether you need raw HTML, sanitized output, markdown, or fully structured data, Crawl4AI has you covered!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/crawler-result/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/crawler-result/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/crawler-result/)\n\n\nESC to close\n#### On this page\n  * [1. The CrawlResult Model](https://docs.crawl4ai.com/core/crawler-result/#1-the-crawlresult-model)\n  * [Table: Key Fields in CrawlResult](https://docs.crawl4ai.com/core/crawler-result/#table-key-fields-in-crawlresult)\n  * [2. HTML Variants](https://docs.crawl4ai.com/core/crawler-result/#2-html-variants)\n  * [html: Raw HTML](https://docs.crawl4ai.com/core/crawler-result/#html-raw-html)\n  * [cleaned_html: Sanitized](https://docs.crawl4ai.com/core/crawler-result/#cleaned_html-sanitized)\n  * [3. Markdown Generation](https://docs.crawl4ai.com/core/crawler-result/#3-markdown-generation)\n  * [3.1 markdown](https://docs.crawl4ai.com/core/crawler-result/#31-markdown)\n  * [3.2 Basic Example with a Markdown Generator](https://docs.crawl4ai.com/core/crawler-result/#32-basic-example-with-a-markdown-generator)\n  * [4. Structured Extraction: extracted_content](https://docs.crawl4ai.com/core/crawler-result/#4-structured-extraction-extracted_content)\n  * [Example: CSS Extraction with raw:// HTML](https://docs.crawl4ai.com/core/crawler-result/#example-css-extraction-with-raw-html)\n  * [5. More Fields: Links, Media, Tables and More](https://docs.crawl4ai.com/core/crawler-result/#5-more-fields-links-media-tables-and-more)\n  * [5.1 links](https://docs.crawl4ai.com/core/crawler-result/#51-links)\n  * [5.2 media](https://docs.crawl4ai.com/core/crawler-result/#52-media)\n  * [5.3 tables](https://docs.crawl4ai.com/core/crawler-result/#53-tables)\n  * [Accessing Table data:](https://docs.crawl4ai.com/core/crawler-result/#accessing-table-data)\n  * [Configuring Table Extraction:](https://docs.crawl4ai.com/core/crawler-result/#configuring-table-extraction)\n  * [Table Extraction Tips](https://docs.crawl4ai.com/core/crawler-result/#table-extraction-tips)\n  * [5.4 screenshot, pdf, and mhtml](https://docs.crawl4ai.com/core/crawler-result/#54-screenshot-pdf-and-mhtml)\n  * [5.5 ssl_certificate](https://docs.crawl4ai.com/core/crawler-result/#55-ssl_certificate)\n  * [6. Accessing These Fields](https://docs.crawl4ai.com/core/crawler-result/#6-accessing-these-fields)\n  * [7. Next Steps](https://docs.crawl4ai.com/core/crawler-result/#7-next-steps)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/content-selection",
    "depth": 1,
    "title": "Content Selection - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "2c634756036c5d623f9457c675a1e829",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/content-selection/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * Content Selection\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Content Selection](https://docs.crawl4ai.com/core/content-selection/#content-selection)\n  * [1. CSS-Based Selection](https://docs.crawl4ai.com/core/content-selection/#1-css-based-selection)\n  * [2. Content Filtering & Exclusions](https://docs.crawl4ai.com/core/content-selection/#2-content-filtering-exclusions)\n  * [3. Handling Iframes](https://docs.crawl4ai.com/core/content-selection/#3-handling-iframes)\n  * [4. Structured Extraction Examples](https://docs.crawl4ai.com/core/content-selection/#4-structured-extraction-examples)\n  * [5. Comprehensive Example](https://docs.crawl4ai.com/core/content-selection/#5-comprehensive-example)\n  * [6. Scraping Modes](https://docs.crawl4ai.com/core/content-selection/#6-scraping-modes)\n  * [7. Combining CSS Selection Methods](https://docs.crawl4ai.com/core/content-selection/#7-combining-css-selection-methods)\n  * [8. Conclusion](https://docs.crawl4ai.com/core/content-selection/#8-conclusion)\n\n\n# Content Selection\nCrawl4AI provides multiple ways to **select** , **filter** , and **refine** the content from your crawls. Whether you need to target a specific CSS region, exclude entire tags, filter out external links, or remove certain domains and images, **`CrawlerRunConfig`**offers a wide range of parameters.\nBelow, we show how to configure these parameters and combine them for precise control.\n* * *\n## 1. CSS-Based Selection\nThere are two ways to select content from a page: using `css_selector` or the more flexible `target_elements`.\n### 1.1 Using `css_selector`\nA straightforward way to **limit** your crawl results to a certain region of the page is **`css_selector`**in**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # e.g., first 30 items from Hacker News\n        css_selector=\".athing:nth-child(-n+30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        print(\"Partial HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Result** : Only elements matching that selector remain in `result.cleaned_html`.\n### 1.2 Using `target_elements`\nThe `target_elements` parameter provides more flexibility by allowing you to target **multiple elements** for content extraction while preserving the entire page context for other features:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Target article body and sidebar, but not other content\n        target_elements=[\"article.main-content\", \"aside.sidebar\"]\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog-post\", \n            config=config\n        )\n        print(\"Markdown focused on target elements\")\n        print(\"Links from entire page still available:\", len(result.links.get(\"internal\", [])))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key difference** : With `target_elements`, the markdown generation and structural data extraction focus on those elements, but other page elements (like links, images, and tables) are still extracted from the entire page. This gives you fine-grained control over what appears in your markdown content while preserving full page context for link analysis and media collection.\n* * *\n## 2. Content Filtering & Exclusions\n### 2.1 Basic Overview\n```\nconfig = CrawlerRunConfig(\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n\n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n\n    # Link filtering\n    exclude_external_links=True,    \n    exclude_social_media_links=True,\n    # Block entire domains\n    exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],    \n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n\n    # Media filtering\n    exclude_external_images=True\n)\nCopy\n```\n\n**Explanation** :\n  * **`word_count_threshold`**: Ignores text blocks under X words. Helps skip trivial blocks like short nav or disclaimers.\n  * **`excluded_tags`**: Removes entire tags (`<form>` , `<header>`, `<footer>`, etc.). \n  * **Link Filtering** : \n  * `exclude_external_links`: Strips out external links and may remove them from `result.links`. \n  * `exclude_social_media_links`: Removes links pointing to known social media domains. \n  * `exclude_domains`: A custom list of domains to block if discovered in links. \n  * `exclude_social_media_domains`: A curated list (override or add to it) for social media sites. \n  * **Media Filtering** : \n  * `exclude_external_images`: Discards images not hosted on the same domain as the main page (or its subdomains).\n\n\nBy default in case you set `exclude_social_media_links=True`, the following social media domains are excluded: \n```\n[\n    'facebook.com',\n    'twitter.com',\n    'x.com',\n    'linkedin.com',\n    'instagram.com',\n    'pinterest.com',\n    'tiktok.com',\n    'snapchat.com',\n    'reddit.com',\n]\nCopy\n```\n\n### 2.2 Example Usage\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        css_selector=\"main.content\", \n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        exclude_social_media_links=True,\n        exclude_domains=[\"ads.com\", \"spammytrackers.net\"],\n        exclude_external_images=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        print(\"Cleaned HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Note** : If these parameters remove too much, reduce or disable them accordingly.\n* * *\n## 3. Handling Iframes\nSome sites embed content in `<iframe>` tags. If you want that inline: \n```\nconfig = CrawlerRunConfig(\n    # Merge iframe content into the final output\n    process_iframes=True,    \n    remove_overlay_elements=True\n)\nCopy\n```\n\n**Usage** : \n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        process_iframes=True,\n        remove_overlay_elements=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.org/iframe-demo\", \n            config=config\n        )\n        print(\"Iframe-merged length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## 4. Structured Extraction Examples\nYou can combine content selection with a more advanced extraction strategy. For instance, a **CSS-based** or **LLM-based** extraction strategy can run on the filtered HTML.\n### 4.1 Pattern-Based with `JsonCssExtractionStrategy`\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    # Minimal schema for repeated items\n    schema = {\n        \"name\": \"News Items\",\n        \"baseSelector\": \"tr.athing\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"span.titleline a\", \"type\": \"text\"},\n            {\n                \"name\": \"link\", \n                \"selector\": \"span.titleline a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Content filtering\n        excluded_tags=[\"form\", \"header\"],\n        exclude_domains=[\"adsite.com\"],\n\n        # CSS selection or entire page\n        css_selector=\"table.itemlist\",\n\n        # No caching for demonstration\n        cache_mode=CacheMode.BYPASS,\n\n        # Extraction strategy\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        data = json.loads(result.extracted_content)\n        print(\"Sample extracted item:\", data[:1])  # Show first item\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 4.2 LLM-Based Extraction\n```\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass ArticleData(BaseModel):\n    headline: str\n    summary: str\n\nasync def main():\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4\",api_token=\"sk-YOUR_API_KEY\")\n        schema=ArticleData.schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract 'headline' and a short 'summary' from the content.\"\n    )\n\n    config = CrawlerRunConfig(\n        exclude_external_links=True,\n        word_count_threshold=20,\n        extraction_strategy=llm_strategy\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        article = json.loads(result.extracted_content)\n        print(article)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\nHere, the crawler:\n  * Filters out external links (`exclude_external_links=True`). \n  * Ignores very short text blocks (`word_count_threshold=20`). \n  * Passes the final HTML to your LLM strategy for an AI-driven parse.\n\n\n* * *\n## 5. Comprehensive Example\nBelow is a short function that unifies **CSS selection** , **exclusion** logic, and a pattern-based extraction, demonstrating how you can fine-tune your final data:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def extract_main_articles(url: str):\n    schema = {\n        \"name\": \"ArticleBlock\",\n        \"baseSelector\": \"div.article-block\",\n        \"fields\": [\n            {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n            {\n                \"name\": \"metadata\",\n                \"type\": \"nested\",\n                \"fields\": [\n                    {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                    {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n                ]\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Keep only #main-content\n        css_selector=\"#main-content\",\n\n        # Filtering\n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],  \n        exclude_external_links=True,\n        exclude_domains=[\"somebadsite.com\"],\n        exclude_external_images=True,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url, config=config)\n        if not result.success:\n            print(f\"Error: {result.error_message}\")\n            return None\n        return json.loads(result.extracted_content)\n\nasync def main():\n    articles = await extract_main_articles(\"https://news.ycombinator.com/newest\")\n    if articles:\n        print(\"Extracted Articles:\", articles[:2])  # Show first 2\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Why This Works** : - **CSS** scoping with `#main-content`.  \n- Multiple **exclude_** parameters to remove domains, external images, etc.  \n- A **JsonCssExtractionStrategy** to parse repeated article blocks.\n* * *\n## 6. Scraping Modes\nCrawl4AI uses `LXMLWebScrapingStrategy` (LXML-based) as the default scraping strategy for HTML content processing. This strategy offers excellent performance, especially for large HTML documents.\n**Note:** For backward compatibility, `WebScrapingStrategy` is still available as an alias for `LXMLWebScrapingStrategy`.\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy\n\nasync def main():\n    # Default configuration already uses LXMLWebScrapingStrategy\n    config = CrawlerRunConfig()\n\n    # Or explicitly specify it if desired\n    config_explicit = CrawlerRunConfig(\n        scraping_strategy=LXMLWebScrapingStrategy()\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\", \n            config=config\n        )\nCopy\n```\n\nYou can also create your own custom scraping strategy by inheriting from `ContentScrapingStrategy`. The strategy must return a `ScrapingResult` object with the following structure:\n```\nfrom crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\n\nclass CustomScrapingStrategy(ContentScrapingStrategy):\n    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # Implement your custom scraping logic here\n        return ScrapingResult(\n            cleaned_html=\"<html>...</html>\",  # Cleaned HTML content\n            success=True,                     # Whether scraping was successful\n            media=Media(\n                images=[                      # List of images found\n                    MediaItem(\n                        src=\"https://example.com/image.jpg\",\n                        alt=\"Image description\",\n                        desc=\"Surrounding text\",\n                        score=1,\n                        type=\"image\",\n                        group_id=1,\n                        format=\"jpg\",\n                        width=800\n                    )\n                ],\n                videos=[],                    # List of videos (same structure as images)\n                audios=[]                     # List of audio files (same structure as images)\n            ),\n            links=Links(\n                internal=[                    # List of internal links\n                    Link(\n                        href=\"https://example.com/page\",\n                        text=\"Link text\",\n                        title=\"Link title\",\n                        base_domain=\"example.com\"\n                    )\n                ],\n                external=[]                   # List of external links (same structure)\n            ),\n            metadata={                        # Additional metadata\n                \"title\": \"Page Title\",\n                \"description\": \"Page description\"\n            }\n        )\n\n    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # For simple cases, you can use the sync version\n        return await asyncio.to_thread(self.scrap, url, html, **kwargs)\nCopy\n```\n\n### Performance Considerations\nThe LXML strategy provides excellent performance, particularly when processing large HTML documents, offering up to 10-20x faster processing compared to BeautifulSoup-based approaches.\nBenefits of LXML strategy: - Fast processing of large HTML documents (especially >100KB) - Efficient memory usage - Good handling of well-formed HTML - Robust table detection and extraction\n### Backward Compatibility\nFor users upgrading from earlier versions: - `WebScrapingStrategy` is now an alias for `LXMLWebScrapingStrategy` - Existing code using `WebScrapingStrategy` will continue to work without modification - No changes are required to your existing code\n* * *\n## 7. Combining CSS Selection Methods\nYou can combine `css_selector` and `target_elements` in powerful ways to achieve fine-grained control over your output:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Target specific content but preserve page context\n    config = CrawlerRunConfig(\n        # Focus markdown on main content and sidebar\n        target_elements=[\"#main-content\", \".sidebar\"],\n\n        # Global filters applied to entire page\n        excluded_tags=[\"nav\", \"footer\", \"header\"],\n        exclude_external_links=True,\n\n        # Use basic content thresholds\n        word_count_threshold=15,\n\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/article\",\n            config=config\n        )\n\n        print(f\"Content focuses on specific elements, but all links still analyzed\")\n        print(f\"Internal links: {len(result.links.get('internal', []))}\")\n        print(f\"External links: {len(result.links.get('external', []))}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\nThis approach gives you the best of both worlds: - Markdown generation and content extraction focus on the elements you care about - Links, images and other page data still give you the full context of the page - Content filtering still applies globally\n## 8. Conclusion\nBy mixing **target_elements** or **css_selector** scoping, **content filtering** parameters, and advanced **extraction strategies** , you can precisely **choose** which data to keep. Key parameters in **`CrawlerRunConfig`**for content selection include:\n  1. **`target_elements`**â€“ Array of CSS selectors to focus markdown generation and data extraction, while preserving full page context for links and media.\n  2. **`css_selector`**â€“ Basic scoping to an element or region for all extraction processes.\n  3. **`word_count_threshold`**â€“ Skip short blocks.\n  4. **`excluded_tags`**â€“ Remove entire HTML tags.\n  5. **`exclude_external_links`**,**`exclude_social_media_links`**,**`exclude_domains`**â€“ Filter out unwanted links or domains.\n  6. **`exclude_external_images`**â€“ Remove images from external sources.\n  7. **`process_iframes`**â€“ Merge iframe content if needed.\n\n\nCombine these with structured extraction (CSS, LLM-based, or others) to build powerful crawls that yield exactly the content you want, from raw or cleaned HTML up to sophisticated JSON structures. For more detail, see [Configuration Reference](https://docs.crawl4ai.com/api/parameters/). Enjoy curating your data to the max!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/content-selection/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/content-selection/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/content-selection/)\n\n\nESC to close\n#### On this page\n  * [1. CSS-Based Selection](https://docs.crawl4ai.com/core/content-selection/#1-css-based-selection)\n  * [1.1 Using css_selector](https://docs.crawl4ai.com/core/content-selection/#11-using-css_selector)\n  * [1.2 Using target_elements](https://docs.crawl4ai.com/core/content-selection/#12-using-target_elements)\n  * [2. Content Filtering & Exclusions](https://docs.crawl4ai.com/core/content-selection/#2-content-filtering-exclusions)\n  * [2.1 Basic Overview](https://docs.crawl4ai.com/core/content-selection/#21-basic-overview)\n  * [2.2 Example Usage](https://docs.crawl4ai.com/core/content-selection/#22-example-usage)\n  * [3. Handling Iframes](https://docs.crawl4ai.com/core/content-selection/#3-handling-iframes)\n  * [4. Structured Extraction Examples](https://docs.crawl4ai.com/core/content-selection/#4-structured-extraction-examples)\n  * [4.1 Pattern-Based with JsonCssExtractionStrategy](https://docs.crawl4ai.com/core/content-selection/#41-pattern-based-with-jsoncssextractionstrategy)\n  * [4.2 LLM-Based Extraction](https://docs.crawl4ai.com/core/content-selection/#42-llm-based-extraction)\n  * [5. Comprehensive Example](https://docs.crawl4ai.com/core/content-selection/#5-comprehensive-example)\n  * [6. Scraping Modes](https://docs.crawl4ai.com/core/content-selection/#6-scraping-modes)\n  * [Performance Considerations](https://docs.crawl4ai.com/core/content-selection/#performance-considerations)\n  * [Backward Compatibility](https://docs.crawl4ai.com/core/content-selection/#backward-compatibility)\n  * [7. Combining CSS Selection Methods](https://docs.crawl4ai.com/core/content-selection/#7-combining-css-selection-methods)\n  * [8. Conclusion](https://docs.crawl4ai.com/core/content-selection/#8-conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/examples",
    "depth": 1,
    "title": "Code Examples - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "3a0098788b33627ebb0329776528e85c",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/examples/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * Code Examples\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/#code-examples)\n  * [Getting Started Examples](https://docs.crawl4ai.com/core/examples/#getting-started-examples)\n  * [Proxies](https://docs.crawl4ai.com/core/examples/#proxies)\n  * [Browser & Crawling Features](https://docs.crawl4ai.com/core/examples/#browser-crawling-features)\n  * [Advanced Crawling & Deep Crawling](https://docs.crawl4ai.com/core/examples/#advanced-crawling-deep-crawling)\n  * [Extraction Strategies](https://docs.crawl4ai.com/core/examples/#extraction-strategies)\n  * [E-commerce & Specialized Crawling](https://docs.crawl4ai.com/core/examples/#e-commerce-specialized-crawling)\n  * [Anti-Bot & Stealth Features](https://docs.crawl4ai.com/core/examples/#anti-bot-stealth-features)\n  * [Customization & Security](https://docs.crawl4ai.com/core/examples/#customization-security)\n  * [Docker & Deployment](https://docs.crawl4ai.com/core/examples/#docker-deployment)\n  * [Application Examples](https://docs.crawl4ai.com/core/examples/#application-examples)\n  * [Content Generation & Markdown](https://docs.crawl4ai.com/core/examples/#content-generation-markdown)\n  * [Running the Examples](https://docs.crawl4ai.com/core/examples/#running-the-examples)\n  * [Contributing New Examples](https://docs.crawl4ai.com/core/examples/#contributing-new-examples)\n\n\n# Code Examples\nThis page provides a comprehensive list of example scripts that demonstrate various features and capabilities of Crawl4AI. Each example is designed to showcase specific functionality, making it easier for you to understand how to implement these features in your own projects.\n## Getting Started Examples\nExample | Description | Link  \n---|---|---  \nHello World | A simple introductory example demonstrating basic usage of AsyncWebCrawler with JavaScript execution and content filtering. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/hello_world.py)  \nQuickstart | A comprehensive collection of examples showcasing various features including basic crawling, content cleaning, link analysis, JavaScript execution, CSS selectors, media handling, custom hooks, proxy configuration, screenshots, and multiple extraction strategies. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.py)  \nQuickstart Set 1 | Basic examples for getting started with Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart_examples_set_1.py)  \nQuickstart Set 2 | More advanced examples for working with Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart_examples_set_2.py)  \n## Proxies\nExample | Description | Link  \n---|---|---  \n**NSTProxy** |  [NSTProxy](https://www.nstproxy.com/?utm_source=crawl4ai) Seamlessly integrates with crawl4ai â€” no setup required. Access high-performance residential, datacenter, ISP, and IPv6 proxies with smart rotation and anti-blocking technology. Starts from $0.1/GB. Use code crawl4ai for 10% off. | [View Code](https://github.com/unclecode/crawl4ai/tree/main/docs/examples/proxy)  \n## Browser & Crawling Features\nExample | Description | Link  \n---|---|---  \nBuilt-in Browser | Demonstrates how to use the built-in browser capabilities. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/builtin_browser_example.py)  \nBrowser Optimization | Focuses on browser performance optimization techniques. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/browser_optimization_example.py)  \narun vs arun_many | Compares the `arun` and `arun_many` methods for single vs. multiple URL crawling. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/arun_vs_arun_many.py)  \nMultiple URLs | Shows how to crawl multiple URLs asynchronously. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/async_webcrawler_multiple_urls_example.py)  \nPage Interaction | Guide on interacting with dynamic elements through clicks. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md)  \nCrawler Monitor | Shows how to monitor the crawler's activities and status. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crawler_monitor_example.py)  \nFull Page Screenshot & PDF | Guide on capturing full-page screenshots and PDFs from massive webpages. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/full_page_screenshot_and_pdf_export.md)  \n## Advanced Crawling & Deep Crawling\nExample | Description | Link  \n---|---|---  \nDeep Crawling | An extensive tutorial on deep crawling capabilities, demonstrating BFS and BestFirst strategies, stream vs. non-stream execution, filters, scorers, and advanced configurations. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/deepcrawl_example.py)  \nVirtual Scroll | Comprehensive examples for handling virtualized scrolling on sites like Twitter, Instagram. Demonstrates different scrolling scenarios with local test server. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/virtual_scroll_example.py)  \nAdaptive Crawling | Demonstrates intelligent crawling that automatically determines when sufficient information has been gathered. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/adaptive_crawling/)  \nDispatcher | Shows how to use the crawl dispatcher for advanced workload management. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/dispatcher_example.py)  \nStorage State | Tutorial on managing browser storage state for persistence. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md)  \nNetwork Console Capture | Demonstrates how to capture and analyze network requests and console logs. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/network_console_capture_example.py)  \n## Extraction Strategies\nExample | Description | Link  \n---|---|---  \nExtraction Strategies | Demonstrates different extraction strategies with various input formats (markdown, HTML, fit_markdown) and JSON-based extractors (CSS and XPath). | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/extraction_strategies_examples.py)  \nScraping Strategies | Compares the performance of different scraping strategies. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/scraping_strategies_performance.py)  \nLLM Extraction | Demonstrates LLM-based extraction specifically for OpenAI pricing data. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/llm_extraction_openai_pricing.py)  \nLLM Markdown | Shows how to use LLMs to generate markdown from crawled content. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/llm_markdown_generator.py)  \nSummarize Page | Shows how to summarize web page content. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/summarize_page.py)  \n## E-commerce & Specialized Crawling\nExample | Description | Link  \n---|---|---  \nAmazon Product Extraction | Demonstrates how to extract structured product data from Amazon search results using CSS selectors. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/amazon_product_extraction_direct_url.py)  \nAmazon with Hooks | Shows how to use hooks with Amazon product extraction. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/amazon_product_extraction_using_hooks.py)  \nAmazon with JavaScript | Demonstrates using custom JavaScript for Amazon product extraction. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/amazon_product_extraction_using_use_javascript.py)  \nCrypto Analysis | Demonstrates how to crawl and analyze cryptocurrency data. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crypto_analysis_example.py)  \nSERP API | Demonstrates using Crawl4AI with search engine result pages. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/serp_api_project_11_feb.py)  \n## Anti-Bot & Stealth Features\nExample | Description | Link  \n---|---|---  \nStealth Mode Quick Start | Five practical examples showing how to use stealth mode for bypassing basic bot detection. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/stealth_mode_quick_start.py)  \nStealth Mode Comprehensive | Comprehensive demonstration of stealth mode features with bot detection testing and comparisons. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/stealth_mode_example.py)  \nUndetected Browser | Simple example showing how to use the undetected browser adapter. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/hello_world_undetected.py)  \nUndetected Browser Demo | Basic demo comparing regular and undetected browser modes. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/undetected_simple_demo.py)  \nUndetected Tests | Advanced tests comparing regular vs undetected browsers on various bot detection services. | [View Folder](https://github.com/unclecode/crawl4ai/tree/main/docs/examples/undetectability/)  \nCapSolver Captcha Solver | Seamlessly integrate with [CapSolver](https://www.capsolver.com/?utm_source=crawl4ai&utm_medium=github_pr&utm_campaign=crawl4ai_integration) to automatically solve reCAPTCHA v2/v3, Cloudflare Turnstile / Challenges, AWS WAF and more for uninterrupted scraping and automation. | [View Folder](https://github.com/unclecode/crawl4ai/tree/main/docs/examples/capsolver_captcha_solver/)  \n## Customization & Security\nExample | Description | Link  \n---|---|---  \nHooks | Illustrates how to use hooks at different stages of the crawling process for advanced customization. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/hooks_example.py)  \nIdentity-Based Browsing | Illustrates identity-based browsing configurations for authentic browsing experiences. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/identity_based_browsing.py)  \nProxy Rotation | Shows how to use proxy rotation for web scraping and avoiding IP blocks. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/proxy_rotation_demo.py)  \nSSL Certificate | Illustrates SSL certificate handling and verification. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/ssl_example.py)  \nLanguage Support | Shows how to handle different languages during crawling. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/language_support_example.py)  \nGeolocation | Demonstrates how to use geolocation features. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/use_geo_location.py)  \n## Docker & Deployment\nExample | Description | Link  \n---|---|---  \nDocker Config | Demonstrates how to create and use Docker configuration objects. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_config_obj.py)  \nDocker Basic | A test suite for Docker deployment, showcasing various functionalities through the Docker API. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py)  \nDocker REST API | Shows how to interact with Crawl4AI Docker using REST API calls. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_python_rest_api.py)  \nDocker SDK | Demonstrates using the Python SDK for Crawl4AI Docker. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_python_sdk.py)  \n## Application Examples\nExample | Description | Link  \n---|---|---  \nResearch Assistant | Demonstrates how to build a research assistant using Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/research_assistant.py)  \nREST Call | Shows how to make REST API calls with Crawl4AI. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/rest_call.py)  \nChainlit Integration | Shows how to integrate Crawl4AI with Chainlit. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/chainlit.md)  \nCrawl4AI vs FireCrawl | Compares Crawl4AI with the FireCrawl library. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crawlai_vs_firecrawl.py)  \n## Content Generation & Markdown\nExample | Description | Link  \n---|---|---  \nContent Source | Demonstrates how to work with different content sources in markdown generation. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/markdown/content_source_example.py)  \nContent Source (Short) | A simplified version of content source usage. | [View Code](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/markdown/content_source_short_example.py)  \nBuilt-in Browser Guide | Guide for using the built-in browser capabilities. | [View Guide](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md)  \n## Running the Examples\nTo run any of these examples, you'll need to have Crawl4AI installed:\n```\npip install crawl4ai\nCopy\n```\n\nThen, you can run an example script like this:\n```\npython -m docs.examples.hello_world\nCopy\n```\n\nFor examples that require additional dependencies or environment variables, refer to the comments at the top of each file.\nSome examples may require: - API keys (for LLM-based examples) - Docker setup (for Docker-related examples) - Additional dependencies (specified in the example files)\n## Contributing New Examples\nIf you've created an interesting example that demonstrates a unique use case or feature of Crawl4AI, we encourage you to contribute it to our examples collection. Please see our [contribution guidelines](https://github.com/unclecode/crawl4ai/blob/main/CONTRIBUTORS.md) for more information.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/examples/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/examples/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/examples/)\n\n\nESC to close\n#### On this page\n  * [Getting Started Examples](https://docs.crawl4ai.com/core/examples/#getting-started-examples)\n  * [Proxies](https://docs.crawl4ai.com/core/examples/#proxies)\n  * [Browser & Crawling Features](https://docs.crawl4ai.com/core/examples/#browser-crawling-features)\n  * [Advanced Crawling & Deep Crawling](https://docs.crawl4ai.com/core/examples/#advanced-crawling-deep-crawling)\n  * [Extraction Strategies](https://docs.crawl4ai.com/core/examples/#extraction-strategies)\n  * [E-commerce & Specialized Crawling](https://docs.crawl4ai.com/core/examples/#e-commerce-specialized-crawling)\n  * [Anti-Bot & Stealth Features](https://docs.crawl4ai.com/core/examples/#anti-bot-stealth-features)\n  * [Customization & Security](https://docs.crawl4ai.com/core/examples/#customization-security)\n  * [Docker & Deployment](https://docs.crawl4ai.com/core/examples/#docker-deployment)\n  * [Application Examples](https://docs.crawl4ai.com/core/examples/#application-examples)\n  * [Content Generation & Markdown](https://docs.crawl4ai.com/core/examples/#content-generation-markdown)\n  * [Running the Examples](https://docs.crawl4ai.com/core/examples/#running-the-examples)\n  * [Contributing New Examples](https://docs.crawl4ai.com/core/examples/#contributing-new-examples)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/installation",
    "depth": 1,
    "title": "Installation - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "352b1464e986347382fdc38082218f7a",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/installation/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * Installation\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Installation & Setup (2023 Edition)](https://docs.crawl4ai.com/core/installation/#installation-setup-2023-edition)\n  * [1. Basic Installation](https://docs.crawl4ai.com/core/installation/#1-basic-installation)\n  * [2. Initial Setup & Diagnostics](https://docs.crawl4ai.com/core/installation/#2-initial-setup-diagnostics)\n  * [3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor)](https://docs.crawl4ai.com/core/installation/#3-verifying-installation-a-simple-crawl-skip-this-step-if-you-already-run-crawl4ai-doctor)\n  * [4. Advanced Installation (Optional)](https://docs.crawl4ai.com/core/installation/#4-advanced-installation-optional)\n  * [5. Docker (Experimental)](https://docs.crawl4ai.com/core/installation/#5-docker-experimental)\n  * [6. Local Server Mode (Legacy)](https://docs.crawl4ai.com/core/installation/#6-local-server-mode-legacy)\n  * [Summary](https://docs.crawl4ai.com/core/installation/#summary)\n\n\n# Installation & Setup (2023 Edition)\n## 1. Basic Installation\n```\npip install crawl4ai\nCopy\n```\n\nThis installs the **core** Crawl4AI library along with essential dependencies. **No** advanced features (like transformers or PyTorch) are included yet.\n## 2. Initial Setup & Diagnostics\n### 2.1 Run the Setup Command\nAfter installing, call:\n```\ncrawl4ai-setup\nCopy\n```\n\n**What does it do?** - Installs or updates required browser dependencies for both regular and undetected modes - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl\n### 2.2 Diagnostics\nOptionally, you can run **diagnostics** to confirm everything is functioning:\n```\ncrawl4ai-doctor\nCopy\n```\n\nThis command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts\nIf any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run `crawl4ai-setup`.\n* * *\n## 3. Verifying Installation: A Simple Crawl (Skip this step if you already run `crawl4ai-doctor`)\nBelow is a minimal Python script demonstrating a **basic** crawl. It uses our new **`BrowserConfig`**and**`CrawlerRunConfig`**for clarity, though no custom settings are passed in this example:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n        )\n        print(result.markdown[:300])  # Show the first 300 characters of extracted text\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Expected** outcome: - A headless browser session loads `example.com` - Crawl4AI returns ~300 characters of markdown.  \nIf errors occur, rerun `crawl4ai-doctor` or manually ensure Playwright is installed correctly.\n* * *\n## 4. Advanced Installation (Optional)\n**Warning** : Only install these **if you truly need them**. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly.\n### 4.1 Torch, Transformers, or All\n  * **Text Clustering (Torch)**  \n\n```\npip install crawl4ai[torch]\ncrawl4ai-setup\nCopy\n```\n\nInstalls PyTorch-based features (e.g., cosine similarity or advanced semantic chunking).\n  * **Transformers**  \n\n```\npip install crawl4ai[transformer]\ncrawl4ai-setup\nCopy\n```\n\nAdds Hugging Face-based summarization or generation strategies.\n  * **All Features**  \n\n```\npip install crawl4ai[all]\ncrawl4ai-setup\nCopy\n```\n\n\n\n#### (Optional) Pre-Fetching Models\n```\ncrawl4ai-download-models\nCopy\n```\n\nThis step caches large models locally (if needed). **Only do this** if your workflow requires them.\n* * *\n## 5. Docker (Experimental)\nWe provide a **temporary** Docker approach for testing. **Itâ€™s not stable and may break** with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try:\n```\ndocker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic\nCopy\n```\n\nYou can then make POST requests to `http://localhost:11235/crawl` to perform crawls. **Production usage** is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025).\n* * *\n## 6. Local Server Mode (Legacy)\nSome older docs mention running Crawl4AI as a local server. This approach has been **partially replaced** by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized.\n* * *\n## Summary\n1. **Install** with `pip install crawl4ai` and run `crawl4ai-setup`. 2. **Diagnose** with `crawl4ai-doctor` if you see errors. 3. **Verify** by crawling `example.com` with minimal `BrowserConfig` + `CrawlerRunConfig`. 4. **Advanced** features (Torch, Transformers) are **optional** â€”avoid them if you donâ€™t need them (they significantly increase resource usage). 5. **Docker** is **experimental** â€”use at your own risk until the stable version is released. 6. **Local server** references in older docs are largely deprecated; a new solution is in progress.\n**Got questions?** Check [GitHub issues](https://github.com/unclecode/crawl4ai/issues) for updates or ask the community!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/installation/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/installation/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/installation/)\n\n\nESC to close\n#### On this page\n  * [1. Basic Installation](https://docs.crawl4ai.com/core/installation/#1-basic-installation)\n  * [2. Initial Setup & Diagnostics](https://docs.crawl4ai.com/core/installation/#2-initial-setup-diagnostics)\n  * [2.1 Run the Setup Command](https://docs.crawl4ai.com/core/installation/#21-run-the-setup-command)\n  * [2.2 Diagnostics](https://docs.crawl4ai.com/core/installation/#22-diagnostics)\n  * [3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor)](https://docs.crawl4ai.com/core/installation/#3-verifying-installation-a-simple-crawl-skip-this-step-if-you-already-run-crawl4ai-doctor)\n  * [4. Advanced Installation (Optional)](https://docs.crawl4ai.com/core/installation/#4-advanced-installation-optional)\n  * [4.1 Torch, Transformers, or All](https://docs.crawl4ai.com/core/installation/#41-torch-transformers-or-all)\n  * [(Optional) Pre-Fetching Models](https://docs.crawl4ai.com/core/installation/#optional-pre-fetching-models)\n  * [5. Docker (Experimental)](https://docs.crawl4ai.com/core/installation/#5-docker-experimental)\n  * [6. Local Server Mode (Legacy)](https://docs.crawl4ai.com/core/installation/#6-local-server-mode-legacy)\n  * [Summary](https://docs.crawl4ai.com/core/installation/#summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/fit-markdown",
    "depth": 1,
    "title": "Fit Markdown - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "3b62090e66b5777a5de3f34376360da6",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/fit-markdown/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * Fit Markdown\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Fit Markdown with Pruning & BM25](https://docs.crawl4ai.com/core/fit-markdown/#fit-markdown-with-pruning-bm25)\n  * [1. How â€œFit Markdownâ€ Works](https://docs.crawl4ai.com/core/fit-markdown/#1-how-fit-markdown-works)\n  * [2. PruningContentFilter](https://docs.crawl4ai.com/core/fit-markdown/#2-pruningcontentfilter)\n  * [3. BM25ContentFilter](https://docs.crawl4ai.com/core/fit-markdown/#3-bm25contentfilter)\n  * [4. Accessing the â€œFitâ€ Output](https://docs.crawl4ai.com/core/fit-markdown/#4-accessing-the-fit-output)\n  * [5. Code Patterns Recap](https://docs.crawl4ai.com/core/fit-markdown/#5-code-patterns-recap)\n  * [6. Combining with â€œword_count_thresholdâ€ & Exclusions](https://docs.crawl4ai.com/core/fit-markdown/#6-combining-with-word_count_threshold-exclusions)\n  * [7. Custom Filters](https://docs.crawl4ai.com/core/fit-markdown/#7-custom-filters)\n  * [8. Final Thoughts](https://docs.crawl4ai.com/core/fit-markdown/#8-final-thoughts)\n\n\n# Fit Markdown with Pruning & BM25\n**Fit Markdown** is a specialized **filtered** version of your pageâ€™s markdown, focusing on the most relevant content. By default, Crawl4AI converts the entire HTML into a broad **raw_markdown**. With fit markdown, we apply a **content filter** algorithm (e.g., **Pruning** or **BM25**) to remove or rank low-value sectionsâ€”such as repetitive sidebars, shallow text blocks, or irrelevanciesâ€”leaving a concise textual â€œcore.â€\n* * *\n## 1. How â€œFit Markdownâ€ Works\n### 1.1 The `content_filter`\nIn **`CrawlerRunConfig`**, you can specify a**`content_filter`**to shape how content is pruned or ranked before final markdown generation. A filterâ€™s logic is applied**before** or **during** the HTMLâ†’Markdown process, producing:\n  * **`result.markdown.raw_markdown`**(unfiltered)\n  * **`result.markdown.fit_markdown`**(filtered or â€œfitâ€ version)\n  * **`result.markdown.fit_html`**(the corresponding HTML snippet that produced`fit_markdown`)\n\n\n### 1.2 Common Filters\n1. **PruningContentFilter** â€“ Scores each node by text density, link density, and tag importance, discarding those below a threshold.  \n2. **BM25ContentFilter** â€“ Focuses on textual relevance using BM25 ranking, especially useful if you have a specific user query (e.g., â€œmachine learningâ€ or â€œfood nutritionâ€).\n* * *\n## 2. PruningContentFilter\n**Pruning** discards less relevant nodes based on **text density, link density, and tag importance**. Itâ€™s a heuristic-based approachâ€”if certain sections appear too â€œthinâ€ or too â€œspammy,â€ theyâ€™re pruned.\n### 2.1 Usage Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # Step 1: Create a pruning filter\n    prune_filter = PruningContentFilter(\n        # Lower â†’ more content retained, higher â†’ more content pruned\n        threshold=0.45,           \n        # \"fixed\" or \"dynamic\"\n        threshold_type=\"dynamic\",  \n        # Ignore nodes with <5 words\n        min_word_threshold=5      \n    )\n\n    # Step 2: Insert it into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n\n    # Step 3: Pass it to CrawlerRunConfig\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n\n        if result.success:\n            # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n            print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n            print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 2.2 Key Parameters\n  * **`min_word_threshold`**(int): If a block has fewer words than this, itâ€™s pruned.\n  * **`threshold_type`**(str):\n  * `\"fixed\"` â†’ each node must exceed `threshold` (0â€“1). \n  * `\"dynamic\"` â†’ node scoring adjusts according to tag type, text/link density, etc. \n  * **`threshold`**(float, default ~0.48): The base or â€œanchorâ€ cutoff.\n\n\n**Algorithmic Factors** :\n  * **Text density** â€“ Encourages blocks that have a higher ratio of text to overall content. \n  * **Link density** â€“ Penalizes sections that are mostly links. \n  * **Tag importance** â€“ e.g., an `<article>` or `<p>` might be more important than a `<div>`. \n  * **Structural context** â€“ If a node is deeply nested or in a suspected sidebar, it might be deprioritized.\n\n\n* * *\n## 3. BM25ContentFilter\n**BM25** is a classical text ranking algorithm often used in search engines. If you have a **user query** or rely on page metadata to derive a query, BM25 can identify which text chunks best match that query.\n### 3.1 Usage Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # 1) A BM25 filter with a user query\n    bm25_filter = BM25ContentFilter(\n        user_query=\"startup fundraising tips\",\n        # Adjust for stricter or looser results\n        bm25_threshold=1.2  \n    )\n\n    # 2) Insert into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\n\n    # 3) Pass to crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        if result.success:\n            print(\"Fit Markdown (BM25 query-based):\")\n            print(result.markdown.fit_markdown)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 3.2 Parameters\n  * **`user_query`**(str, optional): E.g.`\"machine learning\"`. If blank, the filter tries to glean a query from page metadata. \n  * **`bm25_threshold`**(float, default 1.0):\n  * Higher â†’ fewer chunks but more relevant. \n  * Lower â†’ more inclusive. \n\n\n> In more advanced scenarios, you might see parameters like `language`, `case_sensitive`, or `priority_tags` to refine how text is tokenized or weighted.\n* * *\n## 4. Accessing the â€œFitâ€ Output\nAfter the crawl, your â€œfitâ€ content is found in **`result.markdown.fit_markdown`**.\n```\nfit_md = result.markdown.fit_markdown\nfit_html = result.markdown.fit_html\nCopy\n```\n\nIf the content filter is **BM25** , you might see additional logic or references in `fit_markdown` that highlight relevant segments. If itâ€™s **Pruning** , the text is typically well-cleaned but not necessarily matched to a query.\n* * *\n## 5. Code Patterns Recap\n### 5.1 Pruning\n```\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",\n    min_word_threshold=10\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n### 5.2 BM25\n```\nbm25_filter = BM25ContentFilter(\n    user_query=\"health benefits fruit\",\n    bm25_threshold=1.2\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n* * *\n## 6. Combining with â€œword_count_thresholdâ€ & Exclusions\nRemember you can also specify:\n```\nconfig = CrawlerRunConfig(\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\", \"header\"],\n    exclude_external_links=True,\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.5)\n    )\n)\nCopy\n```\n\nThus, **multi-level** filtering occurs:\n  1. The crawlerâ€™s `excluded_tags` are removed from the HTML first. \n  2. The content filter (Pruning, BM25, or custom) prunes or ranks the remaining text blocks. \n  3. The final â€œfitâ€ content is generated in `result.markdown.fit_markdown`.\n\n\n* * *\n## 7. Custom Filters\nIf you need a different approach (like a specialized ML model or site-specific heuristics), you can create a new class inheriting from `RelevantContentFilter` and implement `filter_content(html)`. Then inject it into your **markdown generator** :\n```\nfrom crawl4ai.content_filter_strategy import RelevantContentFilter\n\nclass MyCustomFilter(RelevantContentFilter):\n    def filter_content(self, html, min_word_threshold=None):\n        # parse HTML, implement custom logic\n        return [block for block in ... if ... some condition...]\nCopy\n```\n\n**Steps** :\n  1. Subclass `RelevantContentFilter`. \n  2. Implement `filter_content(...)`. \n  3. Use it in your `DefaultMarkdownGenerator(content_filter=MyCustomFilter(...))`.\n\n\n* * *\n## 8. Final Thoughts\n**Fit Markdown** is a crucial feature for:\n  * **Summaries** : Quickly get the important text from a cluttered page. \n  * **Search** : Combine with **BM25** to produce content relevant to a query. \n  * **AI Pipelines** : Filter out boilerplate so LLM-based extraction or summarization runs on denser text.\n\n\n**Key Points** : - **PruningContentFilter** : Great if you just want the â€œmeatiestâ€ text without a user query.  \n- **BM25ContentFilter** : Perfect for query-based extraction or searching.  \n- Combine with **`excluded_tags`,`exclude_external_links` , `word_count_threshold`** to refine your final â€œfitâ€ text.  \n- Fit markdown ends up in **`result.markdown.fit_markdown`**; eventually**`result.markdown.fit_markdown`**in future versions.\nWith these tools, you can **zero in** on the text that truly matters, ignoring spammy or boilerplate content, and produce a concise, relevant â€œfit markdownâ€ for your AI or data pipelines. Happy pruning and searching!\n  * Last Updated: 2025-01-01\n\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/fit-markdown/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/fit-markdown/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/fit-markdown/)\n\n\nESC to close\n#### On this page\n  * [1. How â€œFit Markdownâ€ Works](https://docs.crawl4ai.com/core/fit-markdown/#1-how-fit-markdown-works)\n  * [1.1 The content_filter](https://docs.crawl4ai.com/core/fit-markdown/#11-the-content_filter)\n  * [1.2 Common Filters](https://docs.crawl4ai.com/core/fit-markdown/#12-common-filters)\n  * [2. PruningContentFilter](https://docs.crawl4ai.com/core/fit-markdown/#2-pruningcontentfilter)\n  * [2.1 Usage Example](https://docs.crawl4ai.com/core/fit-markdown/#21-usage-example)\n  * [2.2 Key Parameters](https://docs.crawl4ai.com/core/fit-markdown/#22-key-parameters)\n  * [3. BM25ContentFilter](https://docs.crawl4ai.com/core/fit-markdown/#3-bm25contentfilter)\n  * [3.1 Usage Example](https://docs.crawl4ai.com/core/fit-markdown/#31-usage-example)\n  * [3.2 Parameters](https://docs.crawl4ai.com/core/fit-markdown/#32-parameters)\n  * [4. Accessing the â€œFitâ€ Output](https://docs.crawl4ai.com/core/fit-markdown/#4-accessing-the-fit-output)\n  * [5. Code Patterns Recap](https://docs.crawl4ai.com/core/fit-markdown/#5-code-patterns-recap)\n  * [5.1 Pruning](https://docs.crawl4ai.com/core/fit-markdown/#51-pruning)\n  * [5.2 BM25](https://docs.crawl4ai.com/core/fit-markdown/#52-bm25)\n  * [6. Combining with â€œword_count_thresholdâ€ & Exclusions](https://docs.crawl4ai.com/core/fit-markdown/#6-combining-with-word_count_threshold-exclusions)\n  * [7. Custom Filters](https://docs.crawl4ai.com/core/fit-markdown/#7-custom-filters)\n  * [8. Final Thoughts](https://docs.crawl4ai.com/core/fit-markdown/#8-final-thoughts)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/link-media",
    "depth": 1,
    "title": "Link & Media - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "b2e2b3b0c22545c2f3f9a6336b971a74",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/link-media/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * Link & Media\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Link & Media](https://docs.crawl4ai.com/core/link-media/#link-media)\n  * [Excluding External Images](https://docs.crawl4ai.com/core/link-media/#excluding-external-images)\n  * [Excluding All Images](https://docs.crawl4ai.com/core/link-media/#excluding-all-images)\n  * [1. Link Extraction](https://docs.crawl4ai.com/core/link-media/#1-link-extraction)\n  * [2. Advanced Link Head Extraction & Scoring](https://docs.crawl4ai.com/core/link-media/#2-advanced-link-head-extraction-scoring)\n  * [3. Domain Filtering](https://docs.crawl4ai.com/core/link-media/#3-domain-filtering)\n  * [4. Media Extraction](https://docs.crawl4ai.com/core/link-media/#4-media-extraction)\n  * [5. Putting It All Together: Link & Media Filtering](https://docs.crawl4ai.com/core/link-media/#5-putting-it-all-together-link-media-filtering)\n  * [6. Common Pitfalls & Tips](https://docs.crawl4ai.com/core/link-media/#6-common-pitfalls-tips)\n\n\n# Link & Media\nIn this tutorial, youâ€™ll learn how to:\n  1. Extract links (internal, external) from crawled pages \n  2. Filter or exclude specific domains (e.g., social media or custom domains) \n  3. Access and ma### 3.2 Excluding Images\n\n\n#### Excluding External Images\nIf you're dealing with heavy pages or want to skip third-party images (advertisements, for example), you can turn on:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\nCopy\n```\n\nThis setting attempts to discard images from outside the primary domain, keeping only those from the site you're crawling.\n#### Excluding All Images\nIf you want to completely remove all images from the page to maximize performance and reduce memory usage, use:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_all_images=True\n)\nCopy\n```\n\nThis setting removes all images very early in the processing pipeline, which significantly improves memory efficiency and processing speed. This is particularly useful when: - You don't need image data in your results - You're crawling image-heavy pages that cause memory issues - You want to focus only on text content - You need to maximize crawling speeddata (especially images) in the crawl result  \n4. Configure your crawler to exclude or prioritize certain images\n> **Prerequisites**  \n>  - You have completed or are familiar with the [AsyncWebCrawler Basics](https://docs.crawl4ai.com/core/simple-crawling/) tutorial.  \n>  - You can run Crawl4AI in your environment (Playwright, Python, etc.).\n* * *\nBelow is a revised version of the **Link Extraction** and **Media Extraction** sections that includes example data structures showing how links and media items are stored in `CrawlResult`. Feel free to adjust any field names or descriptions to match your actual output.\n* * *\n## 1. Link Extraction\n### 1.1 `result.links`\nWhen you call `arun()` or `arun_many()` on a URL, Crawl4AI automatically extracts links and stores them in the `links` field of `CrawlResult`. By default, the crawler tries to distinguish **internal** links (same domain) from **external** links (different domains).\n**Basic Example** :\n```\nfrom crawl4ai import AsyncWebCrawler\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://www.example.com\")\n    if result.success:\n        internal_links = result.links.get(\"internal\", [])\n        external_links = result.links.get(\"external\", [])\n        print(f\"Found {len(internal_links)} internal links.\")\n        print(f\"Found {len(internal_links)} external links.\")\n        print(f\"Found {len(result.media)} media items.\")\n\n        # Each link is typically a dictionary with fields like:\n        # { \"href\": \"...\", \"text\": \"...\", \"title\": \"...\", \"base_domain\": \"...\" }\n        if internal_links:\n            print(\"Sample Internal Link:\", internal_links[0])\n    else:\n        print(\"Crawl failed:\", result.error_message)\nCopy\n```\n\n**Structure Example** :\n```\nresult.links = {\n  \"internal\": [\n    {\n      \"href\": \"https://kidocode.com/\",\n      \"text\": \"\",\n      \"title\": \"\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    {\n      \"href\": \"https://kidocode.com/degrees/technology\",\n      \"text\": \"Technology Degree\",\n      \"title\": \"KidoCode Tech Program\",\n      \"base_domain\": \"kidocode.com\"\n    },\n    # ...\n  ],\n  \"external\": [\n    # possibly other links leading to third-party sites\n  ]\n}\nCopy\n```\n\n  * **`href`**: The raw hyperlink URL.\n  * **`text`**: The link text (if any) within the`<a>` tag. \n  * **`title`**: The`title` attribute of the link (if present). \n  * **`base_domain`**: The domain extracted from`href`. Helpful for filtering or grouping by domain.\n\n\n* * *\n## 2. Advanced Link Head Extraction & Scoring\nEver wanted to not just extract links, but also get the actual content (title, description, metadata) from those linked pages? And score them for relevance? This is exactly what Link Head Extraction does - it fetches the `<head>` section from each discovered link and scores them using multiple algorithms.\n### 2.1 Why Link Head Extraction?\nWhen you crawl a page, you get hundreds of links. But which ones are actually valuable? Link Head Extraction solves this by:\n  1. **Fetching head content** from each link (title, description, meta tags)\n  2. **Scoring links intrinsically** based on URL quality, text relevance, and context\n  3. **Scoring links contextually** using BM25 algorithm when you provide a search query\n  4. **Combining scores intelligently** to give you a final relevance ranking\n\n\n### 2.2 Complete Working Example\nHere's a full example you can copy, paste, and run immediately:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai import LinkPreviewConfig\n\nasync def extract_link_heads_example():\n    \"\"\"\n    Complete example showing link head extraction with scoring.\n    This will crawl a documentation site and extract head content from internal links.\n    \"\"\"\n\n    # Configure link head extraction\n    config = CrawlerRunConfig(\n        # Enable link head extraction with detailed configuration\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,           # Extract from internal links\n            include_external=False,          # Skip external links for this example\n            max_links=10,                   # Limit to 10 links for demo\n            concurrency=5,                  # Process 5 links simultaneously\n            timeout=10,                     # 10 second timeout per link\n            query=\"API documentation guide\", # Query for contextual scoring\n            score_threshold=0.3,            # Only include links scoring above 0.3\n            verbose=True                    # Show detailed progress\n        ),\n        # Enable intrinsic scoring (URL quality, text relevance)\n        score_links=True,\n        # Keep output clean\n        only_text=True,\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Crawl a documentation site (great for testing)\n        result = await crawler.arun(\"https://docs.python.org/3/\", config=config)\n\n        if result.success:\n            print(f\"âœ… Successfully crawled: {result.url}\")\n            print(f\"ðŸ“„ Page title: {result.metadata.get('title', 'No title')}\")\n\n            # Access links (now enhanced with head data and scores)\n            internal_links = result.links.get(\"internal\", [])\n            external_links = result.links.get(\"external\", [])\n\n            print(f\"\\nðŸ”— Found {len(internal_links)} internal links\")\n            print(f\"ðŸŒ Found {len(external_links)} external links\")\n\n            # Count links with head data\n            links_with_head = [link for link in internal_links \n                             if link.get(\"head_data\") is not None]\n            print(f\"ðŸ§  Links with head data extracted: {len(links_with_head)}\")\n\n            # Show the top 3 scoring links\n            print(f\"\\nðŸ† Top 3 Links with Full Scoring:\")\n            for i, link in enumerate(links_with_head[:3]):\n                print(f\"\\n{i+1}. {link['href']}\")\n                print(f\"   Link Text: '{link.get('text', 'No text')[:50]}...'\")\n\n                # Show all three score types\n                intrinsic = link.get('intrinsic_score')\n                contextual = link.get('contextual_score') \n                total = link.get('total_score')\n\n                if intrinsic is not None:\n                    print(f\"   ðŸ“Š Intrinsic Score: {intrinsic:.2f}/10.0 (URL quality & context)\")\n                if contextual is not None:\n                    print(f\"   ðŸŽ¯ Contextual Score: {contextual:.3f} (BM25 relevance to query)\")\n                if total is not None:\n                    print(f\"   â­ Total Score: {total:.3f} (combined final score)\")\n\n                # Show extracted head data\n                head_data = link.get(\"head_data\", {})\n                if head_data:\n                    title = head_data.get(\"title\", \"No title\")\n                    description = head_data.get(\"meta\", {}).get(\"description\", \"No description\")\n\n                    print(f\"   ðŸ“° Title: {title[:60]}...\")\n                    if description:\n                        print(f\"   ðŸ“ Description: {description[:80]}...\")\n\n                    # Show extraction status\n                    status = link.get(\"head_extraction_status\", \"unknown\")\n                    print(f\"   âœ… Extraction Status: {status}\")\n        else:\n            print(f\"âŒ Crawl failed: {result.error_message}\")\n\n# Run the example\nif __name__ == \"__main__\":\n    asyncio.run(extract_link_heads_example())\nCopy\n```\n\n**Expected Output:**\n```\nâœ… Successfully crawled: https://docs.python.org/3/\nðŸ“„ Page title: 3.13.5 Documentation\nðŸ”— Found 53 internal links\nðŸŒ Found 1 external links\nðŸ§  Links with head data extracted: 10\n\nðŸ† Top 3 Links with Full Scoring:\n\n1. https://docs.python.org/3.15/\n   Link Text: 'Python 3.15 (in development)...'\n   ðŸ“Š Intrinsic Score: 4.17/10.0 (URL quality & context)\n   ðŸŽ¯ Contextual Score: 1.000 (BM25 relevance to query)\n   â­ Total Score: 5.917 (combined final score)\n   ðŸ“° Title: 3.15.0a0 Documentation...\n   ðŸ“ Description: The official Python documentation...\n   âœ… Extraction Status: valid\nCopy\n```\n\n### 2.3 Configuration Deep Dive\nThe `LinkPreviewConfig` class supports these options:\n```\nfrom crawl4ai import LinkPreviewConfig\n\nlink_preview_config = LinkPreviewConfig(\n    # BASIC SETTINGS\n    verbose=True,                    # Show detailed logs (recommended for learning)\n\n    # LINK FILTERING\n    include_internal=True,           # Include same-domain links\n    include_external=True,           # Include different-domain links\n    max_links=50,                   # Maximum links to process (prevents overload)\n\n    # PATTERN FILTERING\n    include_patterns=[               # Only process links matching these patterns\n        \"*/docs/*\", \n        \"*/api/*\", \n        \"*/reference/*\"\n    ],\n    exclude_patterns=[               # Skip links matching these patterns\n        \"*/login*\",\n        \"*/admin*\"\n    ],\n\n    # PERFORMANCE SETTINGS\n    concurrency=10,                  # How many links to process simultaneously\n    timeout=5,                      # Seconds to wait per link\n\n    # RELEVANCE SCORING\n    query=\"machine learning API\",    # Query for BM25 contextual scoring\n    score_threshold=0.3,            # Only include links above this score\n)\nCopy\n```\n\n### 2.4 Understanding the Three Score Types\nEach extracted link gets three different scores:\n#### 1. **Intrinsic Score (0-10)** - URL and Content Quality\nBased on URL structure, link text quality, and page context:\n```\n# High intrinsic score indicators:\n# âœ… Clean URL structure (docs.python.org/api/reference)\n# âœ… Meaningful link text (\"API Reference Guide\")\n# âœ… Relevant to page context\n# âœ… Not buried deep in navigation\n\n# Low intrinsic score indicators:\n# âŒ Random URLs (site.com/x7f9g2h)\n# âŒ No link text or generic text (\"Click here\")\n# âŒ Unrelated to page content\nCopy\n```\n\n#### 2. **Contextual Score (0-1)** - BM25 Relevance to Query\nOnly available when you provide a `query`. Uses BM25 algorithm against head content:\n```\n# Example: query = \"machine learning tutorial\"\n# High contextual score: Link to \"Complete Machine Learning Guide\"\n# Low contextual score: Link to \"Privacy Policy\"\nCopy\n```\n\n#### 3. **Total Score** - Smart Combination\nIntelligently combines intrinsic and contextual scores with fallbacks:\n```\n# When both scores available: (intrinsic * 0.3) + (contextual * 0.7)\n# When only intrinsic: uses intrinsic score\n# When only contextual: uses contextual score\n# When neither: not calculated\nCopy\n```\n\n### 2.5 Practical Use Cases\n#### Use Case 1: Research Assistant\nFind the most relevant documentation pages:\n```\nasync def research_assistant():\n    config = CrawlerRunConfig(\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,\n            include_external=True,\n            include_patterns=[\"*/docs/*\", \"*/tutorial/*\", \"*/guide/*\"],\n            query=\"machine learning neural networks\",\n            max_links=20,\n            score_threshold=0.5,  # Only high-relevance links\n            verbose=True\n        ),\n        score_links=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://scikit-learn.org/\", config=config)\n\n        if result.success:\n            # Get high-scoring links\n            good_links = [link for link in result.links.get(\"internal\", [])\n                         if link.get(\"total_score\", 0) > 0.7]\n\n            print(f\"ðŸŽ¯ Found {len(good_links)} highly relevant links:\")\n            for link in good_links[:5]:\n                print(f\"â­ {link['total_score']:.3f} - {link['href']}\")\n                print(f\"   {link.get('head_data', {}).get('title', 'No title')}\")\nCopy\n```\n\n#### Use Case 2: Content Discovery\nFind all API endpoints and references:\n```\nasync def api_discovery():\n    config = CrawlerRunConfig(\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,\n            include_patterns=[\"*/api/*\", \"*/reference/*\"],\n            exclude_patterns=[\"*/deprecated/*\"],\n            max_links=100,\n            concurrency=15,\n            verbose=False  # Clean output\n        ),\n        score_links=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://docs.example-api.com/\", config=config)\n\n        if result.success:\n            api_links = result.links.get(\"internal\", [])\n\n            # Group by endpoint type\n            endpoints = {}\n            for link in api_links:\n                if link.get(\"head_data\"):\n                    title = link[\"head_data\"].get(\"title\", \"\")\n                    if \"GET\" in title:\n                        endpoints.setdefault(\"GET\", []).append(link)\n                    elif \"POST\" in title:\n                        endpoints.setdefault(\"POST\", []).append(link)\n\n            for method, links in endpoints.items():\n                print(f\"\\n{method} Endpoints ({len(links)}):\")\n                for link in links[:3]:\n                    print(f\"  â€¢ {link['href']}\")\nCopy\n```\n\n#### Use Case 3: Link Quality Analysis\nAnalyze website structure and content quality:\n```\nasync def quality_analysis():\n    config = CrawlerRunConfig(\n        link_preview_config=LinkPreviewConfig(\n            include_internal=True,\n            max_links=200,\n            concurrency=20,\n        ),\n        score_links=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://your-website.com/\", config=config)\n\n        if result.success:\n            links = result.links.get(\"internal\", [])\n\n            # Analyze intrinsic scores\n            scores = [link.get('intrinsic_score', 0) for link in links]\n            avg_score = sum(scores) / len(scores) if scores else 0\n\n            print(f\"ðŸ“Š Link Quality Analysis:\")\n            print(f\"   Average intrinsic score: {avg_score:.2f}/10.0\")\n            print(f\"   High quality links (>7.0): {len([s for s in scores if s > 7.0])}\")\n            print(f\"   Low quality links (<3.0): {len([s for s in scores if s < 3.0])}\")\n\n            # Find problematic links\n            bad_links = [link for link in links \n                        if link.get('intrinsic_score', 0) < 2.0]\n\n            if bad_links:\n                print(f\"\\nâš ï¸  Links needing attention:\")\n                for link in bad_links[:5]:\n                    print(f\"   {link['href']} (score: {link.get('intrinsic_score', 0):.1f})\")\nCopy\n```\n\n### 2.6 Performance Tips\n  1. **Start Small** : Begin with `max_links: 10` to understand the feature\n  2. **Use Patterns** : Filter with `include_patterns` to focus on relevant sections\n  3. **Adjust Concurrency** : Higher concurrency = faster but more resource usage\n  4. **Set Timeouts** : Use `timeout: 5` to prevent hanging on slow sites\n  5. **Use Score Thresholds** : Filter out low-quality links with `score_threshold`\n\n\n### 2.7 Troubleshooting\n**No head data extracted?**\n```\n# Check your configuration:\nconfig = CrawlerRunConfig(\n    link_preview_config=LinkPreviewConfig(\n        verbose=True   # â† Enable to see what's happening\n    )\n)\nCopy\n```\n\n**Scores showing as None?**\n```\n# Make sure scoring is enabled:\nconfig = CrawlerRunConfig(\n    score_links=True,  # â† Enable intrinsic scoring\n    link_preview_config=LinkPreviewConfig(\n        query=\"your search terms\"  # â† For contextual scoring\n    )\n)\nCopy\n```\n\n**Process taking too long?**\n```\n# Optimize performance:\nlink_preview_config = LinkPreviewConfig(\n    max_links=20,      # â† Reduce number\n    concurrency=10,    # â† Increase parallelism\n    timeout=3,         # â† Shorter timeout\n    include_patterns=[\"*/important/*\"]  # â† Focus on key areas\n)\nCopy\n```\n\n* * *\n## 3. Domain Filtering\nSome websites contain hundreds of third-party or affiliate links. You can filter out certain domains at **crawl time** by configuring the crawler. The most relevant parameters in `CrawlerRunConfig` are:\n  * **`exclude_external_links`**: If`True` , discard any link pointing outside the root domain. \n  * **`exclude_social_media_domains`**: Provide a list of social media platforms (e.g.,`[\"facebook.com\", \"twitter.com\"]`) to exclude from your crawl. \n  * **`exclude_social_media_links`**: If`True` , automatically skip known social platforms. \n  * **`exclude_domains`**: Provide a list of custom domains you want to exclude (e.g.,`[\"spammyads.com\", \"tracker.net\"]`).\n\n\n### 3.1 Example: Excluding External & Social Media Links\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,          # No links outside primary domain\n        exclude_social_media_links=True       # Skip recognized social media domains\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://www.example.com\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            print(\"Internal links count:\", len(result.links.get(\"internal\", [])))\n            print(\"External links count:\", len(result.links.get(\"external\", [])))  \n            # Likely zero external links in this scenario\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### 3.2 Example: Excluding Specific Domains\nIf you want to let external links in, but specifically exclude a domain (e.g., `suspiciousads.com`), do this:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_domains=[\"suspiciousads.com\"]\n)\nCopy\n```\n\nThis approach is handy when you still want external links but need to block certain sites you consider spammy.\n* * *\n## 4. Media Extraction\n### 4.1 Accessing `result.media`\nBy default, Crawl4AI collects images, audio and video URLs it finds on the page. These are stored in `result.media`, a dictionary keyed by media type (e.g., `images`, `videos`, `audio`). **Note: Tables have been moved from`result.media[\"tables\"]` to the new `result.tables` format for better organization and direct access.**\n**Basic Example** :\n```\nif result.success:\n    # Get images\n    images_info = result.media.get(\"images\", [])\n    print(f\"Found {len(images_info)} images in total.\")\n    for i, img in enumerate(images_info[:3]):  # Inspect just the first 3\n        print(f\"[Image {i}] URL: {img['src']}\")\n        print(f\"           Alt text: {img.get('alt', '')}\")\n        print(f\"           Score: {img.get('score')}\")\n        print(f\"           Description: {img.get('desc', '')}\\n\")\nCopy\n```\n\n**Structure Example** :\n```\nresult.media = {\n  \"images\": [\n    {\n      \"src\": \"https://cdn.prod.website-files.com/.../Group%2089.svg\",\n      \"alt\": \"coding school for kids\",\n      \"desc\": \"Trial Class Degrees degrees All Degrees AI Degree Technology ...\",\n      \"score\": 3,\n      \"type\": \"image\",\n      \"group_id\": 0,\n      \"format\": None,\n      \"width\": None,\n      \"height\": None\n    },\n    # ...\n  ],\n  \"videos\": [\n    # Similar structure but with video-specific fields\n  ],\n  \"audio\": [\n    # Similar structure but with audio-specific fields\n  ],\n}\nCopy\n```\n\nDepending on your Crawl4AI version or scraping strategy, these dictionaries can include fields like:\n  * **`src`**: The media URL (e.g., image source)\n  * **`alt`**: The alt text for images (if present)\n  * **`desc`**: A snippet of nearby text or a short description (optional)\n  * **`score`**: A heuristic relevance score if youâ€™re using content-scoring features\n  * **`width`**,**`height`**: If the crawler detects dimensions for the image/video\n  * **`type`**: Usually`\"image\"` , `\"video\"`, or `\"audio\"`\n  * **`group_id`**: If youâ€™re grouping related media items, the crawler might assign an ID\n\n\nWith these details, you can easily filter out or focus on certain images (for instance, ignoring images with very low scores or a different domain), or gather metadata for analytics.\n### 4.2 Excluding External Images\nIf youâ€™re dealing with heavy pages or want to skip third-party images (advertisements, for example), you can turn on:\n```\ncrawler_cfg = CrawlerRunConfig(\n    exclude_external_images=True\n)\nCopy\n```\n\nThis setting attempts to discard images from outside the primary domain, keeping only those from the site youâ€™re crawling.\n### 4.3 Additional Media Config\n  * **`screenshot`**: Set to`True` if you want a full-page screenshot stored as `base64` in `result.screenshot`. \n  * **`pdf`**: Set to`True` if you want a PDF version of the page in `result.pdf`. \n  * **`capture_mhtml`**: Set to`True` if you want an MHTML snapshot of the page in `result.mhtml`. This format preserves the entire web page with all its resources (CSS, images, scripts) in a single file, making it perfect for archiving or offline viewing.\n  * **`wait_for_images`**: If`True` , attempts to wait until images are fully loaded before final extraction.\n\n\n#### Example: Capturing Page as MHTML\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        capture_mhtml=True  # Enable MHTML capture\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=crawler_cfg)\n\n        if result.success and result.mhtml:\n            # Save the MHTML snapshot to a file\n            with open(\"example.mhtml\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.mhtml)\n            print(\"MHTML snapshot saved to example.mhtml\")\n        else:\n            print(\"Failed to capture MHTML:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\nThe MHTML format is particularly useful because: - It captures the complete page state including all resources - It can be opened in most modern browsers for offline viewing - It preserves the page exactly as it appeared during crawling - It's a single file, making it easy to store and transfer\n* * *\n## 5. Putting It All Together: Link & Media Filtering\nHereâ€™s a combined example demonstrating how to filter out external links, skip certain domains, and exclude external images:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # Suppose we want to keep only internal links, remove certain domains, \n    # and discard external images from the final crawl data.\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,\n        exclude_domains=[\"spammyads.com\"],\n        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.\n        exclude_external_images=True,      # keep only images from main domain\n        wait_for_images=True,             # ensure images are loaded\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://www.example.com\", config=crawler_cfg)\n\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n\n            # 1. Links\n            in_links = result.links.get(\"internal\", [])\n            ext_links = result.links.get(\"external\", [])\n            print(\"Internal link count:\", len(in_links))\n            print(\"External link count:\", len(ext_links))  # should be zero with exclude_external_links=True\n\n            # 2. Images\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n\n            # Let's see a snippet of these images\n            for i, img in enumerate(images[:3]):\n                print(f\"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})\")\n        else:\n            print(\"[ERROR] Failed to crawl. Reason:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## 6. Common Pitfalls & Tips\n1. **Conflicting Flags** :  \n- `exclude_external_links=True` but then also specifying `exclude_social_media_links=True` is typically fine, but understand that the first setting already discards _all_ external links. The second becomes somewhat redundant.  \n- `exclude_external_images=True` but want to keep some external images? Currently no partial domain-based setting for images, so you might need a custom approach or hook logic.\n2. **Relevancy Scores** :  \n- If your version of Crawl4AI or your scraping strategy includes an `img[\"score\"]`, itâ€™s typically a heuristic based on size, position, or content analysis. Evaluate carefully if you rely on it.\n3. **Performance** :  \n- Excluding certain domains or external images can speed up your crawl, especially for large, media-heavy pages.  \n- If you want a â€œfullâ€ link map, do _not_ exclude them. Instead, you can post-filter in your own code.\n4. **Social Media Lists** :  \n- `exclude_social_media_links=True` typically references an internal list of known social domains like Facebook, Twitter, LinkedIn, etc. If you need to add or remove from that list, look for library settings or a local config file (depending on your version).\n* * *\n**Thatâ€™s it for Link & Media Analysis!** Youâ€™re now equipped to filter out unwanted sites and zero in on the images and videos that matter for your project.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/link-media/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/link-media/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/link-media/)\n\n\nESC to close\n#### On this page\n  * [Excluding External Images](https://docs.crawl4ai.com/core/link-media/#excluding-external-images)\n  * [Excluding All Images](https://docs.crawl4ai.com/core/link-media/#excluding-all-images)\n  * [1. Link Extraction](https://docs.crawl4ai.com/core/link-media/#1-link-extraction)\n  * [1.1 result.links](https://docs.crawl4ai.com/core/link-media/#11-resultlinks)\n  * [2. Advanced Link Head Extraction & Scoring](https://docs.crawl4ai.com/core/link-media/#2-advanced-link-head-extraction-scoring)\n  * [2.1 Why Link Head Extraction?](https://docs.crawl4ai.com/core/link-media/#21-why-link-head-extraction)\n  * [2.2 Complete Working Example](https://docs.crawl4ai.com/core/link-media/#22-complete-working-example)\n  * [2.3 Configuration Deep Dive](https://docs.crawl4ai.com/core/link-media/#23-configuration-deep-dive)\n  * [2.4 Understanding the Three Score Types](https://docs.crawl4ai.com/core/link-media/#24-understanding-the-three-score-types)\n  * [1. Intrinsic Score (0-10) - URL and Content Quality](https://docs.crawl4ai.com/core/link-media/#1-intrinsic-score-0-10-url-and-content-quality)\n  * [2. Contextual Score (0-1) - BM25 Relevance to Query](https://docs.crawl4ai.com/core/link-media/#2-contextual-score-0-1-bm25-relevance-to-query)\n  * [3. Total Score - Smart Combination](https://docs.crawl4ai.com/core/link-media/#3-total-score-smart-combination)\n  * [2.5 Practical Use Cases](https://docs.crawl4ai.com/core/link-media/#25-practical-use-cases)\n  * [Use Case 1: Research Assistant](https://docs.crawl4ai.com/core/link-media/#use-case-1-research-assistant)\n  * [Use Case 2: Content Discovery](https://docs.crawl4ai.com/core/link-media/#use-case-2-content-discovery)\n  * [Use Case 3: Link Quality Analysis](https://docs.crawl4ai.com/core/link-media/#use-case-3-link-quality-analysis)\n  * [2.6 Performance Tips](https://docs.crawl4ai.com/core/link-media/#26-performance-tips)\n  * [2.7 Troubleshooting](https://docs.crawl4ai.com/core/link-media/#27-troubleshooting)\n  * [3. Domain Filtering](https://docs.crawl4ai.com/core/link-media/#3-domain-filtering)\n  * [3.1 Example: Excluding External & Social Media Links](https://docs.crawl4ai.com/core/link-media/#31-example-excluding-external-social-media-links)\n  * [3.2 Example: Excluding Specific Domains](https://docs.crawl4ai.com/core/link-media/#32-example-excluding-specific-domains)\n  * [4. Media Extraction](https://docs.crawl4ai.com/core/link-media/#4-media-extraction)\n  * [4.1 Accessing result.media](https://docs.crawl4ai.com/core/link-media/#41-accessing-resultmedia)\n  * [4.2 Excluding External Images](https://docs.crawl4ai.com/core/link-media/#42-excluding-external-images)\n  * [4.3 Additional Media Config](https://docs.crawl4ai.com/core/link-media/#43-additional-media-config)\n  * [Example: Capturing Page as MHTML](https://docs.crawl4ai.com/core/link-media/#example-capturing-page-as-mhtml)\n  * [5. Putting It All Together: Link & Media Filtering](https://docs.crawl4ai.com/core/link-media/#5-putting-it-all-together-link-media-filtering)\n  * [6. Common Pitfalls & Tips](https://docs.crawl4ai.com/core/link-media/#6-common-pitfalls-tips)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/local-files",
    "depth": 1,
    "title": "Local Files & Raw HTML - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "9f85d1c240ac595d837583f146a37a41",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/local-files/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * Local Files & Raw HTML\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Prefix-Based Input Handling in Crawl4AI](https://docs.crawl4ai.com/core/local-files/#prefix-based-input-handling-in-crawl4ai)\n  * [Crawling a Web URL](https://docs.crawl4ai.com/core/local-files/#crawling-a-web-url)\n  * [Crawling a Local HTML File](https://docs.crawl4ai.com/core/local-files/#crawling-a-local-html-file)\n  * [Crawling Raw HTML Content](https://docs.crawl4ai.com/core/local-files/#crawling-raw-html-content)\n  * [Complete Example](https://docs.crawl4ai.com/core/local-files/#complete-example)\n  * [Conclusion](https://docs.crawl4ai.com/core/local-files/#conclusion)\n\n\n# Prefix-Based Input Handling in Crawl4AI\nThis guide will walk you through using the Crawl4AI library to crawl web pages, local HTML files, and raw HTML strings. We'll demonstrate these capabilities using a Wikipedia page as an example.\n## Crawling a Web URL\nTo crawl a live web page, provide the URL starting with `http://` or `https://`, using a `CrawlerRunConfig` object:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig\n\nasync def crawl_web():\n    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/apple\", \n            config=config\n        )\n        if result.success:\n            print(\"Markdown Content:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl: {result.error_message}\")\n\nasyncio.run(crawl_web())\nCopy\n```\n\n## Crawling a Local HTML File\nTo crawl a local HTML file, prefix the file path with `file://`.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig\n\nasync def crawl_local_file():\n    local_file_path = \"/path/to/apple.html\"  # Replace with your file path\n    file_url = f\"file://{local_file_path}\"\n    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=file_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Local File:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl local file: {result.error_message}\")\n\nasyncio.run(crawl_local_file())\nCopy\n```\n\n## Crawling Raw HTML Content\nTo crawl raw HTML content, prefix the HTML string with `raw:`.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_raw_html():\n    raw_html = \"<html><body><h1>Hello, World!</h1></body></html>\"\n    raw_html_url = f\"raw:{raw_html}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=raw_html_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Raw HTML:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl raw HTML: {result.error_message}\")\n\nasyncio.run(crawl_raw_html())\nCopy\n```\n\n* * *\n# Complete Example\nBelow is a comprehensive script that:\n  1. Crawls the Wikipedia page for \"Apple.\"\n  2. Saves the HTML content to a local file (`apple.html`).\n  3. Crawls the local HTML file and verifies the markdown length matches the original crawl.\n  4. Crawls the raw HTML content from the saved file and verifies consistency.\n\n\n```\nimport os\nimport sys\nimport asyncio\nfrom pathlib import Path\nfrom crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig\n\nasync def main():\n    wikipedia_url = \"https://en.wikipedia.org/wiki/apple\"\n    script_dir = Path(__file__).parent\n    html_file_path = script_dir / \"apple.html\"\n\n    async with AsyncWebCrawler() as crawler:\n        # Step 1: Crawl the Web URL\n        print(\"\\n=== Step 1: Crawling the Wikipedia URL ===\")\n        web_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n        result = await crawler.arun(url=wikipedia_url, config=web_config)\n\n        if not result.success:\n            print(f\"Failed to crawl {wikipedia_url}: {result.error_message}\")\n            return\n\n        with open(html_file_path, 'w', encoding='utf-8') as f:\n            f.write(result.html)\n        web_crawl_length = len(result.markdown)\n        print(f\"Length of markdown from web crawl: {web_crawl_length}\\n\")\n\n        # Step 2: Crawl from the Local HTML File\n        print(\"=== Step 2: Crawling from the Local HTML File ===\")\n        file_url = f\"file://{html_file_path.resolve()}\"\n        file_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n        local_result = await crawler.arun(url=file_url, config=file_config)\n\n        if not local_result.success:\n            print(f\"Failed to crawl local file {file_url}: {local_result.error_message}\")\n            return\n\n        local_crawl_length = len(local_result.markdown)\n        assert web_crawl_length == local_crawl_length, \"Markdown length mismatch\"\n        print(\"âœ… Markdown length matches between web and local file crawl.\\n\")\n\n        # Step 3: Crawl Using Raw HTML Content\n        print(\"=== Step 3: Crawling Using Raw HTML Content ===\")\n        with open(html_file_path, 'r', encoding='utf-8') as f:\n            raw_html_content = f.read()\n        raw_html_url = f\"raw:{raw_html_content}\"\n        raw_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n        raw_result = await crawler.arun(url=raw_html_url, config=raw_config)\n\n        if not raw_result.success:\n            print(f\"Failed to crawl raw HTML content: {raw_result.error_message}\")\n            return\n\n        raw_crawl_length = len(raw_result.markdown)\n        assert web_crawl_length == raw_crawl_length, \"Markdown length mismatch\"\n        print(\"âœ… Markdown length matches between web and raw HTML crawl.\\n\")\n\n        print(\"All tests passed successfully!\")\n    if html_file_path.exists():\n        os.remove(html_file_path)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n# Conclusion\nWith the unified `url` parameter and prefix-based handling in **Crawl4AI** , you can seamlessly handle web URLs, local HTML files, and raw HTML content. Use `CrawlerRunConfig` for flexible and consistent configuration in all scenarios.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/local-files/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/local-files/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/local-files/)\n\n\nESC to close\n#### On this page\n  * [Crawling a Web URL](https://docs.crawl4ai.com/core/local-files/#crawling-a-web-url)\n  * [Crawling a Local HTML File](https://docs.crawl4ai.com/core/local-files/#crawling-a-local-html-file)\n  * [Crawling Raw HTML Content](https://docs.crawl4ai.com/core/local-files/#crawling-raw-html-content)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/markdown-generation",
    "depth": 1,
    "title": "Markdown Generation - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "3fc2e8ff7a299abeadc9349c46dc1cb7",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/markdown-generation/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * Markdown Generation\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Markdown Generation Basics](https://docs.crawl4ai.com/core/markdown-generation/#markdown-generation-basics)\n  * [1. Quick Example](https://docs.crawl4ai.com/core/markdown-generation/#1-quick-example)\n  * [2. How Markdown Generation Works](https://docs.crawl4ai.com/core/markdown-generation/#2-how-markdown-generation-works)\n  * [3. Configuring the Default Markdown Generator](https://docs.crawl4ai.com/core/markdown-generation/#3-configuring-the-default-markdown-generator)\n  * [4. Selecting the HTML Source for Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/#4-selecting-the-html-source-for-markdown-generation)\n  * [5. Content Filters](https://docs.crawl4ai.com/core/markdown-generation/#5-content-filters)\n  * [6. Using Fit Markdown](https://docs.crawl4ai.com/core/markdown-generation/#6-using-fit-markdown)\n  * [7. The MarkdownGenerationResult Object](https://docs.crawl4ai.com/core/markdown-generation/#7-the-markdowngenerationresult-object)\n  * [8. Combining Filters (BM25 + Pruning) in Two Passes](https://docs.crawl4ai.com/core/markdown-generation/#8-combining-filters-bm25-pruning-in-two-passes)\n  * [9. Common Pitfalls & Tips](https://docs.crawl4ai.com/core/markdown-generation/#9-common-pitfalls-tips)\n  * [10. Summary & Next Steps](https://docs.crawl4ai.com/core/markdown-generation/#10-summary-next-steps)\n\n\n# Markdown Generation Basics\nOne of Crawl4AIâ€™s core features is generating **clean, structured markdown** from web pages. Originally built to solve the problem of extracting only the â€œactualâ€ content and discarding boilerplate or noise, Crawl4AIâ€™s markdown system remains one of its biggest draws for AI workflows.\nIn this tutorial, youâ€™ll learn:\n  1. How to configure the **Default Markdown Generator**\n  2. How **content filters** (BM25 or Pruning) help you refine markdown and discard junk \n  3. The difference between raw markdown (`result.markdown`) and filtered markdown (`fit_markdown`) \n\n\n> **Prerequisites**  \n>  - Youâ€™ve completed or read [AsyncWebCrawler Basics](https://docs.crawl4ai.com/core/simple-crawling/) to understand how to run a simple crawl.  \n>  - You know how to configure `CrawlerRunConfig`.\n* * *\n## 1. Quick Example\nHereâ€™s a minimal code snippet that uses the **DefaultMarkdownGenerator** with no additional filtering:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator()\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n\n        if result.success:\n            print(\"Raw Markdown Output:\\n\")\n            print(result.markdown)  # The unfiltered markdown from the page\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Whatâ€™s happening?**  \n- `CrawlerRunConfig( markdown_generator = DefaultMarkdownGenerator() )` instructs Crawl4AI to convert the final HTML into markdown at the end of each crawl.  \n- The resulting markdown is accessible via `result.markdown`.\n* * *\n## 2. How Markdown Generation Works\n### 2.1 HTML-to-Text Conversion (Forked & Modified)\nUnder the hood, **DefaultMarkdownGenerator** uses a specialized HTML-to-text approach that:\n  * Preserves headings, code blocks, bullet points, etc. \n  * Removes extraneous tags (scripts, styles) that donâ€™t add meaningful content. \n  * Can optionally generate references for links or skip them altogether.\n\n\nA set of **options** (passed as a dict) allows you to customize precisely how HTML converts to markdown. These map to standard html2text-like configuration plus your own enhancements (e.g., ignoring internal links, preserving certain tags verbatim, or adjusting line widths).\n### 2.2 Link Citations & References\nBy default, the generator can convert `<a href=\"...\">` elements into `[text][1]` citations, then place the actual links at the bottom of the document. This is handy for research workflows that demand references in a structured manner.\n### 2.3 Optional Content Filters\nBefore or after the HTML-to-Markdown step, you can apply a **content filter** (like BM25 or Pruning) to reduce noise and produce a â€œfit_markdownâ€â€”a heavily pruned version focusing on the pageâ€™s main text. Weâ€™ll cover these filters shortly.\n* * *\n## 3. Configuring the Default Markdown Generator\nYou can tweak the output by passing an `options` dict to `DefaultMarkdownGenerator`. For example:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters\n    md_generator = DefaultMarkdownGenerator(\n        options={\n            \"ignore_links\": True,\n            \"escape_html\": False,\n            \"body_width\": 80\n        }\n    )\n\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/docs\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown[:500])  # Just a snippet\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\nSome commonly used `options`:\n  * **`ignore_links`**(bool): Whether to remove all hyperlinks in the final markdown.\n  * **`ignore_images`**(bool): Remove all`![image]()` references. \n  * **`escape_html`**(bool): Turn HTML entities into text (default is often`True`). \n  * **`body_width`**(int): Wrap text at N characters.`0` or `None` means no wrapping. \n  * **`skip_internal_links`**(bool): If`True` , omit `#localAnchors` or internal links referencing the same page. \n  * **`include_sup_sub`**(bool): Attempt to handle`<sup>` / `<sub>` in a more readable way.\n\n\n## 4. Selecting the HTML Source for Markdown Generation\nThe `content_source` parameter allows you to control which HTML content is used as input for markdown generation. This gives you flexibility in how the HTML is processed before conversion to markdown.\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Option 1: Use the raw HTML directly from the webpage (before any processing)\n    raw_md_generator = DefaultMarkdownGenerator(\n        content_source=\"raw_html\",\n        options={\"ignore_links\": True}\n    )\n\n    # Option 2: Use the cleaned HTML (after scraping strategy processing - default)\n    cleaned_md_generator = DefaultMarkdownGenerator(\n        content_source=\"cleaned_html\",  # This is the default\n        options={\"ignore_links\": True}\n    )\n\n    # Option 3: Use preprocessed HTML optimized for schema extraction\n    fit_md_generator = DefaultMarkdownGenerator(\n        content_source=\"fit_html\",\n        options={\"ignore_links\": True}\n    )\n\n    # Use one of the generators in your crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=raw_md_generator  # Try each of the generators\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown.raw_markdown[:500])\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\n### HTML Source Options\n  * **`\"cleaned_html\"`**(default): Uses the HTML after it has been processed by the scraping strategy. This HTML is typically cleaner and more focused on content, with some boilerplate removed.\n  * **`\"raw_html\"`**: Uses the original HTML directly from the webpage, before any cleaning or processing. This preserves more of the original content, but may include navigation bars, ads, footers, and other elements that might not be relevant to the main content.\n  * **`\"fit_html\"`**: Uses HTML preprocessed for schema extraction. This HTML is optimized for structured data extraction and may have certain elements simplified or removed.\n\n\n### When to Use Each Option\n  * Use **`\"cleaned_html\"`**(default) for most cases where you want a balance of content preservation and noise removal.\n  * Use **`\"raw_html\"`**when you need to preserve all original content, or when the cleaning process is removing content you actually want to keep.\n  * Use **`\"fit_html\"`**when working with structured data or when you need HTML that's optimized for schema extraction.\n\n\n* * *\n## 5. Content Filters\n**Content filters** selectively remove or rank sections of text before turning them into Markdown. This is especially helpful if your page has ads, nav bars, or other clutter you donâ€™t want.\n### 5.1 BM25ContentFilter\nIf you have a **search query** , BM25 is a good choice:\n```\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai import CrawlerRunConfig\n\nbm25_filter = BM25ContentFilter(\n    user_query=\"machine learning\",\n    bm25_threshold=1.2,\n    language=\"english\"\n)\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=bm25_filter,\n    options={\"ignore_links\": True}\n)\n\nconfig = CrawlerRunConfig(markdown_generator=md_generator)\nCopy\n```\n\n  * **`user_query`**: The term you want to focus on. BM25 tries to keep only content blocks relevant to that query.\n  * **`bm25_threshold`**: Raise it to keep fewer blocks; lower it to keep more.\n  * **`use_stemming`**_(default`True`)_: Whether to apply stemming to the query and content.\n  * **`language (str)`**: Language for stemming (default: 'english').\n\n\n**No query provided?** BM25 tries to glean a context from page metadata, or you can simply treat it as a scorched-earth approach that discards text with low generic score. Realistically, you want to supply a query for best results.\n### 5.2 PruningContentFilter\nIf you **donâ€™t** have a specific query, or if you just want a robust â€œjunk remover,â€ use `PruningContentFilter`. It analyzes text density, link density, HTML structure, and known patterns (like â€œnav,â€ â€œfooterâ€) to systematically prune extraneous or repetitive sections.\n```\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",  # or \"dynamic\"\n    min_word_threshold=50\n)\nCopy\n```\n\n  * **`threshold`**: Score boundary. Blocks below this score get removed.\n  * **`threshold_type`**:\n    * `\"fixed\"`: Straight comparison (`score >= threshold` keeps the block). \n    * `\"dynamic\"`: The filter adjusts threshold in a data-driven manner. \n  * **`min_word_threshold`**: Discard blocks under N words as likely too short or unhelpful.\n\n\n**When to Use PruningContentFilter**  \n- You want a broad cleanup without a user query.  \n- The page has lots of repeated sidebars, footers, or disclaimers that hamper text extraction.\n### 5.3 LLMContentFilter\nFor intelligent content filtering and high-quality markdown generation, you can use the **LLMContentFilter**. This filter leverages LLMs to generate relevant markdown while preserving the original content's meaning and structure:\n```\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig, DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\nasync def main():\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-api-token\"), #or use environment variable\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=4096,  # Adjust based on your needs\n        verbose=True\n    )\n    md_generator = DefaultMarkdownGenerator(\n        content_filter=filter,\n        options={\"ignore_links\": True}\n    )\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        print(result.markdown.fit_markdown)  # Filtered markdown content\nCopy\n```\n\n**Key Features:** - **Intelligent Filtering** : Uses LLMs to understand and extract relevant content while maintaining context - **Customizable Instructions** : Tailor the filtering process with specific instructions - **Chunk Processing** : Handles large documents by processing them in chunks (controlled by `chunk_token_threshold`) - **Parallel Processing** : For better performance, use smaller `chunk_token_threshold` (e.g., 2048 or 4096) to enable parallel processing of content chunks\n**Two Common Use Cases:**\n  1. **Exact Content Preservation** : \n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Extract the main educational content while preserving its original wording and substance completely.\n    1. Maintain the exact language and terminology\n    2. Keep all technical explanations and examples intact\n    3. Preserve the original flow and structure\n    4. Remove only clearly irrelevant elements like navigation menus and ads\n    \"\"\",\n    chunk_token_threshold=4096\n)\nCopy\n```\n\n  2. **Focused Content Extraction** : \n```\nfilter = LLMContentFilter(\n    instruction=\"\"\"\n    Focus on extracting specific types of content:\n    - Technical documentation\n    - Code examples\n    - API references\n    Reformat the content into clear, well-structured markdown\n    \"\"\",\n    chunk_token_threshold=4096\n)\nCopy\n```\n\n\n\n> **Performance Tip** : Set a smaller `chunk_token_threshold` (e.g., 2048 or 4096) to enable parallel processing of content chunks. The default value is infinity, which processes the entire content as a single chunk.\n* * *\n## 6. Using Fit Markdown\nWhen a content filter is active, the library produces two forms of markdown inside `result.markdown`:\n1. **`raw_markdown`**: The full unfiltered markdown.  \n2. **`fit_markdown`**: A â€œfitâ€ version where the filter has removed or trimmed noisy segments.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.6),\n            options={\"ignore_links\": True}\n        )\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://news.example.com/tech\", config=config)\n        if result.success:\n            print(\"Raw markdown:\\n\", result.markdown)\n\n            # If a filter is used, we also have .fit_markdown:\n            md_object = result.markdown  # or your equivalent\n            print(\"Filtered markdown:\\n\", md_object.fit_markdown)\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## 7. The `MarkdownGenerationResult` Object\nIf your library stores detailed markdown output in an object like `MarkdownGenerationResult`, youâ€™ll see fields such as:\n  * **`raw_markdown`**: The direct HTML-to-markdown transformation (no filtering).\n  * **`markdown_with_citations`**: A version that moves links to reference-style footnotes.\n  * **`references_markdown`**: A separate string or section containing the gathered references.\n  * **`fit_markdown`**: The filtered markdown if you used a content filter.\n  * **`fit_html`**: The corresponding HTML snippet used to generate`fit_markdown` (helpful for debugging or advanced usage).\n\n\n**Example** :\n```\nmd_obj = result.markdown  # your libraryâ€™s naming may vary\nprint(\"RAW:\\n\", md_obj.raw_markdown)\nprint(\"CITED:\\n\", md_obj.markdown_with_citations)\nprint(\"REFERENCES:\\n\", md_obj.references_markdown)\nprint(\"FIT:\\n\", md_obj.fit_markdown)\nCopy\n```\n\n**Why Does This Matter?**  \n- You can supply `raw_markdown` to an LLM if you want the entire text.  \n- Or feed `fit_markdown` into a vector database to reduce token usage.  \n- `references_markdown` can help you keep track of link provenance.\n* * *\nBelow is a **revised section** under â€œCombining Filters (BM25 + Pruning)â€ that demonstrates how you can run **two** passes of content filtering without re-crawling, by taking the HTML (or text) from a first pass and feeding it into the second filter. It uses real code patterns from the snippet you provided for **BM25ContentFilter** , which directly accepts **HTML** strings (and can also handle plain text with minimal adaptation).\n* * *\n## 8. Combining Filters (BM25 + Pruning) in Two Passes\nYou might want to **prune out** noisy boilerplate first (with `PruningContentFilter`), and then **rank whatâ€™s left** against a user query (with `BM25ContentFilter`). You donâ€™t have to crawl the page twice. Instead:\n1. **First pass** : Apply `PruningContentFilter` directly to the raw HTML from `result.html` (the crawlerâ€™s downloaded HTML).  \n2. **Second pass** : Take the pruned HTML (or text) from step 1, and feed it into `BM25ContentFilter`, focusing on a user query.\n### Two-Pass Example\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom bs4 import BeautifulSoup\n\nasync def main():\n    # 1. Crawl with minimal or no markdown generator, just get raw HTML\n    config = CrawlerRunConfig(\n        # If you only want raw HTML, you can skip passing a markdown_generator\n        # or provide one but focus on .html in this example\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/tech-article\", config=config)\n\n        if not result.success or not result.html:\n            print(\"Crawl failed or no HTML content.\")\n            return\n\n        raw_html = result.html\n\n        # 2. First pass: PruningContentFilter on raw HTML\n        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)\n\n        # filter_content returns a list of \"text chunks\" or cleaned HTML sections\n        pruned_chunks = pruning_filter.filter_content(raw_html)\n        # This list is basically pruned content blocks, presumably in HTML or text form\n\n        # For demonstration, let's combine these chunks back into a single HTML-like string\n        # or you could do further processing. It's up to your pipeline design.\n        pruned_html = \"\\n\".join(pruned_chunks)\n\n        # 3. Second pass: BM25ContentFilter with a user query\n        bm25_filter = BM25ContentFilter(\n            user_query=\"machine learning\",\n            bm25_threshold=1.2,\n            language=\"english\"\n        )\n\n        # returns a list of text chunks\n        bm25_chunks = bm25_filter.filter_content(pruned_html)  \n\n        if not bm25_chunks:\n            print(\"Nothing matched the BM25 query after pruning.\")\n            return\n\n        # 4. Combine or display final results\n        final_text = \"\\n---\\n\".join(bm25_chunks)\n\n        print(\"==== PRUNED OUTPUT (first pass) ====\")\n        print(pruned_html[:500], \"... (truncated)\")  # preview\n\n        print(\"\\n==== BM25 OUTPUT (second pass) ====\")\n        print(final_text[:500], \"... (truncated)\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n### Whatâ€™s Happening?\n1. **Raw HTML** : We crawl once and store the raw HTML in `result.html`.  \n2. **PruningContentFilter** : Takes HTML + optional parameters. It extracts blocks of text or partial HTML, removing headings/sections deemed â€œnoise.â€ It returns a **list of text chunks**.  \n3. **Combine or Transform** : We join these pruned chunks back into a single HTML-like string. (Alternatively, you could store them in a list for further logicâ€”whatever suits your pipeline.)  \n4. **BM25ContentFilter** : We feed the pruned string into `BM25ContentFilter` with a user query. This second pass further narrows the content to chunks relevant to â€œmachine learning.â€\n**No Re-Crawling** : We used `raw_html` from the first pass, so thereâ€™s no need to run `arun()` againâ€”**no second network request**.\n### Tips & Variations\n  * **Plain Text vs. HTML** : If your pruned output is mostly text, BM25 can still handle it; just keep in mind it expects a valid string input. If you supply partial HTML (like `\"<p>some text</p>\"`), it will parse it as HTML. \n  * **Chaining in a Single Pipeline** : If your code supports it, you can chain multiple filters automatically. Otherwise, manual two-pass filtering (as shown) is straightforward. \n  * **Adjust Thresholds** : If you see too much or too little text in step one, tweak `threshold=0.5` or `min_word_threshold=50`. Similarly, `bm25_threshold=1.2` can be raised/lowered for more or fewer chunks in step two.\n\n\n### One-Pass Combination?\nIf your codebase or pipeline design allows applying multiple filters in one pass, you could do so. But often itâ€™s simplerâ€”and more transparentâ€”to run them sequentially, analyzing each stepâ€™s result.\n**Bottom Line** : By **manually chaining** your filtering logic in two passes, you get powerful incremental control over the final content. First, remove â€œglobalâ€ clutter with Pruning, then refine further with BM25-based query relevanceâ€”without incurring a second network crawl.\n* * *\n## 9. Common Pitfalls & Tips\n1. **No Markdown Output?**  \n- Make sure the crawler actually retrieved HTML. If the site is heavily JS-based, you may need to enable dynamic rendering or wait for elements.  \n- Check if your content filter is too aggressive. Lower thresholds or disable the filter to see if content reappears.\n2. **Performance Considerations**  \n- Very large pages with multiple filters can be slower. Consider `cache_mode` to avoid re-downloading.  \n- If your final use case is LLM ingestion, consider summarizing further or chunking big texts.\n3. **Take Advantage of`fit_markdown`**  \n- Great for RAG pipelines, semantic search, or any scenario where extraneous boilerplate is unwanted.  \n- Still verify the textual qualityâ€”some sites have crucial data in footers or sidebars.\n4. **Adjusting`html2text` Options**  \n- If you see lots of raw HTML slipping into the text, turn on `escape_html`.  \n- If code blocks look messy, experiment with `mark_code` or `handle_code_in_pre`.\n* * *\n## 10. Summary & Next Steps\nIn this **Markdown Generation Basics** tutorial, you learned to:\n  * Configure the **DefaultMarkdownGenerator** with HTML-to-text options. \n  * Select different HTML sources using the `content_source` parameter. \n  * Use **BM25ContentFilter** for query-specific extraction or **PruningContentFilter** for general noise removal. \n  * Distinguish between raw and filtered markdown (`fit_markdown`). \n  * Leverage the `MarkdownGenerationResult` object to handle different forms of output (citations, references, etc.).\n\n\nNow you can produce high-quality Markdown from any website, focusing on exactly the content you needâ€”an essential step for powering AI models, summarization pipelines, or knowledge-base queries.\n**Last Updated** : 2025-01-01\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/markdown-generation/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/markdown-generation/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/markdown-generation/)\n\n\nESC to close\n#### On this page\n  * [1. Quick Example](https://docs.crawl4ai.com/core/markdown-generation/#1-quick-example)\n  * [2. How Markdown Generation Works](https://docs.crawl4ai.com/core/markdown-generation/#2-how-markdown-generation-works)\n  * [2.1 HTML-to-Text Conversion (Forked & Modified)](https://docs.crawl4ai.com/core/markdown-generation/#21-html-to-text-conversion-forked-modified)\n  * [2.2 Link Citations & References](https://docs.crawl4ai.com/core/markdown-generation/#22-link-citations-references)\n  * [2.3 Optional Content Filters](https://docs.crawl4ai.com/core/markdown-generation/#23-optional-content-filters)\n  * [3. Configuring the Default Markdown Generator](https://docs.crawl4ai.com/core/markdown-generation/#3-configuring-the-default-markdown-generator)\n  * [4. Selecting the HTML Source for Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/#4-selecting-the-html-source-for-markdown-generation)\n  * [HTML Source Options](https://docs.crawl4ai.com/core/markdown-generation/#html-source-options)\n  * [When to Use Each Option](https://docs.crawl4ai.com/core/markdown-generation/#when-to-use-each-option)\n  * [5. Content Filters](https://docs.crawl4ai.com/core/markdown-generation/#5-content-filters)\n  * [5.1 BM25ContentFilter](https://docs.crawl4ai.com/core/markdown-generation/#51-bm25contentfilter)\n  * [5.2 PruningContentFilter](https://docs.crawl4ai.com/core/markdown-generation/#52-pruningcontentfilter)\n  * [5.3 LLMContentFilter](https://docs.crawl4ai.com/core/markdown-generation/#53-llmcontentfilter)\n  * [6. Using Fit Markdown](https://docs.crawl4ai.com/core/markdown-generation/#6-using-fit-markdown)\n  * [7. The MarkdownGenerationResult Object](https://docs.crawl4ai.com/core/markdown-generation/#7-the-markdowngenerationresult-object)\n  * [8. Combining Filters (BM25 + Pruning) in Two Passes](https://docs.crawl4ai.com/core/markdown-generation/#8-combining-filters-bm25-pruning-in-two-passes)\n  * [Two-Pass Example](https://docs.crawl4ai.com/core/markdown-generation/#two-pass-example)\n  * [Whatâ€™s Happening?](https://docs.crawl4ai.com/core/markdown-generation/#whats-happening)\n  * [Tips & Variations](https://docs.crawl4ai.com/core/markdown-generation/#tips-variations)\n  * [One-Pass Combination?](https://docs.crawl4ai.com/core/markdown-generation/#one-pass-combination)\n  * [9. Common Pitfalls & Tips](https://docs.crawl4ai.com/core/markdown-generation/#9-common-pitfalls-tips)\n  * [10. Summary & Next Steps](https://docs.crawl4ai.com/core/markdown-generation/#10-summary-next-steps)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/page-interaction",
    "depth": 1,
    "title": "Page Interaction - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "464caf7a879f666bc61ee9ec13c32eb1",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/page-interaction/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * Page Interaction\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/#page-interaction)\n  * [1. JavaScript Execution](https://docs.crawl4ai.com/core/page-interaction/#1-javascript-execution)\n  * [2. Wait Conditions](https://docs.crawl4ai.com/core/page-interaction/#2-wait-conditions)\n  * [3. Handling Dynamic Content](https://docs.crawl4ai.com/core/page-interaction/#3-handling-dynamic-content)\n  * [4. Timing Control](https://docs.crawl4ai.com/core/page-interaction/#4-timing-control)\n  * [5. Multi-Step Interaction Example](https://docs.crawl4ai.com/core/page-interaction/#5-multi-step-interaction-example)\n  * [6. Combine Interaction with Extraction](https://docs.crawl4ai.com/core/page-interaction/#6-combine-interaction-with-extraction)\n  * [7. Relevant CrawlerRunConfig Parameters](https://docs.crawl4ai.com/core/page-interaction/#7-relevant-crawlerrunconfig-parameters)\n  * [8. Conclusion](https://docs.crawl4ai.com/core/page-interaction/#8-conclusion)\n  * [9. Virtual Scrolling](https://docs.crawl4ai.com/core/page-interaction/#9-virtual-scrolling)\n\n\n# Page Interaction\nCrawl4AI provides powerful features for interacting with **dynamic** webpages, handling JavaScript execution, waiting for conditions, and managing multi-step flows. By combining **js_code** , **wait_for** , and certain **CrawlerRunConfig** parameters, you can:\n  1. Click â€œLoad Moreâ€ buttons \n  2. Fill forms and submit them \n  3. Wait for elements or data to appear \n  4. Reuse sessions across multiple steps \n\n\nBelow is a quick overview of how to do it.\n* * *\n## 1. JavaScript Execution\n### Basic Execution\n**`js_code`**in**`CrawlerRunConfig`**accepts either a single JS string or a list of JS snippets.  \n**Example** : Weâ€™ll scroll to the bottom of the page, then optionally click a â€œLoad Moreâ€ button.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Single JS command\n    config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Example site\n            config=config\n        )\n        print(\"Crawled length:\", len(result.cleaned_html))\n\n    # Multiple commands\n    js_commands = [\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # 'More' link on Hacker News\n        \"document.querySelector('a.morelink')?.click();\",  \n    ]\n    config = CrawlerRunConfig(js_code=js_commands)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Another pass\n            config=config\n        )\n        print(\"After scroll+click, length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Relevant`CrawlerRunConfig` params**: - **`js_code`**: A string or list of strings with JavaScript to run after the page loads. -**`js_only`**: If set to`True` on subsequent calls, indicates weâ€™re continuing an existing session without a new full navigation.  \n- **`session_id`**: If you want to keep the same page across multiple calls, specify an ID.\n* * *\n## 2. Wait Conditions\n### 2.1 CSS-Based Waiting\nSometimes, you just want to wait for a specific element to appear. For example:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Wait for at least 30 items on Hacker News\n        wait_for=\"css:.athing:nth-child(30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"We have at least 30 items loaded!\")\n        # Rough check\n        print(\"Total items in HTML:\", result.cleaned_html.count(\"athing\"))  \n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key param** : - **`wait_for=\"css:...\"`**: Tells the crawler to wait until that CSS selector is present.\n### 2.2 JavaScript-Based Waiting\nFor more complex conditions (e.g., waiting for content length to exceed a threshold), prefix `js:`:\n```\nwait_condition = \"\"\"() => {\n    const items = document.querySelectorAll('.athing');\n    return items.length > 50;  // Wait for at least 51 items\n}\"\"\"\n\nconfig = CrawlerRunConfig(wait_for=f\"js:{wait_condition}\")\nCopy\n```\n\n**Behind the Scenes** : Crawl4AI keeps polling the JS function until it returns `true` or a timeout occurs.\n* * *\n## 3. Handling Dynamic Content\nMany modern sites require **multiple steps** : scrolling, clicking â€œLoad More,â€ or updating via JavaScript. Below are typical patterns.\n### 3.1 Load More Example (Hacker News â€œMoreâ€ Link)\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Step 1: Load initial Hacker News page\n    config = CrawlerRunConfig(\n        wait_for=\"css:.athing:nth-child(30)\"  # Wait for 30 items\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"Initial items loaded.\")\n\n        # Step 2: Let's scroll and click the \"More\" link\n        load_more_js = [\n            \"window.scrollTo(0, document.body.scrollHeight);\",\n            # The \"More\" link at page bottom\n            \"document.querySelector('a.morelink')?.click();\"  \n        ]\n\n        next_page_conf = CrawlerRunConfig(\n            js_code=load_more_js,\n            wait_for=\"\"\"js:() => {\n                return document.querySelectorAll('.athing').length > 30;\n            }\"\"\",\n            # Mark that we do not re-navigate, but run JS in the same session:\n            js_only=True,\n            session_id=\"hn_session\"\n        )\n\n        # Re-use the same crawler session\n        result2 = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # same URL but continuing session\n            config=next_page_conf\n        )\n        total_items = result2.cleaned_html.count(\"athing\")\n        print(\"Items after load-more:\", total_items)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key params** : - **`session_id=\"hn_session\"`**: Keep the same page across multiple calls to`arun()`. - **`js_only=True`**: Weâ€™re not performing a full reload, just applying JS in the existing page. -**`wait_for`**with`js:` : Wait for item count to grow beyond 30.\n* * *\n### 3.2 Form Interaction\nIf the site has a search or login form, you can fill fields and submit them with **`js_code`**. For instance, if GitHub had a local search form:\n```\njs_form_interaction = \"\"\"\ndocument.querySelector('#your-search').value = 'TypeScript commits';\ndocument.querySelector('form').submit();\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    js_code=js_form_interaction,\n    wait_for=\"css:.commit\"\n)\nresult = await crawler.arun(url=\"https://github.com/search\", config=config)\nCopy\n```\n\n**In reality** : Replace IDs or classes with the real siteâ€™s form selectors.\n* * *\n## 4. Timing Control\n1. **`page_timeout`**(ms): Overall page load or script execution time limit.  \n2. **`delay_before_return_html`**(seconds): Wait an extra moment before capturing the final HTML.  \n3. **`mean_delay`** & **`max_range`**: If you call`arun_many()` with multiple URLs, these add a random pause between each request.\n**Example** :\n```\nconfig = CrawlerRunConfig(\n    page_timeout=60000,  # 60s limit\n    delay_before_return_html=2.5\n)\nCopy\n```\n\n* * *\n## 5. Multi-Step Interaction Example\nBelow is a simplified script that does multiple â€œLoad Moreâ€ clicks on GitHubâ€™s TypeScript commits page. It **re-uses** the same session to accumulate new commits each time. The code includes the relevant **`CrawlerRunConfig`**parameters youâ€™d rely on.\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def multi_page_commits():\n    browser_cfg = BrowserConfig(\n        headless=False,  # Visible for demonstration\n        verbose=True\n    )\n    session_id = \"github_ts_commits\"\n\n    base_wait = \"\"\"js:() => {\n        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n        return commits.length > 0;\n    }\"\"\"\n\n    # Step 1: Load initial commits\n    config1 = CrawlerRunConfig(\n        wait_for=base_wait,\n        session_id=session_id,\n        cache_mode=CacheMode.BYPASS,\n        # Not using js_only yet since it's our first load\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://github.com/microsoft/TypeScript/commits/main\",\n            config=config1\n        )\n        print(\"Initial commits loaded. Count:\", result.cleaned_html.count(\"commit\"))\n\n        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists\n        js_next_page = \"\"\"\n        const selector = 'a[data-testid=\"pagination-next-button\"]';\n        const button = document.querySelector(selector);\n        if (button) button.click();\n        \"\"\"\n\n        # Wait until new commits appear\n        wait_for_more = \"\"\"js:() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (!window.firstCommit && commits.length>0) {\n                window.firstCommit = commits[0].textContent;\n                return false;\n            }\n            // If top commit changes, we have new commits\n            const topNow = commits[0]?.textContent.trim();\n            return topNow && topNow !== window.firstCommit;\n        }\"\"\"\n\n        for page in range(2):  # let's do 2 more \"Next\" pages\n            config_next = CrawlerRunConfig(\n                session_id=session_id,\n                js_code=js_next_page,\n                wait_for=wait_for_more,\n                js_only=True,       # We're continuing from the open tab\n                cache_mode=CacheMode.BYPASS\n            )\n            result2 = await crawler.arun(\n                url=\"https://github.com/microsoft/TypeScript/commits/main\",\n                config=config_next\n            )\n            print(f\"Page {page+2} commits count:\", result2.cleaned_html.count(\"commit\"))\n\n        # Optionally kill session\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasync def main():\n    await multi_page_commits()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Points** :\n  * **`session_id`**: Keep the same page open.\n  * **`js_code`**+**`wait_for`**+**`js_only=True`**: We do partial refreshes, waiting for new commits to appear.\n  * **`cache_mode=CacheMode.BYPASS`**ensures we always see fresh data each step.\n\n\n* * *\n## 6. Combine Interaction with Extraction\nOnce dynamic content is loaded, you can attach an **`extraction_strategy`**(like`JsonCssExtractionStrategy` or `LLMExtractionStrategy`). For example:\n```\nfrom crawl4ai import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Commits\",\n    \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"}\n    ]\n}\nconfig = CrawlerRunConfig(\n    session_id=\"ts_commits_session\",\n    js_code=js_next_page,\n    wait_for=wait_for_more,\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)\nCopy\n```\n\nWhen done, check `result.extracted_content` for the JSON.\n* * *\n## 7. Relevant `CrawlerRunConfig` Parameters\nBelow are the key interaction-related parameters in `CrawlerRunConfig`. For a full list, see [Configuration Parameters](https://docs.crawl4ai.com/api/parameters/).\n  * **`js_code`**: JavaScript to run after initial load.\n  * **`js_only`**: If`True` , no new page navigationâ€”only JS in the existing session. \n  * **`wait_for`**: CSS (`\"css:...\"`) or JS (`\"js:...\"`) expression to wait for. \n  * **`session_id`**: Reuse the same page across calls.\n  * **`cache_mode`**: Whether to read/write from the cache or bypass.\n  * **`remove_overlay_elements`**: Remove certain popups automatically.\n  * **`simulate_user`,`override_navigator` , `magic`**: Anti-bot or â€œhuman-likeâ€ interactions.\n\n\n* * *\n## 8. Conclusion\nCrawl4AIâ€™s **page interaction** features let you:\n1. **Execute JavaScript** for scrolling, clicks, or form filling.  \n2. **Wait** for CSS or custom JS conditions before capturing data.  \n3. **Handle** multi-step flows (like â€œLoad Moreâ€) with partial reloads or persistent sessions.  \n4. Combine with **structured extraction** for dynamic sites.\nWith these tools, you can scrape modern, interactive webpages confidently. For advanced hooking, user simulation, or in-depth config, check the [API reference](https://docs.crawl4ai.com/api/parameters/) or related advanced docs. Happy scripting!\n* * *\n## 9. Virtual Scrolling\nFor sites that use **virtual scrolling** (where content is replaced rather than appended as you scroll, like Twitter or Instagram), Crawl4AI provides a dedicated `VirtualScrollConfig`:\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, VirtualScrollConfig\n\nasync def crawl_twitter_timeline():\n    # Configure virtual scroll for Twitter-like feeds\n    virtual_config = VirtualScrollConfig(\n        container_selector=\"[data-testid='primaryColumn']\",  # Twitter's main column\n        scroll_count=30,                # Scroll 30 times\n        scroll_by=\"container_height\",   # Scroll by container height each time\n        wait_after_scroll=1.0          # Wait 1 second after each scroll\n    )\n\n    config = CrawlerRunConfig(\n        virtual_scroll_config=virtual_config\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://twitter.com/search?q=AI\",\n            config=config\n        )\n        # result.html now contains ALL tweets from the virtual scroll\nCopy\n```\n\n### Virtual Scroll vs JavaScript Scrolling\nFeature | Virtual Scroll | JS Code Scrolling  \n---|---|---  \n**Use Case** | Content replaced during scroll | Content appended or simple scroll  \n**Configuration** |  `VirtualScrollConfig` object |  `js_code` with scroll commands  \n**Automatic Merging** | Yes - merges all unique content | No - captures final state only  \n**Best For** | Twitter, Instagram, virtual tables | Traditional pages, load more buttons  \nFor detailed examples and configuration options, see the [Virtual Scroll documentation](https://docs.crawl4ai.com/advanced/virtual-scroll/).\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/page-interaction/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/page-interaction/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/page-interaction/)\n\n\nESC to close\n#### On this page\n  * [1. JavaScript Execution](https://docs.crawl4ai.com/core/page-interaction/#1-javascript-execution)\n  * [Basic Execution](https://docs.crawl4ai.com/core/page-interaction/#basic-execution)\n  * [2. Wait Conditions](https://docs.crawl4ai.com/core/page-interaction/#2-wait-conditions)\n  * [2.1 CSS-Based Waiting](https://docs.crawl4ai.com/core/page-interaction/#21-css-based-waiting)\n  * [2.2 JavaScript-Based Waiting](https://docs.crawl4ai.com/core/page-interaction/#22-javascript-based-waiting)\n  * [3. Handling Dynamic Content](https://docs.crawl4ai.com/core/page-interaction/#3-handling-dynamic-content)\n  * [3.1 Load More Example (Hacker News â€œMoreâ€ Link)](https://docs.crawl4ai.com/core/page-interaction/#31-load-more-example-hacker-news-more-link)\n  * [3.2 Form Interaction](https://docs.crawl4ai.com/core/page-interaction/#32-form-interaction)\n  * [4. Timing Control](https://docs.crawl4ai.com/core/page-interaction/#4-timing-control)\n  * [5. Multi-Step Interaction Example](https://docs.crawl4ai.com/core/page-interaction/#5-multi-step-interaction-example)\n  * [6. Combine Interaction with Extraction](https://docs.crawl4ai.com/core/page-interaction/#6-combine-interaction-with-extraction)\n  * [7. Relevant CrawlerRunConfig Parameters](https://docs.crawl4ai.com/core/page-interaction/#7-relevant-crawlerrunconfig-parameters)\n  * [8. Conclusion](https://docs.crawl4ai.com/core/page-interaction/#8-conclusion)\n  * [9. Virtual Scrolling](https://docs.crawl4ai.com/core/page-interaction/#9-virtual-scrolling)\n  * [Virtual Scroll vs JavaScript Scrolling](https://docs.crawl4ai.com/core/page-interaction/#virtual-scroll-vs-javascript-scrolling)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/quickstart",
    "depth": 1,
    "title": "Quick Start - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "7a7dd0a67defda9e73d12a66d371bef5",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/quickstart/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * Quick Start\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Getting Started with Crawl4AI](https://docs.crawl4ai.com/core/quickstart/#getting-started-with-crawl4ai)\n  * [1. Introduction](https://docs.crawl4ai.com/core/quickstart/#1-introduction)\n  * [2. Your First Crawl](https://docs.crawl4ai.com/core/quickstart/#2-your-first-crawl)\n  * [3. Basic Configuration (Light Introduction)](https://docs.crawl4ai.com/core/quickstart/#3-basic-configuration-light-introduction)\n  * [4. Generating Markdown Output](https://docs.crawl4ai.com/core/quickstart/#4-generating-markdown-output)\n  * [5. Simple Data Extraction (CSS-based)](https://docs.crawl4ai.com/core/quickstart/#5-simple-data-extraction-css-based)\n  * [6. Simple Data Extraction (LLM-based)](https://docs.crawl4ai.com/core/quickstart/#6-simple-data-extraction-llm-based)\n  * [7. Adaptive Crawling (New!)](https://docs.crawl4ai.com/core/quickstart/#7-adaptive-crawling-new)\n  * [8. Multi-URL Concurrency (Preview)](https://docs.crawl4ai.com/core/quickstart/#8-multi-url-concurrency-preview)\n  * [8. Dynamic Content Example](https://docs.crawl4ai.com/core/quickstart/#8-dynamic-content-example)\n  * [9. Next Steps](https://docs.crawl4ai.com/core/quickstart/#9-next-steps)\n\n\n# Getting Started with Crawl4AI\nWelcome to **Crawl4AI** , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, youâ€™ll:\n  1. Run your **first crawl** using minimal configuration. \n  2. Generate **Markdown** output (and learn how itâ€™s influenced by content filters). \n  3. Experiment with a simple **CSS-based extraction** strategy. \n  4. See a glimpse of **LLM-based extraction** (including open-source and closed-source model options). \n  5. Crawl a **dynamic** page that loads content via JavaScript.\n\n\n* * *\n## 1. Introduction\nCrawl4AI provides:\n  * An asynchronous crawler, **`AsyncWebCrawler`**.\n  * Configurable browser and run settings via **`BrowserConfig`**and**`CrawlerRunConfig`**.\n  * Automatic HTML-to-Markdown conversion via **`DefaultMarkdownGenerator`**(supports optional filters).\n  * Multiple extraction strategies (LLM-based or â€œtraditionalâ€ CSS/XPath-based).\n\n\nBy the end of this guide, youâ€™ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses â€œLoad Moreâ€ buttons or JavaScript updates.\n* * *\n## 2. Your First Crawl\nHereâ€™s a minimal Python script that creates an **`AsyncWebCrawler`**, fetches a webpage, and prints the first 300 characters of its Markdown output:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(result.markdown[:300])  # Print first 300 chars\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Whatâ€™s happening?** - **`AsyncWebCrawler`**launches a headless browser (Chromium by default). - It fetches`https://example.com`. - Crawl4AI automatically converts the HTML into Markdown.\nYou now have a simple, working crawl!\n* * *\n## 3. Basic Configuration (Light Introduction)\nCrawl4AIâ€™s crawler can be heavily customized using two main classes:\n1. **`BrowserConfig`**: Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.).  \n2. **`CrawlerRunConfig`**: Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.).\nBelow is an example with minimal usage:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_conf\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n> IMPORTANT: By default cache mode is set to `CacheMode.BYPASS` to have fresh content. Set `CacheMode.ENABLED` to enable caching.\nWeâ€™ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.\n* * *\n## 4. Generating Markdown Output\nBy default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a **markdown generator** or **content filter**.\n  * **`result.markdown`**:  \nThe direct HTML-to-Markdown conversion. \n  * **`result.markdown.fit_markdown`**:  \nThe same content after applying any configured **content filter** (e.g., `PruningContentFilter`).\n\n\n### Example: Using a Filter with `DefaultMarkdownGenerator`\n```\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\nCopy\n```\n\n**Note** : If you do **not** specify a content filter or markdown generator, youâ€™ll typically see only the raw Markdown. `PruningContentFilter` may adds around `50ms` in processing time. Weâ€™ll dive deeper into these strategies in a dedicated **Markdown Generation** tutorial.\n* * *\n## 5. Simple Data Extraction (CSS-based)\nCrawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example:\n> **New!** Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions:\n```\nfrom crawl4ai import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")  # Required for OpenAI\n)\n\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)\nCopy\n```\n\nFor a complete guide on schema generation and advanced usage, see [No-LLM Extraction Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/).\nHere's a basic extraction example:\n```\nimport asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        # The JSON output is stored in 'extracted_content'\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Why is this helpful?** - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store.\n> Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with `raw://`.\n* * *\n## 6. Simple Data Extraction (LLM-based)\nFor more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports **open-source** or **closed-source** providers:\n  * **Open-Source Models** (e.g., `ollama/llama3.3`, `no_token`) \n  * **OpenAI Models** (e.g., `openai/gpt-4`, requires `api_token`) \n  * Or any provider supported by the underlying library\n\n\nBelow is an example using **open-source** style (no token) and closed-source:\n```\nimport os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config = LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n        )\n    )\nCopy\n```\n\n**Whatâ€™s happening?** - We define a Pydantic schema (`PricingInfo`) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the **provider** and **api_token** , you can use local models or a remote API.\n* * *\n## 7. Adaptive Crawling (New!)\nCrawl4AI now includes intelligent adaptive crawling that automatically determines when sufficient information has been gathered. Here's a quick example:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, AdaptiveCrawler\n\nasync def adaptive_example():\n    async with AsyncWebCrawler() as crawler:\n        adaptive = AdaptiveCrawler(crawler)\n\n        # Start adaptive crawling\n        result = await adaptive.digest(\n            start_url=\"https://docs.python.org/3/\",\n            query=\"async context managers\"\n        )\n\n        # View results\n        adaptive.print_stats()\n        print(f\"Crawled {len(result.crawled_urls)} pages\")\n        print(f\"Achieved {adaptive.confidence:.0%} confidence\")\n\nif __name__ == \"__main__\":\n    asyncio.run(adaptive_example())\nCopy\n```\n\n**What's special about adaptive crawling?** - **Automatic stopping** : Stops when sufficient information is gathered - **Intelligent link selection** : Follows only relevant links - **Confidence scoring** : Know how complete your information is\n[Learn more about Adaptive Crawling â†’](https://docs.crawl4ai.com/core/adaptive-crawling/)\n* * *\n## 8. Multi-URL Concurrency (Preview)\nIf you need to crawl multiple URLs in **parallel** , you can use `arun_many()`. By default, Crawl4AI employs a **MemoryAdaptiveDispatcher** , automatically adjusting concurrency based on system resources. Hereâ€™s a quick glimpse:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def quick_parallel_example():\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Stream results as they complete\n        async for result in await crawler.arun_many(urls, config=run_conf):\n            if result.success:\n                print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {result.url} => {result.error_message}\")\n\n        # Or get all results at once (default behavior)\n        run_conf = run_conf.clone(stream=False)\n        results = await crawler.arun_many(urls, config=run_conf)\n        for res in results:\n            if res.success:\n                print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {res.url} => {res.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(quick_parallel_example())\nCopy\n```\n\nThe example above shows two ways to handle multiple URLs: 1. **Streaming mode** (`stream=True`): Process results as they become available using `async for` 2. **Batch mode** (`stream=False`): Wait for all results to complete\nFor more advanced concurrency (e.g., a **semaphore-based** approach, **adaptive memory usage throttling** , or customized rate limiting), see [Advanced Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/).\n* * *\n## 8. Dynamic Content Example\nSome sites require multiple â€œpage clicksâ€ or dynamic JavaScript updates. Below is an example showing how to **click** a â€œNext Pageâ€ button and wait for new commits to load on GitHub, using **`BrowserConfig`**and**`CrawlerRunConfig`**:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n        \"fields\": [\n            {\n                \"name\": \"section_title\",\n                \"selector\": \"h3.heading-50\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"section_description\",\n                \"selector\": \".charge-content\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_name\",\n                \"selector\": \".text-block-93\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_description\",\n                \"selector\": \".course-content-text\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_icon\",\n                \"selector\": \".image-92\",\n                \"type\": \"attribute\",\n                \"attribute\": \"src\",\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500));\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\nasync def main():\n    await extract_structured_data_using_css_extractor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Points** :\n  * **`BrowserConfig(headless=False)`**: We want to watch it click â€œNext Page.â€\n  * **`CrawlerRunConfig(...)`**: We specify the extraction strategy, pass`session_id` to reuse the same page. \n  * **`js_code`**and**`wait_for`**are used for subsequent pages (`page > 0`) to click the â€œNextâ€ button and wait for new commits to load. \n  * **`js_only=True`**indicates weâ€™re not re-navigating but continuing the existing session.\n  * Finally, we call `kill_session()` to clean up the page and browser session.\n\n\n* * *\n## 9. Next Steps\nCongratulations! You have:\n  1. Performed a basic crawl and printed Markdown. \n  2. Used **content filters** with a markdown generator. \n  3. Extracted JSON via **CSS** or **LLM** strategies. \n  4. Handled **dynamic** pages with JavaScript triggers.\n\n\nIf youâ€™re ready for more, check out:\n  * **Installation** : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. \n  * **Hooks & Auth**: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. \n  * **Deployment** : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. \n  * **Browser Management** : Delve into user simulation, stealth modes, and concurrency best practices. \n\n\nCrawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/quickstart/)\n\n\nESC to close\n#### On this page\n  * [1. Introduction](https://docs.crawl4ai.com/core/quickstart/#1-introduction)\n  * [2. Your First Crawl](https://docs.crawl4ai.com/core/quickstart/#2-your-first-crawl)\n  * [3. Basic Configuration (Light Introduction)](https://docs.crawl4ai.com/core/quickstart/#3-basic-configuration-light-introduction)\n  * [4. Generating Markdown Output](https://docs.crawl4ai.com/core/quickstart/#4-generating-markdown-output)\n  * [Example: Using a Filter with DefaultMarkdownGenerator](https://docs.crawl4ai.com/core/quickstart/#example-using-a-filter-with-defaultmarkdowngenerator)\n  * [5. Simple Data Extraction (CSS-based)](https://docs.crawl4ai.com/core/quickstart/#5-simple-data-extraction-css-based)\n  * [6. Simple Data Extraction (LLM-based)](https://docs.crawl4ai.com/core/quickstart/#6-simple-data-extraction-llm-based)\n  * [7. Adaptive Crawling (New!)](https://docs.crawl4ai.com/core/quickstart/#7-adaptive-crawling-new)\n  * [8. Multi-URL Concurrency (Preview)](https://docs.crawl4ai.com/core/quickstart/#8-multi-url-concurrency-preview)\n  * [8. Dynamic Content Example](https://docs.crawl4ai.com/core/quickstart/#8-dynamic-content-example)\n  * [9. Next Steps](https://docs.crawl4ai.com/core/quickstart/#9-next-steps)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/self-hosting",
    "depth": 1,
    "title": "Self-Hosting Guide - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "3906e6dfad7e5081d5abce9f7be771f8",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/self-hosting/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * Self-Hosting Guide\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Self-Hosting Crawl4AI ðŸš€](https://docs.crawl4ai.com/core/self-hosting/#self-hosting-crawl4ai)\n  * [Why Self-Host?](https://docs.crawl4ai.com/core/self-hosting/#why-self-host)\n  * [Table of Contents](https://docs.crawl4ai.com/core/self-hosting/#table-of-contents)\n  * [Prerequisites](https://docs.crawl4ai.com/core/self-hosting/#prerequisites)\n  * [Installation](https://docs.crawl4ai.com/core/self-hosting/#installation)\n  * [MCP (Model Context Protocol) Support](https://docs.crawl4ai.com/core/self-hosting/#mcp-model-context-protocol-support)\n  * [Additional API Endpoints](https://docs.crawl4ai.com/core/self-hosting/#additional-api-endpoints)\n  * [User-Provided Hooks API](https://docs.crawl4ai.com/core/self-hosting/#user-provided-hooks-api)\n  * [Job Queue & Webhook API](https://docs.crawl4ai.com/core/self-hosting/#job-queue-webhook-api)\n  * [Dockerfile Parameters](https://docs.crawl4ai.com/core/self-hosting/#dockerfile-parameters)\n  * [Using the API](https://docs.crawl4ai.com/core/self-hosting/#using-the-api)\n  * [Real-time Monitoring & Operations](https://docs.crawl4ai.com/core/self-hosting/#real-time-monitoring-operations)\n  * [Server Configuration](https://docs.crawl4ai.com/core/self-hosting/#server-configuration)\n  * [Getting Help](https://docs.crawl4ai.com/core/self-hosting/#getting-help)\n  * [Summary](https://docs.crawl4ai.com/core/self-hosting/#summary)\n\n\n# Self-Hosting Crawl4AI ðŸš€\n**Take Control of Your Web Crawling Infrastructure**\nSelf-hosting Crawl4AI gives you complete control over your web crawling and data extraction pipeline. Unlike cloud-based solutions, you own your data, infrastructure, and destiny.\n## Why Self-Host?\n  * **ðŸ”’ Data Privacy** : Your crawled data never leaves your infrastructure\n  * **ðŸ’° Cost Control** : No per-request pricing - scale within your own resources\n  * **ðŸŽ¯ Customization** : Full control over browser configurations, extraction strategies, and performance tuning\n  * **ðŸ“Š Transparency** : Real-time monitoring dashboard shows exactly what's happening\n  * **âš¡ Performance** : Direct access without API rate limits or geographic restrictions\n  * **ðŸ›¡ï¸ Security** : Keep sensitive data extraction workflows behind your firewall\n  * **ðŸ”§ Flexibility** : Customize, extend, and integrate with your existing infrastructure\n\n\nWhen you self-host, you can scale from a single container to a full browser infrastructure, all while maintaining complete control and visibility.\n## Table of Contents\n  * [Prerequisites](https://docs.crawl4ai.com/core/self-hosting/#prerequisites)\n  * [Installation](https://docs.crawl4ai.com/core/self-hosting/#installation)\n  * [Option 1: Using Pre-built Docker Hub Images (Recommended)](https://docs.crawl4ai.com/core/self-hosting/#option-1-using-pre-built-docker-hub-images-recommended)\n  * [Option 2: Using Docker Compose](https://docs.crawl4ai.com/core/self-hosting/#option-2-using-docker-compose)\n  * [Option 3: Manual Local Build & Run](https://docs.crawl4ai.com/core/self-hosting/#option-3-manual-local-build--run)\n  * [MCP (Model Context Protocol) Support](https://docs.crawl4ai.com/core/self-hosting/#mcp-model-context-protocol-support)\n  * [What is MCP?](https://docs.crawl4ai.com/core/self-hosting/#what-is-mcp)\n  * [Connecting via MCP](https://docs.crawl4ai.com/core/self-hosting/#connecting-via-mcp)\n  * [Using with Claude Code](https://docs.crawl4ai.com/core/self-hosting/#using-with-claude-code)\n  * [Available MCP Tools](https://docs.crawl4ai.com/core/self-hosting/#available-mcp-tools)\n  * [Testing MCP Connections](https://docs.crawl4ai.com/core/self-hosting/#testing-mcp-connections)\n  * [MCP Schemas](https://docs.crawl4ai.com/core/self-hosting/#mcp-schemas)\n  * [Real-time Monitoring & Operations](https://docs.crawl4ai.com/core/self-hosting/#real-time-monitoring--operations)\n  * [Monitoring Dashboard](https://docs.crawl4ai.com/core/self-hosting/#monitoring-dashboard)\n  * [Monitor API Endpoints](https://docs.crawl4ai.com/core/self-hosting/#monitor-api-endpoints)\n  * [WebSocket Streaming](https://docs.crawl4ai.com/core/self-hosting/#websocket-streaming)\n  * [Control Actions](https://docs.crawl4ai.com/core/self-hosting/#control-actions)\n  * [Production Integration](https://docs.crawl4ai.com/core/self-hosting/#production-integration)\n  * [Deployment Scenarios](https://docs.crawl4ai.com/core/self-hosting/#deployment-scenarios)\n  * [Complete Examples](https://docs.crawl4ai.com/core/self-hosting/#complete-examples)\n  * [Server Configuration](https://docs.crawl4ai.com/core/self-hosting/#server-configuration)\n  * [Understanding config.yml](https://docs.crawl4ai.com/core/self-hosting/#understanding-configyml)\n  * [JWT Authentication](https://docs.crawl4ai.com/core/self-hosting/#jwt-authentication)\n  * [Configuration Tips and Best Practices](https://docs.crawl4ai.com/core/self-hosting/#configuration-tips-and-best-practices)\n  * [Customizing Your Configuration](https://docs.crawl4ai.com/core/self-hosting/#customizing-your-configuration)\n  * [Configuration Recommendations](https://docs.crawl4ai.com/core/self-hosting/#configuration-recommendations)\n  * [Getting Help](https://docs.crawl4ai.com/core/self-hosting/#getting-help)\n  * [Summary](https://docs.crawl4ai.com/core/self-hosting/#summary)\n\n\n## Prerequisites\nBefore we dive in, make sure you have: - Docker installed and running (version 20.10.0 or higher), including `docker compose` (usually bundled with Docker Desktop). - `git` for cloning the repository. - At least 4GB of RAM available for the container (more recommended for heavy use). - Python 3.10+ (if using the Python SDK). - Node.js 16+ (if using the Node.js examples).\n> ðŸ’¡ **Pro tip** : Run `docker info` to check your Docker installation and available resources.\n## Installation\nWe offer several ways to get the Crawl4AI server running. The quickest way is to use our pre-built Docker Hub images.\n### Option 1: Using Pre-built Docker Hub Images (Recommended)\nPull and run images directly from Docker Hub without building locally.\n#### 1. Pull the Image\nOur latest release is `0.7.6`. Images are built with multi-arch manifests, so Docker automatically pulls the correct version for your system.\n> ðŸ’¡ **Note** : The `latest` tag points to the stable `0.7.6` version.\n```\n# Pull the latest version\ndocker pull unclecode/crawl4ai:0.7.6\n\n# Or pull using the latest tag\ndocker pull unclecode/crawl4ai:latest\nCopy\n```\n\n#### 2. Setup Environment (API Keys)\nIf you plan to use LLMs, create a `.llm.env` file in your working directory:\n```\n# Create a .llm.env file with your API keys\ncat > .llm.env << EOL\n# OpenAI\nOPENAI_API_KEY=sk-your-key\n\n# Anthropic\nANTHROPIC_API_KEY=your-anthropic-key\n\n# Other providers as needed\n# DEEPSEEK_API_KEY=your-deepseek-key\n# GROQ_API_KEY=your-groq-key\n# TOGETHER_API_KEY=your-together-key\n# MISTRAL_API_KEY=your-mistral-key\n# GEMINI_API_TOKEN=your-gemini-token\n\n# Optional: Global LLM settings\n# LLM_PROVIDER=openai/gpt-4o-mini\n# LLM_TEMPERATURE=0.7\n# LLM_BASE_URL=https://api.custom.com/v1\n\n# Optional: Provider-specific overrides\n# OPENAI_TEMPERATURE=0.5\n# OPENAI_BASE_URL=https://custom-openai.com/v1\n# ANTHROPIC_TEMPERATURE=0.3\nEOL\nCopy\n```\n\n> ðŸ”‘ **Note** : Keep your API keys secure! Never commit `.llm.env` to version control.\n#### 3. Run the Container\n  * **Basic run:**\n```\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai \\\n  --shm-size=1g \\\n  unclecode/crawl4ai:latest\nCopy\n```\n\n  * **With LLM support:**\n```\n# Make sure .llm.env is in the current directory\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai \\\n  --env-file .llm.env \\\n  --shm-size=1g \\\n  unclecode/crawl4ai:latest\nCopy\n```\n\n\n\n> The server will be available at `http://localhost:11235`. Visit `/playground` to access the interactive testing interface.\n#### 4. Stopping the Container\n```\ndocker stop crawl4ai && docker rm crawl4ai\nCopy\n```\n\n#### Docker Hub Versioning Explained\n  * **Image Name:** `unclecode/crawl4ai`\n  * **Tag Format:** `LIBRARY_VERSION[-SUFFIX]` (e.g., `0.7.6`)\n    * `LIBRARY_VERSION`: The semantic version of the core `crawl4ai` Python library\n    * `SUFFIX`: Optional tag for release candidates (``) and revisions (`r1`)\n  * **`latest`Tag:** Points to the most recent stable version\n  * **Multi-Architecture Support:** All images support both `linux/amd64` and `linux/arm64` architectures through a single tag\n\n\n### Option 2: Using Docker Compose\nDocker Compose simplifies building and running the service, especially for local development and testing.\n#### 1. Clone Repository\n```\ngit clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\nCopy\n```\n\n#### 2. Environment Setup (API Keys)\nIf you plan to use LLMs, copy the example environment file and add your API keys. This file should be in the **project root directory**.\n```\n# Make sure you are in the 'crawl4ai' root directory\ncp deploy/docker/.llm.env.example .llm.env\n\n# Now edit .llm.env and add your API keys\nCopy\n```\n\n**Flexible LLM Provider Configuration:**\nThe Docker setup now supports flexible LLM provider configuration through a hierarchical system:\n  1. **API Request Parameters** (Highest Priority): Specify per request \n```\n{\n  \"url\": \"https://example.com\",\n  \"f\": \"llm\",\n  \"provider\": \"groq/mixtral-8x7b\",\n  \"temperature\": 0.7,\n  \"base_url\": \"https://api.custom.com/v1\"\n}\nCopy\n```\n\n  2. **Provider-Specific Environment Variables** : Override for specific providers \n```\n# In your .llm.env file:\nOPENAI_TEMPERATURE=0.5\nOPENAI_BASE_URL=https://custom-openai.com/v1\nANTHROPIC_TEMPERATURE=0.3\nCopy\n```\n\n  3. **Global Environment Variables** : Set defaults for all providers \n```\n# In your .llm.env file:\nLLM_PROVIDER=anthropic/claude-3-opus\nLLM_TEMPERATURE=0.7\nLLM_BASE_URL=https://api.proxy.com/v1\nCopy\n```\n\n  4. **Config File Default** : Falls back to `config.yml` (default: `openai/gpt-4o-mini`)\n\n\nThe system automatically selects the appropriate API key based on the provider. LiteLLM handles finding the correct environment variable for each provider (e.g., OPENAI_API_KEY for OpenAI, GEMINI_API_TOKEN for Google Gemini, etc.).\n**Supported LLM Parameters:** - `provider`: LLM provider and model (e.g., \"openai/gpt-4\", \"anthropic/claude-3-opus\") - `temperature`: Controls randomness (0.0-2.0, lower = more focused, higher = more creative) - `base_url`: Custom API endpoint for proxy servers or alternative endpoints\n#### 3. Build and Run with Compose\nThe `docker-compose.yml` file in the project root provides a simplified approach that automatically handles architecture detection using buildx.\n  * **Run Pre-built Image from Docker Hub:**\n```\n# Pulls and runs the release candidate from Docker Hub\n# Automatically selects the correct architecture\nIMAGE=unclecode/crawl4ai:latest docker compose up -d\nCopy\n```\n\n  * **Build and Run Locally:**\n```\n# Builds the image locally using Dockerfile and runs it\n# Automatically uses the correct architecture for your machine\ndocker compose up --build -d\nCopy\n```\n\n  * **Customize the Build:**\n```\n# Build with all features (includes torch and transformers)\nINSTALL_TYPE=all docker compose up --build -d\n\n# Build with GPU support (for AMD64 platforms)\nENABLE_GPU=true docker compose up --build -d\nCopy\n```\n\n\n\n> The server will be available at `http://localhost:11235`.\n#### 4. Stopping the Service\n```\n# Stop the service\ndocker compose down\nCopy\n```\n\n### Option 3: Manual Local Build & Run\nIf you prefer not to use Docker Compose for direct control over the build and run process.\n#### 1. Clone Repository & Setup Environment\nFollow steps 1 and 2 from the Docker Compose section above (clone repo, `cd crawl4ai`, create `.llm.env` in the root).\n#### 2. Build the Image (Multi-Arch)\nUse `docker buildx` to build the image. Crawl4AI now uses buildx to handle multi-architecture builds automatically.\n```\n# Make sure you are in the 'crawl4ai' root directory\n# Build for the current architecture and load it into Docker\ndocker buildx build -t crawl4ai-local:latest --load .\n\n# Or build for multiple architectures (useful for publishing)\ndocker buildx build --platform linux/amd64,linux/arm64 -t crawl4ai-local:latest --load .\n\n# Build with additional options\ndocker buildx build \\\n  --build-arg INSTALL_TYPE=all \\\n  --build-arg ENABLE_GPU=false \\\n  -t crawl4ai-local:latest --load .\nCopy\n```\n\n#### 3. Run the Container\n  * **Basic run (no LLM support):**\n```\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai-standalone \\\n  --shm-size=1g \\\n  crawl4ai-local:latest\nCopy\n```\n\n  * **With LLM support:**\n```\n# Make sure .llm.env is in the current directory (project root)\ndocker run -d \\\n  -p 11235:11235 \\\n  --name crawl4ai-standalone \\\n  --env-file .llm.env \\\n  --shm-size=1g \\\n  crawl4ai-local:latest\nCopy\n```\n\n\n\n> The server will be available at `http://localhost:11235`.\n#### 4. Stopping the Manual Container\n```\ndocker stop crawl4ai-standalone && docker rm crawl4ai-standalone\nCopy\n```\n\n* * *\n## MCP (Model Context Protocol) Support\nCrawl4AI server includes support for the Model Context Protocol (MCP), allowing you to connect the server's capabilities directly to MCP-compatible clients like Claude Code.\n### What is MCP?\nMCP is an open protocol that standardizes how applications provide context to LLMs. It allows AI models to access external tools, data sources, and services through a standardized interface.\n### Connecting via MCP\nThe Crawl4AI server exposes two MCP endpoints:\n  * **Server-Sent Events (SSE)** : `http://localhost:11235/mcp/sse`\n  * **WebSocket** : `ws://localhost:11235/mcp/ws`\n\n\n### Using with Claude Code\nYou can add Crawl4AI as an MCP tool provider in Claude Code with a simple command:\n```\n# Add the Crawl4AI server as an MCP provider\nclaude mcp add --transport sse c4ai-sse http://localhost:11235/mcp/sse\n\n# List all MCP providers to verify it was added\nclaude mcp list\nCopy\n```\n\nOnce connected, Claude Code can directly use Crawl4AI's capabilities like screenshot capture, PDF generation, and HTML processing without having to make separate API calls.\n### Available MCP Tools\nWhen connected via MCP, the following tools are available:\n  * `md` - Generate markdown from web content\n  * `html` - Extract preprocessed HTML\n  * `screenshot` - Capture webpage screenshots\n  * `pdf` - Generate PDF documents\n  * `execute_js` - Run JavaScript on web pages\n  * `crawl` - Perform multi-URL crawling\n  * `ask` - Query the Crawl4AI library context\n\n\n### Testing MCP Connections\nYou can test the MCP WebSocket connection using the test file included in the repository:\n```\n# From the repository root\npython tests/mcp/test_mcp_socket.py\nCopy\n```\n\n### MCP Schemas\nAccess the MCP tool schemas at `http://localhost:11235/mcp/schema` for detailed information on each tool's parameters and capabilities.\n* * *\n## Additional API Endpoints\nIn addition to the core `/crawl` and `/crawl/stream` endpoints, the server provides several specialized endpoints:\n### HTML Extraction Endpoint\n```\nPOST /html\nCopy\n```\n\nCrawls the URL and returns preprocessed HTML optimized for schema extraction.\n```\n{\n  \"url\": \"https://example.com\"\n}\nCopy\n```\n\n### Screenshot Endpoint\n```\nPOST /screenshot\nCopy\n```\n\nCaptures a full-page PNG screenshot of the specified URL.\n```\n{\n  \"url\": \"https://example.com\",\n  \"screenshot_wait_for\": 2,\n  \"output_path\": \"/path/to/save/screenshot.png\"\n}\nCopy\n```\n\n  * `screenshot_wait_for`: Optional delay in seconds before capture (default: 2)\n  * `output_path`: Optional path to save the screenshot (recommended)\n\n\n### PDF Export Endpoint\n```\nPOST /pdf\nCopy\n```\n\nGenerates a PDF document of the specified URL.\n```\n{\n  \"url\": \"https://example.com\",\n  \"output_path\": \"/path/to/save/document.pdf\"\n}\nCopy\n```\n\n  * `output_path`: Optional path to save the PDF (recommended)\n\n\n### JavaScript Execution Endpoint\n```\nPOST /execute_js\nCopy\n```\n\nExecutes JavaScript snippets on the specified URL and returns the full crawl result.\n```\n{\n  \"url\": \"https://example.com\",\n  \"scripts\": [\n    \"return document.title\",\n    \"return Array.from(document.querySelectorAll('a')).map(a => a.href)\"\n  ]\n}\nCopy\n```\n\n  * `scripts`: List of JavaScript snippets to execute sequentially\n\n\n* * *\n## User-Provided Hooks API\nThe Docker API supports user-provided hook functions, allowing you to customize the crawling behavior by injecting your own Python code at specific points in the crawling pipeline. This powerful feature enables authentication, performance optimization, and custom content extraction without modifying the server code.\n> âš ï¸ **IMPORTANT SECURITY WARNING** : - **Never use hooks with untrusted code or on untrusted websites** - **Be extremely careful when crawling sites that might be phishing or malicious** - **Hook code has access to page context and can interact with the website** - **Always validate and sanitize any data extracted through hooks** - **Never expose credentials or sensitive data in hook code** - **Consider running the Docker container in an isolated network when testing**\n### Hook Information Endpoint\n```\nGET /hooks/info\nCopy\n```\n\nReturns information about available hook points and their signatures:\n```\ncurl http://localhost:11235/hooks/info\nCopy\n```\n\n### Available Hook Points\nThe API supports 8 hook points that match the local SDK:\nHook Point | Parameters | Description | Best Use Cases  \n---|---|---|---  \n`on_browser_created` | `browser` | After browser instance creation | Light setup tasks  \n`on_page_context_created` | `page, context` | After page/context creation | **Authentication, cookies, route blocking**  \n`before_goto` | `page, context, url` | Before navigating to URL | Custom headers, logging  \n`after_goto` | `page, context, url, response` | After navigation completes | Verification, waiting for elements  \n`on_user_agent_updated` | `page, context, user_agent` | When user agent changes | UA-specific logic  \n`on_execution_started` | `page, context` | When JS execution begins | JS-related setup  \n`before_retrieve_html` | `page, context` | Before getting final HTML | **Scrolling, lazy loading**  \n`before_return_html` | `page, context, html` | Before returning HTML | Final modifications, metrics  \n### Using Hooks in Requests\nAdd hooks to any crawl request by including the `hooks` parameter:\n```\n{\n  \"urls\": [\"https://httpbin.org/html\"],\n  \"hooks\": {\n    \"code\": {\n      \"hook_point_name\": \"async def hook(...): ...\",\n      \"another_hook\": \"async def hook(...): ...\"\n    },\n    \"timeout\": 30  // Optional, default 30 seconds (max 120)\n  }\n}\nCopy\n```\n\n### Hook Examples with Real URLs\n#### 1. Authentication with Cookies (GitHub)\n```\nimport requests\n\n# Example: Setting GitHub session cookie (use your actual session)\nhooks_code = {\n    \"on_page_context_created\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Add authentication cookies for GitHub\n    # WARNING: Never hardcode real credentials!\n    await context.add_cookies([\n        {\n            'name': 'user_session',\n            'value': 'your_github_session_token',  # Replace with actual token\n            'domain': '.github.com',\n            'path': '/',\n            'httpOnly': True,\n            'secure': True,\n            'sameSite': 'Lax'\n        }\n    ])\n    return page\n\"\"\"\n}\n\nresponse = requests.post(\"http://localhost:11235/crawl\", json={\n    \"urls\": [\"https://github.com/settings/profile\"],  # Protected page\n    \"hooks\": {\"code\": hooks_code, \"timeout\": 30}\n})\nCopy\n```\n\n#### 2. Basic Authentication (httpbin.org for testing)\n```\n# Safe testing with httpbin.org (a service designed for HTTP testing)\nhooks_code = {\n    \"before_goto\": \"\"\"\nasync def hook(page, context, url, **kwargs):\n    import base64\n    # httpbin.org/basic-auth expects username=\"user\" and password=\"passwd\"\n    credentials = base64.b64encode(b\"user:passwd\").decode('ascii')\n\n    await page.set_extra_http_headers({\n        'Authorization': f'Basic {credentials}'\n    })\n    return page\n\"\"\"\n}\n\nresponse = requests.post(\"http://localhost:11235/crawl\", json={\n    \"urls\": [\"https://httpbin.org/basic-auth/user/passwd\"],\n    \"hooks\": {\"code\": hooks_code, \"timeout\": 15}\n})\nCopy\n```\n\n#### 3. Performance Optimization (News Sites)\n```\n# Example: Optimizing crawling of news sites like CNN or BBC\nhooks_code = {\n    \"on_page_context_created\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Block images, fonts, and media to speed up crawling\n    await context.route(\"**/*.{png,jpg,jpeg,gif,webp,svg,ico}\", lambda route: route.abort())\n    await context.route(\"**/*.{woff,woff2,ttf,otf,eot}\", lambda route: route.abort())\n    await context.route(\"**/*.{mp4,webm,ogg,mp3,wav,flac}\", lambda route: route.abort())\n\n    # Block common tracking and ad domains\n    await context.route(\"**/googletagmanager.com/*\", lambda route: route.abort())\n    await context.route(\"**/google-analytics.com/*\", lambda route: route.abort())\n    await context.route(\"**/doubleclick.net/*\", lambda route: route.abort())\n    await context.route(\"**/facebook.com/tr/*\", lambda route: route.abort())\n    await context.route(\"**/amazon-adsystem.com/*\", lambda route: route.abort())\n\n    # Disable CSS animations for faster rendering\n    await page.add_style_tag(content='''\n        *, *::before, *::after {\n            animation-duration: 0s !important;\n            transition-duration: 0s !important;\n        }\n    ''')\n\n    return page\n\"\"\"\n}\n\nresponse = requests.post(\"http://localhost:11235/crawl\", json={\n    \"urls\": [\"https://www.bbc.com/news\"],  # Heavy news site\n    \"hooks\": {\"code\": hooks_code, \"timeout\": 30}\n})\nCopy\n```\n\n#### 4. Handling Infinite Scroll (Twitter/X)\n```\n# Example: Scrolling on Twitter/X (requires authentication)\nhooks_code = {\n    \"before_retrieve_html\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Scroll to load more tweets\n    previous_height = 0\n    for i in range(5):  # Limit scrolls to avoid infinite loop\n        current_height = await page.evaluate(\"document.body.scrollHeight\")\n        if current_height == previous_height:\n            break  # No more content to load\n\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n        await page.wait_for_timeout(2000)  # Wait for content to load\n        previous_height = current_height\n\n    return page\n\"\"\"\n}\n\n# Note: Twitter requires authentication for most content\nresponse = requests.post(\"http://localhost:11235/crawl\", json={\n    \"urls\": [\"https://twitter.com/nasa\"],  # Public profile\n    \"hooks\": {\"code\": hooks_code, \"timeout\": 30}\n})\nCopy\n```\n\n#### 5. E-commerce Login (Example Pattern)\n```\n# SECURITY WARNING: This is a pattern example. \n# Never use real credentials in code!\n# Always use environment variables or secure vaults.\n\nhooks_code = {\n    \"on_page_context_created\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Example pattern for e-commerce sites\n    # DO NOT use real credentials here!\n\n    # Navigate to login page first\n    await page.goto(\"https://example-shop.com/login\")\n\n    # Wait for login form to load\n    await page.wait_for_selector(\"#email\", timeout=5000)\n\n    # Fill login form (use environment variables in production!)\n    await page.fill(\"#email\", \"test@example.com\")  # Never use real email\n    await page.fill(\"#password\", \"test_password\")   # Never use real password\n\n    # Handle \"Remember Me\" checkbox if present\n    try:\n        await page.uncheck(\"#remember_me\")  # Don't remember on shared systems\n    except:\n        pass\n\n    # Submit form\n    await page.click(\"button[type='submit']\")\n\n    # Wait for redirect after login\n    await page.wait_for_url(\"**/account/**\", timeout=10000)\n\n    return page\n\"\"\"\n}\nCopy\n```\n\n#### 6. Extracting Structured Data (Wikipedia)\n```\n# Safe example using Wikipedia\nhooks_code = {\n    \"after_goto\": \"\"\"\nasync def hook(page, context, url, response, **kwargs):\n    # Wait for Wikipedia content to load\n    await page.wait_for_selector(\"#content\", timeout=5000)\n    return page\n\"\"\",\n\n    \"before_retrieve_html\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Extract structured data from Wikipedia infobox\n    metadata = await page.evaluate('''() => {\n        const infobox = document.querySelector('.infobox');\n        if (!infobox) return null;\n\n        const data = {};\n        const rows = infobox.querySelectorAll('tr');\n\n        rows.forEach(row => {\n            const header = row.querySelector('th');\n            const value = row.querySelector('td');\n            if (header && value) {\n                data[header.innerText.trim()] = value.innerText.trim();\n            }\n        });\n\n        return data;\n    }''')\n\n    if metadata:\n        print(\"Extracted metadata:\", metadata)\n\n    return page\n\"\"\"\n}\n\nresponse = requests.post(\"http://localhost:11235/crawl\", json={\n    \"urls\": [\"https://en.wikipedia.org/wiki/Python_(programming_language)\"],\n    \"hooks\": {\"code\": hooks_code, \"timeout\": 20}\n})\nCopy\n```\n\n### Security Best Practices\n> ðŸ”’ **Critical Security Guidelines** :\n  1. **Never Trust User Input** : If accepting hook code from users, always validate and sandbox it\n  2. **Avoid Phishing Sites** : Never use hooks on suspicious or unverified websites\n  3. **Protect Credentials** : \n  4. Never hardcode passwords, tokens, or API keys in hook code\n  5. Use environment variables or secure secret management\n  6. Rotate credentials regularly\n  7. **Network Isolation** : Run the Docker container in an isolated network when testing\n  8. **Audit Hook Code** : Always review hook code before execution\n  9. **Limit Permissions** : Use the least privileged access needed\n  10. **Monitor Execution** : Check hook execution logs for suspicious behavior\n  11. **Timeout Protection** : Always set reasonable timeouts (default 30s)\n\n\n### Hook Response Information\nWhen hooks are used, the response includes detailed execution information:\n```\n{\n  \"success\": true,\n  \"results\": [...],\n  \"hooks\": {\n    \"status\": {\n      \"status\": \"success\",  // or \"partial\" or \"failed\"\n      \"attached_hooks\": [\"on_page_context_created\", \"before_retrieve_html\"],\n      \"validation_errors\": [],\n      \"successfully_attached\": 2,\n      \"failed_validation\": 0\n    },\n    \"execution_log\": [\n      {\n        \"hook_point\": \"on_page_context_created\",\n        \"status\": \"success\",\n        \"execution_time\": 0.523,\n        \"timestamp\": 1234567890.123\n      }\n    ],\n    \"errors\": [],  // Any runtime errors\n    \"summary\": {\n      \"total_executions\": 2,\n      \"successful\": 2,\n      \"failed\": 0,\n      \"timed_out\": 0,\n      \"success_rate\": 100.0\n    }\n  }\n}\nCopy\n```\n\n### Error Handling\nThe hooks system is designed to be resilient:\n  1. **Validation Errors** : Caught before execution (syntax errors, wrong parameters)\n  2. **Runtime Errors** : Handled gracefully - crawl continues with original page object\n  3. **Timeout Protection** : Hooks automatically terminated after timeout (configurable 1-120s)\n\n\n### Complete Example: Safe Multi-Hook Crawling\n```\nimport requests\nimport json\nimport os\n\n# Safe example using httpbin.org for testing\nhooks_code = {\n    \"on_page_context_created\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Set viewport and test cookies\n    await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n    await context.add_cookies([\n        {\"name\": \"test_cookie\", \"value\": \"test_value\", \"domain\": \".httpbin.org\", \"path\": \"/\"}\n    ])\n\n    # Block unnecessary resources for httpbin\n    await context.route(\"**/*.{png,jpg,jpeg}\", lambda route: route.abort())\n    return page\n\"\"\",\n\n    \"before_goto\": \"\"\"\nasync def hook(page, context, url, **kwargs):\n    # Add custom headers for testing\n    await page.set_extra_http_headers({\n        \"X-Test-Header\": \"crawl4ai-test\",\n        \"Accept-Language\": \"en-US,en;q=0.9\"\n    })\n    print(f\"[HOOK] Navigating to: {url}\")\n    return page\n\"\"\",\n\n    \"before_retrieve_html\": \"\"\"\nasync def hook(page, context, **kwargs):\n    # Simple scroll for any lazy-loaded content\n    await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n    await page.wait_for_timeout(1000)\n    return page\n\"\"\"\n}\n\n# Make the request to safe testing endpoints\nresponse = requests.post(\"http://localhost:11235/crawl\", json={\n    \"urls\": [\n        \"https://httpbin.org/html\",\n        \"https://httpbin.org/json\"\n    ],\n    \"hooks\": {\n        \"code\": hooks_code,\n        \"timeout\": 30\n    },\n    \"crawler_config\": {\n        \"cache_mode\": \"bypass\"\n    }\n})\n\n# Check results\nif response.status_code == 200:\n    data = response.json()\n\n    # Check hook execution\n    if data['hooks']['status']['status'] == 'success':\n        print(f\"âœ… All {len(data['hooks']['status']['attached_hooks'])} hooks executed successfully\")\n        print(f\"Execution stats: {data['hooks']['summary']}\")\n\n    # Process crawl results\n    for result in data['results']:\n        print(f\"Crawled: {result['url']} - Success: {result['success']}\")\nelse:\n    print(f\"Error: {response.status_code}\")\nCopy\n```\n\n> ðŸ’¡ **Remember** : Always test your hooks on safe, known websites first before using them on production sites. Never crawl sites that you don't have permission to access or that might be malicious.\n### Hooks Utility: Function-Based Approach (Python)\nFor Python developers, Crawl4AI provides a more convenient way to work with hooks using the `hooks_to_string()` utility function and Docker client integration.\n#### Why Use Function-Based Hooks?\n**String-Based Approach (shown above)** : \n```\nhooks_code = {\n    \"on_page_context_created\": \"\"\"\nasync def hook(page, context, **kwargs):\n    await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n    return page\n\"\"\"\n}\nCopy\n```\n\n**Function-Based Approach (recommended for Python)** : \n```\nfrom crawl4ai import Crawl4aiDockerClient\n\nasync def my_hook(page, context, **kwargs):\n    await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n    return page\n\nasync with Crawl4aiDockerClient(base_url=\"http://localhost:11235\") as client:\n    result = await client.crawl(\n        [\"https://example.com\"],\n        hooks={\"on_page_context_created\": my_hook}\n    )\nCopy\n```\n\n**Benefits** : - âœ… Write hooks as regular Python functions - âœ… Full IDE support (autocomplete, syntax highlighting, type checking) - âœ… Easy to test and debug - âœ… Reusable hook libraries - âœ… Automatic conversion to API format\n#### Using the Hooks Utility\nThe `hooks_to_string()` utility converts Python function objects to the string format required by the API:\n```\nfrom crawl4ai import hooks_to_string\n\n# Define your hooks as functions\nasync def setup_hook(page, context, **kwargs):\n    await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n    await context.add_cookies([{\n        \"name\": \"session\",\n        \"value\": \"token\",\n        \"domain\": \".example.com\"\n    }])\n    return page\n\nasync def scroll_hook(page, context, **kwargs):\n    await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n    return page\n\n# Convert to string format\nhooks_dict = {\n    \"on_page_context_created\": setup_hook,\n    \"before_retrieve_html\": scroll_hook\n}\nhooks_string = hooks_to_string(hooks_dict)\n\n# Now use with REST API or Docker client\n# hooks_string contains the string representations\nCopy\n```\n\n#### Docker Client with Automatic Conversion\nThe Docker client automatically detects and converts function objects:\n```\nfrom crawl4ai import Crawl4aiDockerClient\n\nasync def auth_hook(page, context, **kwargs):\n    \"\"\"Add authentication cookies\"\"\"\n    await context.add_cookies([{\n        \"name\": \"auth_token\",\n        \"value\": \"your_token\",\n        \"domain\": \".example.com\"\n    }])\n    return page\n\nasync def performance_hook(page, context, **kwargs):\n    \"\"\"Block unnecessary resources\"\"\"\n    await context.route(\"**/*.{png,jpg,gif}\", lambda r: r.abort())\n    await context.route(\"**/analytics/*\", lambda r: r.abort())\n    return page\n\nasync with Crawl4aiDockerClient(base_url=\"http://localhost:11235\") as client:\n    # Pass functions directly - automatic conversion!\n    result = await client.crawl(\n        [\"https://example.com\"],\n        hooks={\n            \"on_page_context_created\": performance_hook,\n            \"before_goto\": auth_hook\n        },\n        hooks_timeout=30  # Optional timeout in seconds (1-120)\n    )\n\n    print(f\"Success: {result.success}\")\n    print(f\"HTML: {len(result.html)} chars\")\nCopy\n```\n\n#### Creating Reusable Hook Libraries\nBuild collections of reusable hooks:\n```\n# hooks_library.py\nclass CrawlHooks:\n    \"\"\"Reusable hook collection for common crawling tasks\"\"\"\n\n    @staticmethod\n    async def block_images(page, context, **kwargs):\n        \"\"\"Block all images to speed up crawling\"\"\"\n        await context.route(\"**/*.{png,jpg,jpeg,gif,webp}\", lambda r: r.abort())\n        return page\n\n    @staticmethod\n    async def block_analytics(page, context, **kwargs):\n        \"\"\"Block analytics and tracking scripts\"\"\"\n        tracking_domains = [\n            \"**/google-analytics.com/*\",\n            \"**/googletagmanager.com/*\",\n            \"**/facebook.com/tr/*\",\n            \"**/doubleclick.net/*\"\n        ]\n        for domain in tracking_domains:\n            await context.route(domain, lambda r: r.abort())\n        return page\n\n    @staticmethod\n    async def scroll_infinite(page, context, **kwargs):\n        \"\"\"Handle infinite scroll to load more content\"\"\"\n        previous_height = 0\n        for i in range(5):  # Max 5 scrolls\n            current_height = await page.evaluate(\"document.body.scrollHeight\")\n            if current_height == previous_height:\n                break\n            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n            await page.wait_for_timeout(1000)\n            previous_height = current_height\n        return page\n\n    @staticmethod\n    async def wait_for_dynamic_content(page, context, url, response, **kwargs):\n        \"\"\"Wait for dynamic content to load\"\"\"\n        await page.wait_for_timeout(2000)\n        try:\n            # Click \"Load More\" if present\n            load_more = await page.query_selector('[class*=\"load-more\"]')\n            if load_more:\n                await load_more.click()\n                await page.wait_for_timeout(1000)\n        except:\n            pass\n        return page\n\n# Use in your application\nfrom hooks_library import CrawlHooks\nfrom crawl4ai import Crawl4aiDockerClient\n\nasync def crawl_with_optimizations(url):\n    async with Crawl4aiDockerClient() as client:\n        result = await client.crawl(\n            [url],\n            hooks={\n                \"on_page_context_created\": CrawlHooks.block_images,\n                \"before_retrieve_html\": CrawlHooks.scroll_infinite\n            }\n        )\n        return result\nCopy\n```\n\n#### Choosing the Right Approach\nApproach | Best For | IDE Support | Language  \n---|---|---|---  \n**String-based** | Non-Python clients, REST APIs, other languages | âŒ None | Any  \n**Function-based** | Python applications, local development | âœ… Full | Python only  \n**Docker Client** | Python apps with automatic conversion | âœ… Full | Python only  \n**Recommendation** : - **Python applications** : Use Docker client with function objects (easiest) - **Non-Python or REST API** : Use string-based hooks (most flexible) - **Manual control** : Use `hooks_to_string()` utility (middle ground)\n#### Complete Example with Function Hooks\n```\nfrom crawl4ai import Crawl4aiDockerClient, BrowserConfig, CrawlerRunConfig, CacheMode\n\n# Define hooks as regular Python functions\nasync def setup_environment(page, context, **kwargs):\n    \"\"\"Setup crawling environment\"\"\"\n    # Set viewport\n    await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n\n    # Block resources for speed\n    await context.route(\"**/*.{png,jpg,gif}\", lambda r: r.abort())\n\n    # Add custom headers\n    await page.set_extra_http_headers({\n        \"Accept-Language\": \"en-US\",\n        \"X-Custom-Header\": \"Crawl4AI\"\n    })\n\n    print(\"[HOOK] Environment configured\")\n    return page\n\nasync def extract_content(page, context, **kwargs):\n    \"\"\"Extract and prepare content\"\"\"\n    # Scroll to load lazy content\n    await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n    await page.wait_for_timeout(1000)\n\n    # Extract metadata\n    metadata = await page.evaluate('''() => ({\n        title: document.title,\n        links: document.links.length,\n        images: document.images.length\n    })''')\n\n    print(f\"[HOOK] Page metadata: {metadata}\")\n    return page\n\nasync def main():\n    async with Crawl4aiDockerClient(base_url=\"http://localhost:11235\", verbose=True) as client:\n        # Configure crawl\n        browser_config = BrowserConfig(headless=True)\n        crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n        # Crawl with hooks\n        result = await client.crawl(\n            [\"https://httpbin.org/html\"],\n            browser_config=browser_config,\n            crawler_config=crawler_config,\n            hooks={\n                \"on_page_context_created\": setup_environment,\n                \"before_retrieve_html\": extract_content\n            },\n            hooks_timeout=30\n        )\n\n        if result.success:\n            print(f\"âœ… Crawl successful!\")\n            print(f\"   URL: {result.url}\")\n            print(f\"   HTML: {len(result.html)} chars\")\n            print(f\"   Markdown: {len(result.markdown)} chars\")\n        else:\n            print(f\"âŒ Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nCopy\n```\n\n#### Additional Resources\n  * **Comprehensive Examples** : See `/docs/examples/hooks_docker_client_example.py` for Python function-based examples\n  * **REST API Examples** : See `/docs/examples/hooks_rest_api_example.py` for string-based examples\n  * **Comparison Guide** : See `/docs/examples/README_HOOKS.md` for detailed comparison\n  * **Utility Documentation** : See `/docs/hooks-utility-guide.md` for complete guide\n\n\n* * *\n## Job Queue & Webhook API\nThe Docker deployment includes a powerful asynchronous job queue system with webhook support for both crawling and LLM extraction tasks. Instead of waiting for long-running operations to complete, submit jobs and receive real-time notifications via webhooks when they finish.\n### Why Use the Job Queue API?\n**Traditional Synchronous API (`/crawl`):** - Client waits for entire crawl to complete - Timeout issues with long-running crawls - Resource blocking during execution - Constant polling required for status updates\n**Asynchronous Job Queue API (`/crawl/job` , `/llm/job`):** - âœ… Submit job and continue immediately - âœ… No timeout concerns for long operations - âœ… Real-time webhook notifications on completion - âœ… Better resource utilization - âœ… Perfect for batch processing - âœ… Ideal for microservice architectures\n### Available Endpoints\n#### 1. Crawl Job Endpoint\n```\nPOST /crawl/job\nCopy\n```\n\nSubmit an asynchronous crawl job with optional webhook notification.\n**Request Body:**\n```\n{\n  \"urls\": [\"https://example.com\"],\n  \"cache_mode\": \"bypass\",\n  \"extraction_strategy\": {\n    \"type\": \"JsonCssExtractionStrategy\",\n    \"schema\": {\n      \"title\": \"h1\",\n      \"content\": \".article-body\"\n    }\n  },\n  \"webhook_config\": {\n    \"webhook_url\": \"https://your-app.com/webhook/crawl-complete\",\n    \"webhook_data_in_payload\": true,\n    \"webhook_headers\": {\n      \"X-Webhook-Secret\": \"your-secret-token\",\n      \"X-Custom-Header\": \"value\"\n    }\n  }\n}\nCopy\n```\n\n**Response:**\n```\n{\n  \"task_id\": \"crawl_1698765432\",\n  \"message\": \"Crawl job submitted\"\n}\nCopy\n```\n\n#### 2. LLM Extraction Job Endpoint\n```\nPOST /llm/job\nCopy\n```\n\nSubmit an asynchronous LLM extraction job with optional webhook notification.\n**Request Body:**\n```\n{\n  \"url\": \"https://example.com/article\",\n  \"q\": \"Extract the article title, author, publication date, and main points\",\n  \"provider\": \"openai/gpt-4o-mini\",\n  \"schema\": \"{\\\"title\\\": \\\"string\\\", \\\"author\\\": \\\"string\\\", \\\"date\\\": \\\"string\\\", \\\"points\\\": [\\\"string\\\"]}\",\n  \"cache\": false,\n  \"webhook_config\": {\n    \"webhook_url\": \"https://your-app.com/webhook/llm-complete\",\n    \"webhook_data_in_payload\": true,\n    \"webhook_headers\": {\n      \"X-Webhook-Secret\": \"your-secret-token\"\n    }\n  }\n}\nCopy\n```\n\n**Response:**\n```\n{\n  \"task_id\": \"llm_1698765432\",\n  \"message\": \"LLM job submitted\"\n}\nCopy\n```\n\n#### 3. Job Status Endpoint\n```\nGET /job/{task_id}\nCopy\n```\n\nCheck the status and retrieve results of a submitted job.\n**Response (In Progress):**\n```\n{\n  \"task_id\": \"crawl_1698765432\",\n  \"status\": \"processing\",\n  \"message\": \"Job is being processed\"\n}\nCopy\n```\n\n**Response (Completed):**\n```\n{\n  \"task_id\": \"crawl_1698765432\",\n  \"status\": \"completed\",\n  \"result\": {\n    \"markdown\": \"# Page Title\\n\\nContent...\",\n    \"extracted_content\": {...},\n    \"links\": {...}\n  }\n}\nCopy\n```\n\n### Webhook Configuration\nWebhooks provide real-time notifications when your jobs complete, eliminating the need for constant polling.\n#### Webhook Config Parameters\nParameter | Type | Required | Description  \n---|---|---|---  \n`webhook_url` | string | Yes | Your HTTP(S) endpoint to receive notifications  \n`webhook_data_in_payload` | boolean | No | Include full result data in webhook payload (default: false)  \n`webhook_headers` | object | No | Custom headers for authentication/identification  \n#### Webhook Payload Format\n**Success Notification (Crawl Job):**\n```\n{\n  \"task_id\": \"crawl_1698765432\",\n  \"task_type\": \"crawl\",\n  \"status\": \"completed\",\n  \"timestamp\": \"2025-10-22T12:30:00.000000+00:00\",\n  \"urls\": [\"https://example.com\"],\n  \"data\": {\n    \"markdown\": \"# Page content...\",\n    \"extracted_content\": {...},\n    \"links\": {...}\n  }\n}\nCopy\n```\n\n**Success Notification (LLM Job):**\n```\n{\n  \"task_id\": \"llm_1698765432\",\n  \"task_type\": \"llm_extraction\",\n  \"status\": \"completed\",\n  \"timestamp\": \"2025-10-22T12:30:00.000000+00:00\",\n  \"urls\": [\"https://example.com/article\"],\n  \"data\": {\n    \"extracted_content\": {\n      \"title\": \"Understanding Web Scraping\",\n      \"author\": \"John Doe\",\n      \"date\": \"2025-10-22\",\n      \"points\": [\"Point 1\", \"Point 2\"]\n    }\n  }\n}\nCopy\n```\n\n**Failure Notification:**\n```\n{\n  \"task_id\": \"crawl_1698765432\",\n  \"task_type\": \"crawl\",\n  \"status\": \"failed\",\n  \"timestamp\": \"2025-10-22T12:30:00.000000+00:00\",\n  \"urls\": [\"https://example.com\"],\n  \"error\": \"Connection timeout after 30 seconds\"\n}\nCopy\n```\n\n#### Webhook Delivery & Retry\n  * **Delivery Method:** HTTP POST to your `webhook_url`\n  * **Content-Type:** `application/json`\n  * **Retry Policy:** Exponential backoff with 5 attempts\n  * Attempt 1: Immediate\n  * Attempt 2: 1 second delay\n  * Attempt 3: 2 seconds delay\n  * Attempt 4: 4 seconds delay\n  * Attempt 5: 8 seconds delay\n  * **Success Status Codes:** 200-299\n  * **Custom Headers:** Your `webhook_headers` are included in every request\n\n\n### Usage Examples\n#### Example 1: Python with Webhook Handler (Flask)\n```\nfrom flask import Flask, request, jsonify\nimport requests\n\napp = Flask(__name__)\n\n# Webhook handler\n@app.route('/webhook/crawl-complete', methods=['POST'])\ndef handle_crawl_webhook():\n    payload = request.json\n\n    if payload['status'] == 'completed':\n        print(f\"âœ… Job {payload['task_id']} completed!\")\n        print(f\"Task type: {payload['task_type']}\")\n\n        # Access the crawl results\n        if 'data' in payload:\n            markdown = payload['data'].get('markdown', '')\n            extracted = payload['data'].get('extracted_content', {})\n            print(f\"Extracted {len(markdown)} characters\")\n            print(f\"Structured data: {extracted}\")\n    else:\n        print(f\"âŒ Job {payload['task_id']} failed: {payload.get('error')}\")\n\n    return jsonify({\"status\": \"received\"}), 200\n\n# Submit a crawl job with webhook\ndef submit_crawl_job():\n    response = requests.post(\n        \"http://localhost:11235/crawl/job\",\n        json={\n            \"urls\": [\"https://example.com\"],\n            \"extraction_strategy\": {\n                \"type\": \"JsonCssExtractionStrategy\",\n                \"schema\": {\n                    \"name\": \"Example Schema\",\n                    \"baseSelector\": \"body\",\n                    \"fields\": [\n                        {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n                        {\"name\": \"description\", \"selector\": \"meta[name='description']\", \"type\": \"attribute\", \"attribute\": \"content\"}\n                    ]\n                }\n            },\n            \"webhook_config\": {\n                \"webhook_url\": \"https://your-app.com/webhook/crawl-complete\",\n                \"webhook_data_in_payload\": True,\n                \"webhook_headers\": {\n                    \"X-Webhook-Secret\": \"your-secret-token\"\n                }\n            }\n        }\n    )\n\n    task_id = response.json()['task_id']\n    print(f\"Job submitted: {task_id}\")\n    return task_id\n\nif __name__ == '__main__':\n    app.run(port=5000)\nCopy\n```\n\n#### Example 2: LLM Extraction with Webhooks\n```\nimport requests\n\ndef submit_llm_job_with_webhook():\n    response = requests.post(\n        \"http://localhost:11235/llm/job\",\n        json={\n            \"url\": \"https://example.com/article\",\n            \"q\": \"Extract the article title, author, and main points\",\n            \"provider\": \"openai/gpt-4o-mini\",\n            \"webhook_config\": {\n                \"webhook_url\": \"https://your-app.com/webhook/llm-complete\",\n                \"webhook_data_in_payload\": True,\n                \"webhook_headers\": {\n                    \"X-Webhook-Secret\": \"your-secret-token\"\n                }\n            }\n        }\n    )\n\n    task_id = response.json()['task_id']\n    print(f\"LLM job submitted: {task_id}\")\n    return task_id\n\n# Webhook handler for LLM jobs\n@app.route('/webhook/llm-complete', methods=['POST'])\ndef handle_llm_webhook():\n    payload = request.json\n\n    if payload['status'] == 'completed':\n        extracted = payload['data']['extracted_content']\n        print(f\"âœ… LLM extraction completed!\")\n        print(f\"Results: {extracted}\")\n    else:\n        print(f\"âŒ LLM extraction failed: {payload.get('error')}\")\n\n    return jsonify({\"status\": \"received\"}), 200\nCopy\n```\n\n#### Example 3: Without Webhooks (Polling)\nIf you don't use webhooks, you can poll for results:\n```\nimport requests\nimport time\n\n# Submit job\nresponse = requests.post(\n    \"http://localhost:11235/crawl/job\",\n    json={\"urls\": [\"https://example.com\"]}\n)\ntask_id = response.json()['task_id']\n\n# Poll for results\nwhile True:\n    result = requests.get(f\"http://localhost:11235/job/{task_id}\")\n    data = result.json()\n\n    if data['status'] == 'completed':\n        print(\"Job completed!\")\n        print(data['result'])\n        break\n    elif data['status'] == 'failed':\n        print(f\"Job failed: {data.get('error')}\")\n        break\n\n    print(\"Still processing...\")\n    time.sleep(2)\nCopy\n```\n\n#### Example 4: Global Webhook Configuration\nSet a default webhook URL in your `config.yml` to avoid repeating it in every request:\n```\n# config.yml\napi:\n  crawler:\n    # ... other settings ...\n    webhook:\n      default_url: \"https://your-app.com/webhook/default\"\n      default_headers:\n        X-Webhook-Secret: \"your-secret-token\"\nCopy\n```\n\nThen submit jobs without webhook config:\n```\n# Uses the global webhook configuration\nresponse = requests.post(\n    \"http://localhost:11235/crawl/job\",\n    json={\"urls\": [\"https://example.com\"]}\n)\nCopy\n```\n\n### Webhook Best Practices\n  1. **Authentication:** Always use custom headers for webhook authentication \n```\n\"webhook_headers\": {\n  \"X-Webhook-Secret\": \"your-secret-token\"\n}\nCopy\n```\n\n  2. **Idempotency:** Design your webhook handler to be idempotent (safe to receive duplicate notifications)\n  3. **Fast Response:** Return HTTP 200 quickly; process data asynchronously if needed \n```\n@app.route('/webhook', methods=['POST'])\ndef webhook():\n    payload = request.json\n    # Queue for background processing\n    queue.enqueue(process_webhook, payload)\n    return jsonify({\"status\": \"received\"}), 200\nCopy\n```\n\n  4. **Error Handling:** Handle both success and failure notifications \n```\nif payload['status'] == 'completed':\n    # Process success\nelif payload['status'] == 'failed':\n    # Log error, retry, or alert\nCopy\n```\n\n  5. **Validation:** Verify webhook authenticity using custom headers \n```\nsecret = request.headers.get('X-Webhook-Secret')\nif secret != os.environ['EXPECTED_SECRET']:\n    return jsonify({\"error\": \"Unauthorized\"}), 401\nCopy\n```\n\n  6. **Logging:** Log webhook deliveries for debugging \n```\nlogger.info(f\"Webhook received: {payload['task_id']} - {payload['status']}\")\nCopy\n```\n\n\n\n### Use Cases\n**1. Batch Processing** Submit hundreds of URLs and get notified as each completes: \n```\nurls = [\"https://site1.com\", \"https://site2.com\", ...]\nfor url in urls:\n    submit_crawl_job(url, webhook_url=\"https://app.com/webhook\")\nCopy\n```\n\n**2. Microservice Integration** Integrate with event-driven architectures: \n```\n# Service A submits job\ntask_id = submit_crawl_job(url)\n\n# Service B receives webhook and triggers next step\n@app.route('/webhook')\ndef webhook():\n    process_result(request.json)\n    trigger_next_service()\n    return \"OK\", 200\nCopy\n```\n\n**3. Long-Running Extractions** Handle complex LLM extractions without timeouts: \n```\nsubmit_llm_job(\n    url=\"https://long-article.com\",\n    q=\"Comprehensive summary with key points and analysis\",\n    webhook_url=\"https://app.com/webhook/llm\"\n)\nCopy\n```\n\n### Troubleshooting\n**Webhook not receiving notifications?** - Check your webhook URL is publicly accessible - Verify firewall/security group settings - Use webhook testing tools like webhook.site for debugging - Check server logs for delivery attempts - Ensure your handler returns 200-299 status code\n**Job stuck in processing?** - Check Redis connection: `docker logs <container_name> | grep redis` - Verify worker processes: `docker exec <container_name> ps aux | grep worker` - Check server logs: `docker logs <container_name>`\n**Need to cancel a job?** Jobs are processed asynchronously. If you need to cancel: - Delete the task from Redis (requires Redis CLI access) - Or implement a cancellation endpoint in your webhook handler\n* * *\n## Dockerfile Parameters\nYou can customize the image build process using build arguments (`--build-arg`). These are typically used via `docker buildx build` or within the `docker-compose.yml` file.\n```\n# Example: Build with 'all' features using buildx\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  --build-arg INSTALL_TYPE=all \\\n  -t yourname/crawl4ai-all:latest \\\n  --load \\\n  . # Build from root context\nCopy\n```\n\n### Build Arguments Explained\nArgument | Description | Default | Options  \n---|---|---|---  \nINSTALL_TYPE | Feature set | `default` |  `default`, `all`, `torch`, `transformer`  \nENABLE_GPU | GPU support (CUDA for AMD64) | `false` |  `true`, `false`  \nAPP_HOME | Install path inside container (advanced) | `/app` | any valid path  \nUSE_LOCAL | Install library from local source | `true` |  `true`, `false`  \nGITHUB_REPO | Git repo to clone if USE_LOCAL=false | _(see Dockerfile)_ | any git URL  \nGITHUB_BRANCH | Git branch to clone if USE_LOCAL=false | `main` | any branch name  \n_(Note: PYTHON_VERSION is fixed by the`FROM` instruction in the Dockerfile)_\n### Build Best Practices\n  1. **Choose the Right Install Type**\n     * `default`: Basic installation, smallest image size. Suitable for most standard web scraping and markdown generation.\n     * `all`: Full features including `torch` and `transformers` for advanced extraction strategies (e.g., CosineStrategy, certain LLM filters). Significantly larger image. Ensure you need these extras.\n  2. **Platform Considerations**\n     * Use `buildx` for building multi-architecture images, especially for pushing to registries.\n     * Use `docker compose` profiles (`local-amd64`, `local-arm64`) for easy platform-specific local builds.\n  3. **Performance Optimization**\n     * The image automatically includes platform-specific optimizations (OpenMP for AMD64, OpenBLAS for ARM64).\n\n\n* * *\n## Using the API\nCommunicate with the running Docker server via its REST API (defaulting to `http://localhost:11235`). You can use the Python SDK or make direct HTTP requests.\n### Playground Interface\nA built-in web playground is available at `http://localhost:11235/playground` for testing and generating API requests. The playground allows you to:\n  1. Configure `CrawlerRunConfig` and `BrowserConfig` using the main library's Python syntax\n  2. Test crawling operations directly from the interface\n  3. Generate corresponding JSON for REST API requests based on your configuration\n\n\nThis is the easiest way to translate Python configuration to JSON requests when building integrations.\n### Python SDK\nInstall the SDK: `pip install crawl4ai`\nThe Python SDK provides a convenient way to interact with the Docker API, including **automatic hook conversion** when using function objects.\n```\nimport asyncio\nfrom crawl4ai.docker_client import Crawl4aiDockerClient\nfrom crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Point to the correct server port\n    async with Crawl4aiDockerClient(base_url=\"http://localhost:11235\", verbose=True) as client:\n        # If JWT is enabled on the server, authenticate first:\n        # await client.authenticate(\"user@example.com\") # See Server Configuration section\n\n        # Example Non-streaming crawl\n        print(\"--- Running Non-Streaming Crawl ---\")\n        results = await client.crawl(\n            [\"https://httpbin.org/html\"],\n            browser_config=BrowserConfig(headless=True),\n            crawler_config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n        )\n        if results:\n            print(f\"Non-streaming results success: {results.success}\")\n            if results.success:\n                for result in results:\n                    print(f\"URL: {result.url}, Success: {result.success}\")\n        else:\n            print(\"Non-streaming crawl failed.\")\n\n        # Example Streaming crawl\n        print(\"\\n--- Running Streaming Crawl ---\")\n        stream_config = CrawlerRunConfig(stream=True, cache_mode=CacheMode.BYPASS)\n        try:\n            async for result in await client.crawl(\n                [\"https://httpbin.org/html\", \"https://httpbin.org/links/5/0\"],\n                browser_config=BrowserConfig(headless=True),\n                crawler_config=stream_config\n            ):\n                print(f\"Streamed result: URL: {result.url}, Success: {result.success}\")\n        except Exception as e:\n            print(f\"Streaming crawl failed: {e}\")\n\n        # Example with hooks (Python function objects)\n        print(\"\\n--- Crawl with Hooks ---\")\n\n        async def my_hook(page, context, **kwargs):\n            \"\"\"Custom hook to optimize performance\"\"\"\n            await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n            await context.route(\"**/*.{png,jpg}\", lambda r: r.abort())\n            print(\"[HOOK] Page optimized\")\n            return page\n\n        result = await client.crawl(\n            [\"https://httpbin.org/html\"],\n            browser_config=BrowserConfig(headless=True),\n            crawler_config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS),\n            hooks={\"on_page_context_created\": my_hook},  # Pass function directly!\n            hooks_timeout=30\n        )\n        print(f\"Crawl with hooks success: {result.success}\")\n\n        # Example Get schema\n        print(\"\\n--- Getting Schema ---\")\n        schema = await client.get_schema()\n        print(f\"Schema received: {bool(schema)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n#### SDK Parameters\nThe Docker client supports the following parameters:\n**Client Initialization** : - `base_url` (str): URL of the Docker server (default: `http://localhost:8000`) - `timeout` (float): Request timeout in seconds (default: 30.0) - `verify_ssl` (bool): Verify SSL certificates (default: True) - `verbose` (bool): Enable verbose logging (default: True) - `log_file` (Optional[str]): Path to log file (default: None)\n**crawl() Method** : - `urls` (List[str]): List of URLs to crawl - `browser_config` (Optional[BrowserConfig]): Browser configuration - `crawler_config` (Optional[CrawlerRunConfig]): Crawler configuration - `hooks` (Optional[Dict]): Hook functions or strings - **automatically converts function objects!** - `hooks_timeout` (int): Timeout for each hook execution in seconds (default: 30)\n**Returns** : - Single URL: `CrawlResult` object - Multiple URLs: `List[CrawlResult]` - Streaming: `AsyncGenerator[CrawlResult]`\n### Second Approach: Direct API Calls\nCrucially, when sending configurations directly via JSON, they **must** follow the `{\"type\": \"ClassName\", \"params\": {...}}` structure for any non-primitive value (like config objects or strategies). Dictionaries must be wrapped as `{\"type\": \"dict\", \"value\": {...}}`.\n_(Keep the detailed explanation of Configuration Structure, Basic Pattern, Simple vs Complex, Strategy Pattern, Complex Nested Example, Quick Grammar Overview, Important Rules, Pro Tip)_\n#### More Examples _(Ensure Schema example uses type/value wrapper)_\n**Advanced Crawler Configuration** _(Keep example, ensure cache_mode uses valid enum value like \"bypass\")_\n**Extraction Strategy**\n```\n{\n    \"crawler_config\": {\n        \"type\": \"CrawlerRunConfig\",\n        \"params\": {\n            \"extraction_strategy\": {\n                \"type\": \"JsonCssExtractionStrategy\",\n                \"params\": {\n                    \"schema\": {\n                        \"type\": \"dict\",\n                        \"value\": {\n                           \"baseSelector\": \"article.post\",\n                           \"fields\": [\n                               {\"name\": \"title\", \"selector\": \"h1\", \"type\": \"text\"},\n                               {\"name\": \"content\", \"selector\": \".content\", \"type\": \"html\"}\n                           ]\n                         }\n                    }\n                }\n            }\n        }\n    }\n}\nCopy\n```\n\n**LLM Extraction Strategy** _(Keep example, ensure schema uses type/value wrapper)_ _(Keep Deep Crawler Example)_\n### LLM Configuration Examples\nThe Docker API supports dynamic LLM configuration through multiple levels:\n#### Temperature Control\nTemperature affects the randomness of LLM responses (0.0 = deterministic, 2.0 = very creative):\n```\nimport requests\n\n# Low temperature for factual extraction\nresponse = requests.post(\n    \"http://localhost:11235/md\",\n    json={\n        \"url\": \"https://example.com\",\n        \"f\": \"llm\",\n        \"q\": \"Extract all dates and numbers from this page\",\n        \"temperature\": 0.2  # Very focused, deterministic\n    }\n)\n\n# High temperature for creative tasks\nresponse = requests.post(\n    \"http://localhost:11235/md\",\n    json={\n        \"url\": \"https://example.com\", \n        \"f\": \"llm\",\n        \"q\": \"Write a creative summary of this content\",\n        \"temperature\": 1.2  # More creative, varied responses\n    }\n)\nCopy\n```\n\n#### Custom API Endpoints\nUse custom base URLs for proxy servers or alternative API endpoints:\n```\n# Using a local LLM server\nresponse = requests.post(\n    \"http://localhost:11235/md\",\n    json={\n        \"url\": \"https://example.com\",\n        \"f\": \"llm\",\n        \"q\": \"Extract key information\",\n        \"provider\": \"ollama/llama2\",\n        \"base_url\": \"http://localhost:11434/v1\"\n    }\n)\nCopy\n```\n\n#### Dynamic Provider Selection\nSwitch between providers based on task requirements:\n```\nasync def smart_extraction(url: str, content_type: str):\n    \"\"\"Select provider and temperature based on content type\"\"\"\n\n    configs = {\n        \"technical\": {\n            \"provider\": \"openai/gpt-4\",\n            \"temperature\": 0.3,\n            \"query\": \"Extract technical specifications and code examples\"\n        },\n        \"creative\": {\n            \"provider\": \"anthropic/claude-3-opus\",\n            \"temperature\": 0.9,\n            \"query\": \"Create an engaging narrative summary\"\n        },\n        \"quick\": {\n            \"provider\": \"groq/mixtral-8x7b\",\n            \"temperature\": 0.5,\n            \"query\": \"Quick summary in bullet points\"\n        }\n    }\n\n    config = configs.get(content_type, configs[\"quick\"])\n\n    response = await httpx.post(\n        \"http://localhost:11235/md\",\n        json={\n            \"url\": url,\n            \"f\": \"llm\",\n            \"q\": config[\"query\"],\n            \"provider\": config[\"provider\"],\n            \"temperature\": config[\"temperature\"]\n        }\n    )\n\n    return response.json()\nCopy\n```\n\n### REST API Examples\nUpdate URLs to use port `11235`.\n#### Simple Crawl\n```\nimport requests\n\n# Configuration objects converted to the required JSON structure\nbrowser_config_payload = {\n    \"type\": \"BrowserConfig\",\n    \"params\": {\"headless\": True}\n}\ncrawler_config_payload = {\n    \"type\": \"CrawlerRunConfig\",\n    \"params\": {\"stream\": False, \"cache_mode\": \"bypass\"} # Use string value of enum\n}\n\ncrawl_payload = {\n    \"urls\": [\"https://httpbin.org/html\"],\n    \"browser_config\": browser_config_payload,\n    \"crawler_config\": crawler_config_payload\n}\nresponse = requests.post(\n    \"http://localhost:11235/crawl\", # Updated port\n    # headers={\"Authorization\": f\"Bearer {token}\"},  # If JWT is enabled\n    json=crawl_payload\n)\nprint(f\"Status Code: {response.status_code}\")\nif response.ok:\n    print(response.json())\nelse:\n    print(f\"Error: {response.text}\")\nCopy\n```\n\n#### Streaming Results\n```\nimport json\nimport httpx # Use httpx for async streaming example\n\nasync def test_stream_crawl(token: str = None): # Made token optional\n    \"\"\"Test the /crawl/stream endpoint with multiple URLs.\"\"\"\n    url = \"http://localhost:11235/crawl/stream\" # Updated port\n    payload = {\n        \"urls\": [\n            \"https://httpbin.org/html\",\n            \"https://httpbin.org/links/5/0\",\n        ],\n        \"browser_config\": {\n            \"type\": \"BrowserConfig\",\n            \"params\": {\"headless\": True, \"viewport\": {\"type\": \"dict\", \"value\": {\"width\": 1200, \"height\": 800}}} # Viewport needs type:dict\n        },\n        \"crawler_config\": {\n            \"type\": \"CrawlerRunConfig\",\n            \"params\": {\"stream\": True, \"cache_mode\": \"bypass\"}\n        }\n    }\n\n    headers = {}\n    # if token:\n    #    headers = {\"Authorization\": f\"Bearer {token}\"} # If JWT is enabled\n\n    try:\n        async with httpx.AsyncClient() as client:\n            async with client.stream(\"POST\", url, json=payload, headers=headers, timeout=120.0) as response:\n                print(f\"Status: {response.status_code} (Expected: 200)\")\n                response.raise_for_status() # Raise exception for bad status codes\n\n                # Read streaming response line-by-line (NDJSON)\n                async for line in response.aiter_lines():\n                    if line:\n                        try:\n                            data = json.loads(line)\n                            # Check for completion marker\n                            if data.get(\"status\") == \"completed\":\n                                print(\"Stream completed.\")\n                                break\n                            print(f\"Streamed Result: {json.dumps(data, indent=2)}\")\n                        except json.JSONDecodeError:\n                            print(f\"Warning: Could not decode JSON line: {line}\")\n\n    except httpx.HTTPStatusError as e:\n         print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n    except Exception as e:\n        print(f\"Error in streaming crawl test: {str(e)}\")\n\n# To run this example:\n# import asyncio\n# asyncio.run(test_stream_crawl())\nCopy\n```\n\n* * *\n## Real-time Monitoring & Operations\nOne of the key advantages of self-hosting is complete visibility into your infrastructure. Crawl4AI includes a comprehensive real-time monitoring system that gives you full transparency and control.\n### Monitoring Dashboard\nAccess the **built-in real-time monitoring dashboard** for complete operational visibility:\n```\nhttp://localhost:11235/monitor\nCopy\n```\n\n![Monitoring Dashboard](https://via.placeholder.com/800x400?text=Crawl4AI+Monitoring+Dashboard)\n**Dashboard Features:**\n#### 1. System Health Overview\n  * **CPU & Memory**: Live usage with progress bars and percentage indicators\n  * **Network I/O** : Total bytes sent/received since startup\n  * **Server Uptime** : How long your server has been running\n  * **Browser Pool Status** :\n  * ðŸ”¥ Permanent browser (always-on default config, ~270MB)\n  * â™¨ï¸ Hot pool (frequently used configs, ~180MB each)\n  * â„ï¸ Cold pool (idle browsers awaiting cleanup, ~180MB each)\n  * **Memory Pressure** : LOW/MEDIUM/HIGH indicator for janitor behavior\n\n\n#### 2. Live Request Tracking\n  * **Active Requests** : Currently running crawls with:\n  * Request ID for tracking\n  * Target URL (truncated for display)\n  * Endpoint being used\n  * Elapsed time (updates in real-time)\n  * Memory usage from start\n  * **Completed Requests** : Last 10 finished requests showing:\n  * Success/failure status (color-coded)\n  * Total execution time\n  * Memory delta (how much memory changed)\n  * Pool hit (was browser reused?)\n  * HTTP status code\n  * **Filtering** : View all, success only, or errors only\n\n\n#### 3. Browser Pool Management\nInteractive table showing all active browsers:\nType | Signature | Age | Last Used | Hits | Actions  \n---|---|---|---|---|---  \npermanent | abc12345 | 2h | 5s ago | 1,247 | Restart  \nhot | def67890 | 45m | 2m ago | 89 | Kill / Restart  \ncold | ghi11213 | 30m | 15m ago | 3 | Kill / Restart  \n  * **Reuse Rate** : Percentage of requests that reused existing browsers\n  * **Memory Estimates** : Total memory used by browser pool\n  * **Manual Control** : Kill or restart individual browsers\n\n\n#### 4. Janitor Events Log\nReal-time log of browser pool cleanup events: - When cold browsers are closed due to memory pressure - When browsers are promoted from cold to hot pool - Forced cleanups triggered manually - Detailed cleanup reasons and browser signatures\n#### 5. Error Monitoring\nRecent errors with full context: - Timestamp - Endpoint where error occurred - Target URL - Error message - Request ID for correlation\n**Live Updates:** The dashboard connects via WebSocket and refreshes every **2 seconds** with the latest data. Connection status indicator shows when you're connected/disconnected.\n* * *\n### Monitor API Endpoints\nFor programmatic monitoring, automation, and integration with your existing infrastructure:\n#### Health & Statistics\n**Get System Health**\n```\nGET /monitor/health\nCopy\n```\n\nReturns current system snapshot: \n```\n{\n  \"container\": {\n    \"memory_percent\": 45.2,\n    \"cpu_percent\": 23.1,\n    \"network_sent_mb\": 1250.45,\n    \"network_recv_mb\": 3421.12,\n    \"uptime_seconds\": 7234\n  },\n  \"pool\": {\n    \"permanent\": {\"active\": true, \"memory_mb\": 270},\n    \"hot\": {\"count\": 3, \"memory_mb\": 540},\n    \"cold\": {\"count\": 1, \"memory_mb\": 180},\n    \"total_memory_mb\": 990\n  },\n  \"janitor\": {\n    \"next_cleanup_estimate\": \"adaptive\",\n    \"memory_pressure\": \"MEDIUM\"\n  }\n}\nCopy\n```\n\n**Get Request Statistics**\n```\nGET /monitor/requests?status=all&limit=50\nCopy\n```\n\nQuery parameters: - `status`: Filter by `all`, `active`, `completed`, `success`, or `error` - `limit`: Number of completed requests to return (1-1000)\n**Get Browser Pool Details**\n```\nGET /monitor/browsers\nCopy\n```\n\nReturns detailed information about all active browsers: \n```\n{\n  \"browsers\": [\n    {\n      \"type\": \"permanent\",\n      \"sig\": \"abc12345\",\n      \"age_seconds\": 7234,\n      \"last_used_seconds\": 5,\n      \"memory_mb\": 270,\n      \"hits\": 1247,\n      \"killable\": false\n    },\n    {\n      \"type\": \"hot\",\n      \"sig\": \"def67890\",\n      \"age_seconds\": 2701,\n      \"last_used_seconds\": 120,\n      \"memory_mb\": 180,\n      \"hits\": 89,\n      \"killable\": true\n    }\n  ],\n  \"summary\": {\n    \"total_count\": 5,\n    \"total_memory_mb\": 990,\n    \"reuse_rate_percent\": 87.3\n  }\n}\nCopy\n```\n\n**Get Endpoint Performance Statistics**\n```\nGET /monitor/endpoints/stats\nCopy\n```\n\nReturns aggregated metrics per endpoint: \n```\n{\n  \"/crawl\": {\n    \"count\": 1523,\n    \"avg_latency_ms\": 2341.5,\n    \"success_rate_percent\": 98.2,\n    \"pool_hit_rate_percent\": 89.1,\n    \"errors\": 27\n  },\n  \"/md\": {\n    \"count\": 891,\n    \"avg_latency_ms\": 1823.7,\n    \"success_rate_percent\": 99.4,\n    \"pool_hit_rate_percent\": 92.3,\n    \"errors\": 5\n  }\n}\nCopy\n```\n\n**Get Timeline Data**\n```\nGET /monitor/timeline?metric=memory&window=5m\nCopy\n```\n\nParameters: - `metric`: `memory`, `requests`, or `browsers` - `window`: Currently only `5m` (5-minute window, 5-second resolution)\nReturns time-series data for charts: \n```\n{\n  \"timestamps\": [1699564800, 1699564805, 1699564810, ...],\n  \"values\": [42.1, 43.5, 41.8, ...]\n}\nCopy\n```\n\n#### Logs\n**Get Janitor Events**\n```\nGET /monitor/logs/janitor?limit=100\nCopy\n```\n\n**Get Error Log**\n```\nGET /monitor/logs/errors?limit=100\nCopy\n```\n\n* * *\n### WebSocket Streaming\nFor real-time monitoring in your own dashboards or applications:\n```\nWS /monitor/ws\nCopy\n```\n\n**Connection Example (Python):**\n```\nimport asyncio\nimport websockets\nimport json\n\nasync def monitor_server():\n    uri = \"ws://localhost:11235/monitor/ws\"\n\n    async with websockets.connect(uri) as websocket:\n        print(\"Connected to Crawl4AI monitor\")\n\n        while True:\n            # Receive update every 2 seconds\n            data = await websocket.recv()\n            update = json.loads(data)\n\n            # Extract key metrics\n            health = update['health']\n            active_requests = len(update['requests']['active'])\n            browsers = len(update['browsers'])\n\n            print(f\"Memory: {health['container']['memory_percent']:.1f}% | \"\n                  f\"Active: {active_requests} | \"\n                  f\"Browsers: {browsers}\")\n\n            # Check for high memory pressure\n            if health['janitor']['memory_pressure'] == 'HIGH':\n                print(\"âš ï¸  HIGH MEMORY PRESSURE - Consider cleanup\")\n\nasyncio.run(monitor_server())\nCopy\n```\n\n**Update Payload Structure:**\n```\n{\n  \"timestamp\": 1699564823.456,\n  \"health\": { /* System health snapshot */ },\n  \"requests\": {\n    \"active\": [ /* Currently running */ ],\n    \"completed\": [ /* Last 10 completed */ ]\n  },\n  \"browsers\": [ /* All active browsers */ ],\n  \"timeline\": {\n    \"memory\": { /* Last 5 minutes */ },\n    \"requests\": { /* Request rate */ },\n    \"browsers\": { /* Pool composition */ }\n  },\n  \"janitor\": [ /* Last 10 cleanup events */ ],\n  \"errors\": [ /* Last 10 errors */ ]\n}\nCopy\n```\n\n* * *\n### Control Actions\nTake manual control when needed:\n**Force Immediate Cleanup**\n```\nPOST /monitor/actions/cleanup\nCopy\n```\n\nKills all cold pool browsers immediately (useful when memory is tight): \n```\n{\n  \"success\": true,\n  \"killed_browsers\": 3\n}\nCopy\n```\n\n**Kill Specific Browser**\n```\nPOST /monitor/actions/kill_browser\nContent-Type: application/json\n\n{\n  \"sig\": \"abc12345\"  // First 8 chars of browser signature\n}\nCopy\n```\n\nResponse: \n```\n{\n  \"success\": true,\n  \"killed_sig\": \"abc12345\",\n  \"pool_type\": \"hot\"\n}\nCopy\n```\n\n**Restart Browser**\n```\nPOST /monitor/actions/restart_browser\nContent-Type: application/json\n\n{\n  \"sig\": \"permanent\"  // Or first 8 chars of signature\n}\nCopy\n```\n\nFor permanent browser, this will close and reinitialize it. For hot/cold browsers, it kills them and lets new requests create fresh ones.\n**Reset Statistics**\n```\nPOST /monitor/stats/reset\nCopy\n```\n\nClears endpoint counters (useful for starting fresh after testing).\n* * *\n### Production Integration\n#### Integration with Existing Monitoring Systems\n**Prometheus Integration:**\n```\n# Scrape metrics endpoint\ncurl http://localhost:11235/metrics\nCopy\n```\n\n**Custom Dashboard Integration:**\n```\n# Example: Push metrics to your monitoring system\nimport asyncio\nimport websockets\nimport json\nfrom your_monitoring import push_metric\n\nasync def integrate_monitoring():\n    async with websockets.connect(\"ws://localhost:11235/monitor/ws\") as ws:\n        while True:\n            data = json.loads(await ws.recv())\n\n            # Push to your monitoring system\n            push_metric(\"crawl4ai.memory.percent\",\n                       data['health']['container']['memory_percent'])\n            push_metric(\"crawl4ai.active_requests\",\n                       len(data['requests']['active']))\n            push_metric(\"crawl4ai.browser_count\",\n                       len(data['browsers']))\nCopy\n```\n\n**Alerting Example:**\n```\nimport requests\nimport time\n\ndef check_health():\n    \"\"\"Poll health endpoint and alert on issues\"\"\"\n    response = requests.get(\"http://localhost:11235/monitor/health\")\n    health = response.json()\n\n    # Alert on high memory\n    if health['container']['memory_percent'] > 85:\n        send_alert(f\"High memory: {health['container']['memory_percent']}%\")\n\n    # Alert on high error rate\n    stats = requests.get(\"http://localhost:11235/monitor/endpoints/stats\").json()\n    for endpoint, metrics in stats.items():\n        if metrics['success_rate_percent'] < 95:\n            send_alert(f\"{endpoint} success rate: {metrics['success_rate_percent']}%\")\n\n# Run every minute\nwhile True:\n    check_health()\n    time.sleep(60)\nCopy\n```\n\n**Log Aggregation:**\n```\nimport requests\nfrom datetime import datetime\n\ndef aggregate_errors():\n    \"\"\"Fetch and aggregate errors for logging system\"\"\"\n    response = requests.get(\"http://localhost:11235/monitor/logs/errors?limit=100\")\n    errors = response.json()['errors']\n\n    for error in errors:\n        log_to_system({\n            'timestamp': datetime.fromtimestamp(error['timestamp']),\n            'service': 'crawl4ai',\n            'endpoint': error['endpoint'],\n            'url': error['url'],\n            'message': error['error'],\n            'request_id': error['request_id']\n        })\nCopy\n```\n\n#### Key Metrics to Track\nFor production self-hosted deployments, monitor these metrics:\n  1. **Memory Usage Trends**\n  2. Track `container.memory_percent` over time\n  3. Alert when consistently above 80%\n  4. Prevents OOM kills\n  5. **Request Success Rates**\n  6. Monitor per-endpoint success rates\n  7. Alert when below 95%\n  8. Indicates crawling issues\n  9. **Average Latency**\n  10. Track `avg_latency_ms` per endpoint\n  11. Detect performance degradation\n  12. Optimize slow endpoints\n  13. **Browser Pool Efficiency**\n  14. Monitor `reuse_rate_percent`\n  15. Should be >80% for good efficiency\n  16. Low rates indicate pool churn\n  17. **Error Frequency**\n  18. Count errors per time window\n  19. Alert on sudden spikes\n  20. Track error patterns\n  21. **Janitor Activity**\n  22. Monitor cleanup frequency\n  23. Excessive cleanup indicates memory pressure\n  24. Adjust pool settings if needed\n\n\n* * *\n### Quick Health Check\nFor simple uptime monitoring:\n```\ncurl http://localhost:11235/health\nCopy\n```\n\nReturns: \n```\n{\n  \"status\": \"healthy\",\n  \"version\": \"0.7.4\"\n}\nCopy\n```\n\nOther useful endpoints: - `/metrics` - Prometheus metrics - `/schema` - Full API schema\n* * *\n## Server Configuration\nThe server's behavior can be customized through the `config.yml` file.\n### Understanding config.yml\nThe configuration file is loaded from `/app/config.yml` inside the container. By default, the file from `deploy/docker/config.yml` in the repository is copied there during the build.\nHere's a detailed breakdown of the configuration options (using defaults from `deploy/docker/config.yml`):\n```\n# Application Configuration\napp:\n  title: \"Crawl4AI API\"\n  version: \"1.0.0\" # Consider setting this to match library version, e.g., \"0.5.1\"\n  host: \"0.0.0.0\"\n  port: 8020 # NOTE: This port is used ONLY when running server.py directly. Gunicorn overrides this (see supervisord.conf).\n  reload: False # Default set to False - suitable for production\n  timeout_keep_alive: 300\n\n# Default LLM Configuration\nllm:\n  provider: \"openai/gpt-4o-mini\"  # Can be overridden by LLM_PROVIDER env var\n  # api_key: sk-...  # If you pass the API key directly (not recommended)\n  # temperature and base_url are controlled via environment variables or request parameters\n\n# Redis Configuration (Used by internal Redis server managed by supervisord)\nredis:\n  host: \"localhost\"\n  port: 6379\n  db: 0\n  password: \"\"\n  # ... other redis options ...\n\n# Rate Limiting Configuration\nrate_limiting:\n  enabled: True\n  default_limit: \"1000/minute\"\n  trusted_proxies: []\n  storage_uri: \"memory://\"  # Use \"redis://localhost:6379\" if you need persistent/shared limits\n\n# Security Configuration\nsecurity:\n  enabled: false # Master toggle for security features\n  jwt_enabled: false # Enable JWT authentication (requires security.enabled=true)\n  https_redirect: false # Force HTTPS (requires security.enabled=true)\n  trusted_hosts: [\"*\"] # Allowed hosts (use specific domains in production)\n  headers: # Security headers (applied if security.enabled=true)\n    x_content_type_options: \"nosniff\"\n    x_frame_options: \"DENY\"\n    content_security_policy: \"default-src 'self'\"\n    strict_transport_security: \"max-age=63072000; includeSubDomains\"\n\n# Crawler Configuration\ncrawler:\n  memory_threshold_percent: 95.0\n  rate_limiter:\n    base_delay: [1.0, 2.0] # Min/max delay between requests in seconds for dispatcher\n  timeouts:\n    stream_init: 30.0  # Timeout for stream initialization\n    batch_process: 300.0 # Timeout for non-streaming /crawl processing\n\n# Logging Configuration\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n# Observability Configuration\nobservability:\n  prometheus:\n    enabled: True\n    endpoint: \"/metrics\"\n  health_check:\n    endpoint: \"/health\"\nCopy\n```\n\n_(JWT Authentication section remains the same, just note the default port is now 11235 for requests)_\n_(Configuration Tips and Best Practices remain the same)_\n### Customizing Your Configuration\nYou can override the default `config.yml`.\n#### Method 1: Modify Before Build\n  1. Edit the `deploy/docker/config.yml` file in your local repository clone.\n  2. Build the image using `docker buildx` or `docker compose --profile local-... up --build`. The modified file will be copied into the image.\n\n\n#### Method 2: Runtime Mount (Recommended for Custom Deploys)\n  1. Create your custom configuration file, e.g., `my-custom-config.yml` locally. Ensure it contains all necessary sections.\n  2. Mount it when running the container:\n     * **Using`docker run` :**\n```\n# Assumes my-custom-config.yml is in the current directory\ndocker run -d -p 11235:11235 \\\n  --name crawl4ai-custom-config \\\n  --env-file .llm.env \\\n  --shm-size=1g \\\n  -v $(pwd)/my-custom-config.yml:/app/config.yml \\\n  unclecode/crawl4ai:latest # Or your specific tag\nCopy\n```\n\n     * **Using`docker-compose.yml` :** Add a `volumes` section to the service definition: \n```\nservices:\n  crawl4ai-hub-amd64: # Or your chosen service\n    image: unclecode/crawl4ai:latest\n    profiles: [\"hub-amd64\"]\n    <<: *base-config\n    volumes:\n      # Mount local custom config over the default one in the container\n      - ./my-custom-config.yml:/app/config.yml\n      # Keep the shared memory volume from base-config\n      - /dev/shm:/dev/shm\nCopy\n```\n\n_(Note: Ensure`my-custom-config.yml` is in the same directory as `docker-compose.yml`)_\n\n\n> ðŸ’¡ When mounting, your custom file _completely replaces_ the default one. Ensure it's a valid and complete configuration.\n### Configuration Recommendations\n  1. **Security First** ðŸ”’\n  2. Always enable security in production\n  3. Use specific trusted_hosts instead of wildcards\n  4. Set up proper rate limiting to protect your server\n  5. Consider your environment before enabling HTTPS redirect\n  6. **Resource Management** ðŸ’»\n  7. Adjust memory_threshold_percent based on available RAM\n  8. Set timeouts according to your content size and network conditions\n  9. Use Redis for rate limiting in multi-container setups\n  10. **Monitoring** ðŸ“Š\n  11. Enable Prometheus if you need metrics\n  12. Set DEBUG logging in development, INFO in production\n  13. Regular health check monitoring is crucial\n  14. **Performance Tuning** âš¡\n  15. Start with conservative rate limiter delays\n  16. Increase batch_process timeout for large content\n  17. Adjust stream_init timeout based on initial response times\n\n\n## Getting Help\nWe're here to help you succeed with Crawl4AI! Here's how to get support:\n  * ðŸ“– Check our [full documentation](https://docs.crawl4ai.com)\n  * ðŸ› Found a bug? [Open an issue](https://github.com/unclecode/crawl4ai/issues)\n  * ðŸ’¬ Join our [Discord community](https://discord.gg/crawl4ai)\n  * â­ Star us on GitHub to show support!\n\n\n## Summary\nCongratulations! You now have everything you need to self-host your own Crawl4AI infrastructure with complete control and visibility.\n**What You've Learned:** - âœ… Multiple deployment options (Docker Hub, Docker Compose, manual builds) - âœ… Environment configuration and LLM integration - âœ… Using the interactive playground for testing - âœ… Making API requests with proper typing (SDK and REST) - âœ… Specialized endpoints (screenshots, PDFs, JavaScript execution) - âœ… MCP integration for AI-assisted development - âœ… **Real-time monitoring dashboard** for operational transparency - âœ… **Monitor API** for programmatic control and integration - âœ… Production deployment best practices\n**Why This Matters:**\nBy self-hosting Crawl4AI, you: - ðŸ”’ **Own Your Data** : Everything stays in your infrastructure - ðŸ“Š **See Everything** : Real-time dashboard shows exactly what's happening - ðŸ’° **Control Costs** : Scale within your resources, no per-request fees - âš¡ **Maximize Performance** : Direct access with smart browser pooling (10x memory efficiency) - ðŸ›¡ï¸ **Stay Secure** : Keep sensitive workflows behind your firewall - ðŸ”§ **Customize Freely** : Full control over configs, strategies, and optimizations\n**Next Steps:**\n  1. **Start Simple** : Deploy with Docker Hub image and test with the playground\n  2. **Monitor Everything** : Open `http://localhost:11235/monitor` to watch your server\n  3. **Integrate** : Connect your applications using the Python SDK or REST API\n  4. **Scale Smart** : Use the monitoring data to optimize your deployment\n  5. **Go Production** : Set up alerting, log aggregation, and automated cleanup\n\n\n**Key Resources:** - ðŸŽ® **Playground** : `http://localhost:11235/playground` - Interactive testing - ðŸ“Š **Monitor Dashboard** : `http://localhost:11235/monitor` - Real-time visibility - ðŸ“– **Architecture Docs** : `deploy/docker/ARCHITECTURE.md` - Deep technical dive - ðŸ’¬ **Discord Community** : Get help and share experiences - â­ **GitHub** : Report issues, contribute, show support\nRemember: The monitoring dashboard is your window into your infrastructure. Use it to understand performance, troubleshoot issues, and optimize your deployment. The examples in the `examples` folder show real-world usage patterns you can adapt.\n**You're now in control of your web crawling destiny!** ðŸš€\nHappy crawling! ðŸ•·ï¸\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/self-hosting/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/self-hosting/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/self-hosting/)\n\n\nESC to close\n#### On this page\n  * [Why Self-Host?](https://docs.crawl4ai.com/core/self-hosting/#why-self-host)\n  * [Table of Contents](https://docs.crawl4ai.com/core/self-hosting/#table-of-contents)\n  * [Prerequisites](https://docs.crawl4ai.com/core/self-hosting/#prerequisites)\n  * [Installation](https://docs.crawl4ai.com/core/self-hosting/#installation)\n  * [Option 1: Using Pre-built Docker Hub Images (Recommended)](https://docs.crawl4ai.com/core/self-hosting/#option-1-using-pre-built-docker-hub-images-recommended)\n  * [1. Pull the Image](https://docs.crawl4ai.com/core/self-hosting/#1-pull-the-image)\n  * [2. Setup Environment (API Keys)](https://docs.crawl4ai.com/core/self-hosting/#2-setup-environment-api-keys)\n  * [3. Run the Container](https://docs.crawl4ai.com/core/self-hosting/#3-run-the-container)\n  * [4. Stopping the Container](https://docs.crawl4ai.com/core/self-hosting/#4-stopping-the-container)\n  * [Docker Hub Versioning Explained](https://docs.crawl4ai.com/core/self-hosting/#docker-hub-versioning-explained)\n  * [Option 2: Using Docker Compose](https://docs.crawl4ai.com/core/self-hosting/#option-2-using-docker-compose)\n  * [1. Clone Repository](https://docs.crawl4ai.com/core/self-hosting/#1-clone-repository)\n  * [2. Environment Setup (API Keys)](https://docs.crawl4ai.com/core/self-hosting/#2-environment-setup-api-keys)\n  * [3. Build and Run with Compose](https://docs.crawl4ai.com/core/self-hosting/#3-build-and-run-with-compose)\n  * [4. Stopping the Service](https://docs.crawl4ai.com/core/self-hosting/#4-stopping-the-service)\n  * [Option 3: Manual Local Build & Run](https://docs.crawl4ai.com/core/self-hosting/#option-3-manual-local-build-run)\n  * [1. Clone Repository & Setup Environment](https://docs.crawl4ai.com/core/self-hosting/#1-clone-repository-setup-environment)\n  * [2. Build the Image (Multi-Arch)](https://docs.crawl4ai.com/core/self-hosting/#2-build-the-image-multi-arch)\n  * [3. Run the Container](https://docs.crawl4ai.com/core/self-hosting/#3-run-the-container_1)\n  * [4. Stopping the Manual Container](https://docs.crawl4ai.com/core/self-hosting/#4-stopping-the-manual-container)\n  * [MCP (Model Context Protocol) Support](https://docs.crawl4ai.com/core/self-hosting/#mcp-model-context-protocol-support)\n  * [What is MCP?](https://docs.crawl4ai.com/core/self-hosting/#what-is-mcp)\n  * [Connecting via MCP](https://docs.crawl4ai.com/core/self-hosting/#connecting-via-mcp)\n  * [Using with Claude Code](https://docs.crawl4ai.com/core/self-hosting/#using-with-claude-code)\n  * [Available MCP Tools](https://docs.crawl4ai.com/core/self-hosting/#available-mcp-tools)\n  * [Testing MCP Connections](https://docs.crawl4ai.com/core/self-hosting/#testing-mcp-connections)\n  * [MCP Schemas](https://docs.crawl4ai.com/core/self-hosting/#mcp-schemas)\n  * [Additional API Endpoints](https://docs.crawl4ai.com/core/self-hosting/#additional-api-endpoints)\n  * [HTML Extraction Endpoint](https://docs.crawl4ai.com/core/self-hosting/#html-extraction-endpoint)\n  * [Screenshot Endpoint](https://docs.crawl4ai.com/core/self-hosting/#screenshot-endpoint)\n  * [PDF Export Endpoint](https://docs.crawl4ai.com/core/self-hosting/#pdf-export-endpoint)\n  * [JavaScript Execution Endpoint](https://docs.crawl4ai.com/core/self-hosting/#javascript-execution-endpoint)\n  * [User-Provided Hooks API](https://docs.crawl4ai.com/core/self-hosting/#user-provided-hooks-api)\n  * [Hook Information Endpoint](https://docs.crawl4ai.com/core/self-hosting/#hook-information-endpoint)\n  * [Available Hook Points](https://docs.crawl4ai.com/core/self-hosting/#available-hook-points)\n  * [Using Hooks in Requests](https://docs.crawl4ai.com/core/self-hosting/#using-hooks-in-requests)\n  * [Hook Examples with Real URLs](https://docs.crawl4ai.com/core/self-hosting/#hook-examples-with-real-urls)\n  * [1. Authentication with Cookies (GitHub)](https://docs.crawl4ai.com/core/self-hosting/#1-authentication-with-cookies-github)\n  * [2. Basic Authentication (httpbin.org for testing)](https://docs.crawl4ai.com/core/self-hosting/#2-basic-authentication-httpbinorg-for-testing)\n  * [3. Performance Optimization (News Sites)](https://docs.crawl4ai.com/core/self-hosting/#3-performance-optimization-news-sites)\n  * [4. Handling Infinite Scroll (Twitter/X)](https://docs.crawl4ai.com/core/self-hosting/#4-handling-infinite-scroll-twitterx)\n  * [5. E-commerce Login (Example Pattern)](https://docs.crawl4ai.com/core/self-hosting/#5-e-commerce-login-example-pattern)\n  * [6. Extracting Structured Data (Wikipedia)](https://docs.crawl4ai.com/core/self-hosting/#6-extracting-structured-data-wikipedia)\n  * [Security Best Practices](https://docs.crawl4ai.com/core/self-hosting/#security-best-practices)\n  * [Hook Response Information](https://docs.crawl4ai.com/core/self-hosting/#hook-response-information)\n  * [Error Handling](https://docs.crawl4ai.com/core/self-hosting/#error-handling)\n  * [Complete Example: Safe Multi-Hook Crawling](https://docs.crawl4ai.com/core/self-hosting/#complete-example-safe-multi-hook-crawling)\n  * [Hooks Utility: Function-Based Approach (Python)](https://docs.crawl4ai.com/core/self-hosting/#hooks-utility-function-based-approach-python)\n  * [Why Use Function-Based Hooks?](https://docs.crawl4ai.com/core/self-hosting/#why-use-function-based-hooks)\n  * [Using the Hooks Utility](https://docs.crawl4ai.com/core/self-hosting/#using-the-hooks-utility)\n  * [Docker Client with Automatic Conversion](https://docs.crawl4ai.com/core/self-hosting/#docker-client-with-automatic-conversion)\n  * [Creating Reusable Hook Libraries](https://docs.crawl4ai.com/core/self-hosting/#creating-reusable-hook-libraries)\n  * [Choosing the Right Approach](https://docs.crawl4ai.com/core/self-hosting/#choosing-the-right-approach)\n  * [Complete Example with Function Hooks](https://docs.crawl4ai.com/core/self-hosting/#complete-example-with-function-hooks)\n  * [Additional Resources](https://docs.crawl4ai.com/core/self-hosting/#additional-resources)\n  * [Job Queue & Webhook API](https://docs.crawl4ai.com/core/self-hosting/#job-queue-webhook-api)\n  * [Why Use the Job Queue API?](https://docs.crawl4ai.com/core/self-hosting/#why-use-the-job-queue-api)\n  * [Available Endpoints](https://docs.crawl4ai.com/core/self-hosting/#available-endpoints)\n  * [1. Crawl Job Endpoint](https://docs.crawl4ai.com/core/self-hosting/#1-crawl-job-endpoint)\n  * [2. LLM Extraction Job Endpoint](https://docs.crawl4ai.com/core/self-hosting/#2-llm-extraction-job-endpoint)\n  * [3. Job Status Endpoint](https://docs.crawl4ai.com/core/self-hosting/#3-job-status-endpoint)\n  * [Webhook Configuration](https://docs.crawl4ai.com/core/self-hosting/#webhook-configuration)\n  * [Webhook Config Parameters](https://docs.crawl4ai.com/core/self-hosting/#webhook-config-parameters)\n  * [Webhook Payload Format](https://docs.crawl4ai.com/core/self-hosting/#webhook-payload-format)\n  * [Webhook Delivery & Retry](https://docs.crawl4ai.com/core/self-hosting/#webhook-delivery-retry)\n  * [Usage Examples](https://docs.crawl4ai.com/core/self-hosting/#usage-examples)\n  * [Example 1: Python with Webhook Handler (Flask)](https://docs.crawl4ai.com/core/self-hosting/#example-1-python-with-webhook-handler-flask)\n  * [Example 2: LLM Extraction with Webhooks](https://docs.crawl4ai.com/core/self-hosting/#example-2-llm-extraction-with-webhooks)\n  * [Example 3: Without Webhooks (Polling)](https://docs.crawl4ai.com/core/self-hosting/#example-3-without-webhooks-polling)\n  * [Example 4: Global Webhook Configuration](https://docs.crawl4ai.com/core/self-hosting/#example-4-global-webhook-configuration)\n  * [Webhook Best Practices](https://docs.crawl4ai.com/core/self-hosting/#webhook-best-practices)\n  * [Use Cases](https://docs.crawl4ai.com/core/self-hosting/#use-cases)\n  * [Troubleshooting](https://docs.crawl4ai.com/core/self-hosting/#troubleshooting)\n  * [Dockerfile Parameters](https://docs.crawl4ai.com/core/self-hosting/#dockerfile-parameters)\n  * [Build Arguments Explained](https://docs.crawl4ai.com/core/self-hosting/#build-arguments-explained)\n  * [Build Best Practices](https://docs.crawl4ai.com/core/self-hosting/#build-best-practices)\n  * [Using the API](https://docs.crawl4ai.com/core/self-hosting/#using-the-api)\n  * [Playground Interface](https://docs.crawl4ai.com/core/self-hosting/#playground-interface)\n  * [Python SDK](https://docs.crawl4ai.com/core/self-hosting/#python-sdk)\n  * [SDK Parameters](https://docs.crawl4ai.com/core/self-hosting/#sdk-parameters)\n  * [Second Approach: Direct API Calls](https://docs.crawl4ai.com/core/self-hosting/#second-approach-direct-api-calls)\n  * [More Examples (Ensure Schema example uses type/value wrapper)](https://docs.crawl4ai.com/core/self-hosting/#more-examples-ensure-schema-example-uses-typevalue-wrapper)\n  * [LLM Configuration Examples](https://docs.crawl4ai.com/core/self-hosting/#llm-configuration-examples)\n  * [Temperature Control](https://docs.crawl4ai.com/core/self-hosting/#temperature-control)\n  * [Custom API Endpoints](https://docs.crawl4ai.com/core/self-hosting/#custom-api-endpoints)\n  * [Dynamic Provider Selection](https://docs.crawl4ai.com/core/self-hosting/#dynamic-provider-selection)\n  * [REST API Examples](https://docs.crawl4ai.com/core/self-hosting/#rest-api-examples)\n  * [Simple Crawl](https://docs.crawl4ai.com/core/self-hosting/#simple-crawl)\n  * [Streaming Results](https://docs.crawl4ai.com/core/self-hosting/#streaming-results)\n  * [Real-time Monitoring & Operations](https://docs.crawl4ai.com/core/self-hosting/#real-time-monitoring-operations)\n  * [Monitoring Dashboard](https://docs.crawl4ai.com/core/self-hosting/#monitoring-dashboard)\n  * [1. System Health Overview](https://docs.crawl4ai.com/core/self-hosting/#1-system-health-overview)\n  * [2. Live Request Tracking](https://docs.crawl4ai.com/core/self-hosting/#2-live-request-tracking)\n  * [3. Browser Pool Management](https://docs.crawl4ai.com/core/self-hosting/#3-browser-pool-management)\n  * [4. Janitor Events Log](https://docs.crawl4ai.com/core/self-hosting/#4-janitor-events-log)\n  * [5. Error Monitoring](https://docs.crawl4ai.com/core/self-hosting/#5-error-monitoring)\n  * [Monitor API Endpoints](https://docs.crawl4ai.com/core/self-hosting/#monitor-api-endpoints)\n  * [Health & Statistics](https://docs.crawl4ai.com/core/self-hosting/#health-statistics)\n  * [Logs](https://docs.crawl4ai.com/core/self-hosting/#logs)\n  * [WebSocket Streaming](https://docs.crawl4ai.com/core/self-hosting/#websocket-streaming)\n  * [Control Actions](https://docs.crawl4ai.com/core/self-hosting/#control-actions)\n  * [Production Integration](https://docs.crawl4ai.com/core/self-hosting/#production-integration)\n  * [Integration with Existing Monitoring Systems](https://docs.crawl4ai.com/core/self-hosting/#integration-with-existing-monitoring-systems)\n  * [Key Metrics to Track](https://docs.crawl4ai.com/core/self-hosting/#key-metrics-to-track)\n  * [Quick Health Check](https://docs.crawl4ai.com/core/self-hosting/#quick-health-check)\n  * [Server Configuration](https://docs.crawl4ai.com/core/self-hosting/#server-configuration)\n  * [Understanding config.yml](https://docs.crawl4ai.com/core/self-hosting/#understanding-configyml)\n  * [Customizing Your Configuration](https://docs.crawl4ai.com/core/self-hosting/#customizing-your-configuration)\n  * [Method 1: Modify Before Build](https://docs.crawl4ai.com/core/self-hosting/#method-1-modify-before-build)\n  * [Method 2: Runtime Mount (Recommended for Custom Deploys)](https://docs.crawl4ai.com/core/self-hosting/#method-2-runtime-mount-recommended-for-custom-deploys)\n  * [Configuration Recommendations](https://docs.crawl4ai.com/core/self-hosting/#configuration-recommendations)\n  * [Getting Help](https://docs.crawl4ai.com/core/self-hosting/#getting-help)\n  * [Summary](https://docs.crawl4ai.com/core/self-hosting/#summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/simple-crawling",
    "depth": 1,
    "title": "Simple Crawling - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "21c36074f561d4ad593e6f6237928228",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/simple-crawling/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * Simple Crawling\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/#simple-crawling)\n  * [Basic Usage](https://docs.crawl4ai.com/core/simple-crawling/#basic-usage)\n  * [Understanding the Response](https://docs.crawl4ai.com/core/simple-crawling/#understanding-the-response)\n  * [Adding Basic Options](https://docs.crawl4ai.com/core/simple-crawling/#adding-basic-options)\n  * [Handling Errors](https://docs.crawl4ai.com/core/simple-crawling/#handling-errors)\n  * [Logging and Debugging](https://docs.crawl4ai.com/core/simple-crawling/#logging-and-debugging)\n  * [Complete Example](https://docs.crawl4ai.com/core/simple-crawling/#complete-example)\n\n\n# Simple Crawling\nThis guide covers the basics of web crawling with Crawl4AI. You'll learn how to set up a crawler, make your first request, and understand the response.\n## Basic Usage\nSet up a simple crawl using `BrowserConfig` and `CrawlerRunConfig`:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_config = BrowserConfig()  # Default browser configuration\n    run_config = CrawlerRunConfig()   # Default crawl run configuration\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n## Understanding the Response\nThe `arun()` method returns a `CrawlResult` object with several useful properties. Here's a quick overview (see [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/) for complete details):\n```\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.6),\n        options={\"ignore_links\": True}\n    )\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=config\n)\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown.raw_markdown) # Raw markdown from cleaned html\nprint(result.markdown.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links\nCopy\n```\n\n## Adding Basic Options\nCustomize your crawl using `CrawlerRunConfig`:\n```\nrun_config = CrawlerRunConfig(\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=run_config\n)\nCopy\n```\n\n## Handling Errors\nAlways check if the crawl was successful:\n```\nrun_config = CrawlerRunConfig()\nresult = await crawler.arun(url=\"https://example.com\", config=run_config)\n\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")\nCopy\n```\n\n## Logging and Debugging\nEnable verbose logging in `BrowserConfig`:\n```\nbrowser_config = BrowserConfig(verbose=True)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    run_config = CrawlerRunConfig()\n    result = await crawler.arun(url=\"https://example.com\", config=run_config)\nCopy\n```\n\n## Complete Example\nHere's a more comprehensive example demonstrating common usage patterns:\n```\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        # Content filtering\n        word_count_threshold=10,\n        excluded_tags=['form', 'header'],\n        exclude_external_links=True,\n\n        # Content processing\n        process_iframes=True,\n        remove_overlay_elements=True,\n\n        # Cache control\n        cache_mode=CacheMode.ENABLED  # Use cache if available\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n\n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n\n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n\n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n\n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/simple-crawling/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/simple-crawling/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/simple-crawling/)\n\n\nESC to close\n#### On this page\n  * [Basic Usage](https://docs.crawl4ai.com/core/simple-crawling/#basic-usage)\n  * [Understanding the Response](https://docs.crawl4ai.com/core/simple-crawling/#understanding-the-response)\n  * [Adding Basic Options](https://docs.crawl4ai.com/core/simple-crawling/#adding-basic-options)\n  * [Handling Errors](https://docs.crawl4ai.com/core/simple-crawling/#handling-errors)\n  * [Logging and Debugging](https://docs.crawl4ai.com/core/simple-crawling/#logging-and-debugging)\n  * [Complete Example](https://docs.crawl4ai.com/core/simple-crawling/#complete-example)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/core/url-seeding",
    "depth": 1,
    "title": "URL Seeding - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "8f4d452f88c03f8e353d0a20a33216f4",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/core/url-seeding/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * URL Seeding\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [URL Seeding: The Smart Way to Crawl at Scale](https://docs.crawl4ai.com/core/url-seeding/#url-seeding-the-smart-way-to-crawl-at-scale)\n  * [Why URL Seeding?](https://docs.crawl4ai.com/core/url-seeding/#why-url-seeding)\n  * [Your First URL Seeding Adventure](https://docs.crawl4ai.com/core/url-seeding/#your-first-url-seeding-adventure)\n  * [Understanding the URL Seeder](https://docs.crawl4ai.com/core/url-seeding/#understanding-the-url-seeder)\n  * [Smart Filtering with BM25 Scoring](https://docs.crawl4ai.com/core/url-seeding/#smart-filtering-with-bm25-scoring)\n  * [Scaling Up: Multiple Domains](https://docs.crawl4ai.com/core/url-seeding/#scaling-up-multiple-domains)\n  * [Advanced Integration Patterns](https://docs.crawl4ai.com/core/url-seeding/#advanced-integration-patterns)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/url-seeding/#best-practices-tips)\n  * [Quick Reference](https://docs.crawl4ai.com/core/url-seeding/#quick-reference)\n  * [Conclusion](https://docs.crawl4ai.com/core/url-seeding/#conclusion)\n\n\n# URL Seeding: The Smart Way to Crawl at Scale\n## Why URL Seeding?\nWeb crawling comes in different flavors, each with its own strengths. Let's understand when to use URL seeding versus deep crawling.\n### Deep Crawling: Real-Time Discovery\nDeep crawling is perfect when you need: - **Fresh, real-time data** - discovering pages as they're created - **Dynamic exploration** - following links based on content - **Selective extraction** - stopping when you find what you need\n```\n# Deep crawling example: Explore a website dynamically\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n\nasync def deep_crawl_example():\n    # Configure a 2-level deep crawl\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(\n            max_depth=2,           # Crawl 2 levels deep\n            include_external=False, # Stay within domain\n            max_pages=50           # Limit for efficiency\n        ),\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Start crawling and follow links dynamically\n        results = await crawler.arun(\"https://example.com\", config=config)\n\n        print(f\"Discovered and crawled {len(results)} pages\")\n        for result in results[:3]:\n            print(f\"Found: {result.url} at depth {result.metadata.get('depth', 0)}\")\n\nasyncio.run(deep_crawl_example())\nCopy\n```\n\n### URL Seeding: Bulk Discovery\nURL seeding shines when you want: - **Comprehensive coverage** - get thousands of URLs in seconds - **Bulk processing** - filter before crawling - **Resource efficiency** - know exactly what you'll crawl\n```\n# URL seeding example: Analyze all documentation\nfrom crawl4ai import AsyncUrlSeeder, SeedingConfig\n\nseeder = AsyncUrlSeeder()\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    pattern=\"*/docs/*\"\n)\n\n# Get ALL documentation URLs instantly\nurls = await seeder.urls(\"example.com\", config)\n# 1000+ URLs discovered in seconds!\nCopy\n```\n\n### The Trade-offs\nAspect | Deep Crawling | URL Seeding  \n---|---|---  \n**Coverage** | Discovers pages dynamically | Gets most existing URLs instantly  \n**Freshness** | Finds brand new pages | May miss very recent pages  \n**Speed** | Slower, page by page | Extremely fast bulk discovery  \n**Resource Usage** | Higher - crawls to discover | Lower - discovers then crawls  \n**Control** | Can stop mid-process | Pre-filters before crawling  \n### When to Use Each\n**Choose Deep Crawling when:** - You need the absolute latest content - You're searching for specific information - The site structure is unknown or dynamic - You want to stop as soon as you find what you need\n**Choose URL Seeding when:** - You need to analyze large portions of a site - You want to filter URLs before crawling - You're doing comparative analysis - You need to optimize resource usage\nThe magic happens when you understand both approaches and choose the right tool for your task. Sometimes, you might even combine them - use URL seeding for bulk discovery, then deep crawl specific sections for the latest updates.\n## Your First URL Seeding Adventure\nLet's see the magic in action. We'll discover blog posts about Python, filter for tutorials, and crawl only those pages.\n```\nimport asyncio\nfrom crawl4ai import AsyncUrlSeeder, AsyncWebCrawler, SeedingConfig, CrawlerRunConfig\n\nasync def smart_blog_crawler():\n    # Step 1: Create our URL discoverer\n    seeder = AsyncUrlSeeder()\n\n    # Step 2: Configure discovery - let's find all blog posts\n    config = SeedingConfig(\n        source=\"sitemap+cc\",      # Use the website's sitemap+cc\n        pattern=\"*/courses/*\",    # Only courses related posts\n        extract_head=True,          # Get page metadata\n        max_urls=100               # Limit for this example\n    )\n\n    # Step 3: Discover URLs from the Python blog\n    print(\"ðŸ” Discovering course posts...\")\n    urls = await seeder.urls(\"realpython.com\", config)\n    print(f\"âœ… Found {len(urls)} course posts\")\n\n    # Step 4: Filter for Python tutorials (using metadata!)\n    tutorials = [\n        url for url in urls \n        if url[\"status\"] == \"valid\" and \n        any(keyword in str(url[\"head_data\"]).lower() \n            for keyword in [\"tutorial\", \"guide\", \"how to\"])\n    ]\n    print(f\"ðŸ“š Filtered to {len(tutorials)} tutorials\")\n\n    # Step 5: Show what we found\n    print(\"\\nðŸŽ¯ Found these tutorials:\")\n    for tutorial in tutorials[:5]:  # First 5\n        title = tutorial[\"head_data\"].get(\"title\", \"No title\")\n        print(f\"  - {title}\")\n        print(f\"    {tutorial['url']}\")\n\n    # Step 6: Now crawl ONLY these relevant pages\n    print(\"\\nðŸš€ Crawling tutorials...\")\n    async with AsyncWebCrawler() as crawler:\n        config = CrawlerRunConfig(\n            only_text=True,\n            word_count_threshold=300,  # Only substantial articles\n            stream=True\n        )\n\n        # Extract URLs and crawl them\n        tutorial_urls = [t[\"url\"] for t in tutorials[:10]]\n        results = await crawler.arun_many(tutorial_urls, config=config)\n\n        successful = 0\n        async for result in results:\n            if result.success:\n                successful += 1\n                print(f\"âœ“ Crawled: {result.url[:60]}...\")\n\n        print(f\"\\nâœ¨ Successfully crawled {successful} tutorials!\")\n\n# Run it!\nasyncio.run(smart_blog_crawler())\nCopy\n```\n\n**What just happened?**\n  1. We discovered all blog URLs from the sitemap+cc\n  2. We filtered using metadata (no crawling needed!)\n  3. We crawled only the relevant tutorials\n  4. We saved tons of time and bandwidth\n\n\nThis is the power of URL seeding - you see everything before you crawl anything.\n## Understanding the URL Seeder\nNow that you've seen the magic, let's understand how it works.\n### Basic Usage\nCreating a URL seeder is simple:\n```\nfrom crawl4ai import AsyncUrlSeeder\n\n# Method 1: Manual cleanup\nseeder = AsyncUrlSeeder()\ntry:\n    config = SeedingConfig(source=\"sitemap\")\n    urls = await seeder.urls(\"example.com\", config)\nfinally:\n    await seeder.close()\n\n# Method 2: Context manager (recommended)\nasync with AsyncUrlSeeder() as seeder:\n    config = SeedingConfig(source=\"sitemap\")\n    urls = await seeder.urls(\"example.com\", config)\n    # Automatically cleaned up on exit\nCopy\n```\n\nThe seeder can discover URLs from two powerful sources:\n#### 1. Sitemaps (Fastest)\n```\n# Discover from sitemap\nconfig = SeedingConfig(source=\"sitemap\")\nurls = await seeder.urls(\"example.com\", config)\nCopy\n```\n\nSitemaps are XML files that websites create specifically to list all their URLs. It's like getting a menu at a restaurant - everything is listed upfront.\n**Sitemap Index Support** : For large websites like TechCrunch that use sitemap indexes (a sitemap of sitemaps), the seeder automatically detects and processes all sub-sitemaps in parallel:\n```\n<!-- Example sitemap index -->\n<sitemapindex>\n  <sitemap>\n    <loc>https://techcrunch.com/sitemap-1.xml</loc>\n  </sitemap>\n  <sitemap>\n    <loc>https://techcrunch.com/sitemap-2.xml</loc>\n  </sitemap>\n  <!-- ... more sitemaps ... -->\n</sitemapindex>\nCopy\n```\n\nThe seeder handles this transparently - you'll get all URLs from all sub-sitemaps automatically!\n#### 2. Common Crawl (Most Comprehensive)\n```\n# Discover from Common Crawl\nconfig = SeedingConfig(source=\"cc\")\nurls = await seeder.urls(\"example.com\", config)\nCopy\n```\n\nCommon Crawl is a massive public dataset that regularly crawls the entire web. It's like having access to a pre-built index of the internet.\n#### 3. Both Sources (Maximum Coverage)\n```\n# Use both sources\nconfig = SeedingConfig(source=\"sitemap+cc\")\nurls = await seeder.urls(\"example.com\", config)\nCopy\n```\n\n### Configuration Magic: SeedingConfig\nThe `SeedingConfig` object is your control panel. Here's everything you can configure:\nParameter | Type | Default | Description  \n---|---|---|---  \n`source` | str | \"sitemap+cc\" | URL source: \"cc\" (Common Crawl), \"sitemap\", or \"sitemap+cc\"  \n`pattern` | str | \"*\" | URL pattern filter (e.g., \"_/blog/_ \", \"*.html\")  \n`extract_head` | bool | False | Extract metadata from page `<head>`  \n`live_check` | bool | False | Verify URLs are accessible  \n`max_urls` | int | -1 | Maximum URLs to return (-1 = unlimited)  \n`concurrency` | int | 10 | Parallel workers for fetching  \n`hits_per_sec` | int | 5 | Rate limit for requests  \n`force` | bool | False | Bypass cache, fetch fresh data  \n`verbose` | bool | False | Show detailed progress  \n`query` | str | None | Search query for BM25 scoring  \n`scoring_method` | str | None | Scoring method (currently \"bm25\")  \n`score_threshold` | float | None | Minimum score to include URL  \n`filter_nonsense_urls` | bool | True | Filter out utility URLs (robots.txt, etc.)  \n#### Pattern Matching Examples\n```\n# Match all blog posts\nconfig = SeedingConfig(pattern=\"*/blog/*\")\n\n# Match only HTML files\nconfig = SeedingConfig(pattern=\"*.html\")\n\n# Match product pages\nconfig = SeedingConfig(pattern=\"*/product/*\")\n\n# Match everything except admin pages\nconfig = SeedingConfig(pattern=\"*\")\n# Then filter: urls = [u for u in urls if \"/admin/\" not in u[\"url\"]]\nCopy\n```\n\n### URL Validation: Live Checking\nSometimes you need to know if URLs are actually accessible. That's where live checking comes in:\n```\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    live_check=True,  # Verify each URL is accessible\n    concurrency=20    # Check 20 URLs in parallel\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"example.com\", config)\n\n# Now you can filter by status\nlive_urls = [u for u in urls if u[\"status\"] == \"valid\"]\ndead_urls = [u for u in urls if u[\"status\"] == \"not_valid\"]\n\nprint(f\"Live URLs: {len(live_urls)}\")\nprint(f\"Dead URLs: {len(dead_urls)}\")\nCopy\n```\n\n**When to use live checking:** - Before a large crawling operation - When working with older sitemaps - When data freshness is critical\n**When to skip it:** - Quick explorations - When you trust the source - When speed is more important than accuracy\n### The Power of Metadata: Head Extraction\nThis is where URL seeding gets really powerful. Instead of crawling entire pages, you can extract just the metadata:\n```\nconfig = SeedingConfig(\n    extract_head=True  # Extract metadata from <head> section\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"example.com\", config)\n\n# Now each URL has rich metadata\nfor url in urls[:3]:\n    print(f\"\\nURL: {url['url']}\")\n    print(f\"Title: {url['head_data'].get('title')}\")\n\n    meta = url['head_data'].get('meta', {})\n    print(f\"Description: {meta.get('description')}\")\n    print(f\"Keywords: {meta.get('keywords')}\")\n\n    # Even Open Graph data!\n    print(f\"OG Image: {meta.get('og:image')}\")\nCopy\n```\n\n#### What Can We Extract?\nThe head extraction gives you a treasure trove of information:\n```\n# Example of extracted head_data\n{\n    \"title\": \"10 Python Tips for Beginners\",\n    \"charset\": \"utf-8\",\n    \"lang\": \"en\",\n    \"meta\": {\n        \"description\": \"Learn essential Python tips...\",\n        \"keywords\": \"python, programming, tutorial\",\n        \"author\": \"Jane Developer\",\n        \"viewport\": \"width=device-width, initial-scale=1\",\n\n        # Open Graph tags\n        \"og:title\": \"10 Python Tips for Beginners\",\n        \"og:description\": \"Essential Python tips for new programmers\",\n        \"og:image\": \"https://example.com/python-tips.jpg\",\n        \"og:type\": \"article\",\n\n        # Twitter Card tags\n        \"twitter:card\": \"summary_large_image\",\n        \"twitter:title\": \"10 Python Tips\",\n\n        # Dublin Core metadata\n        \"dc.creator\": \"Jane Developer\",\n        \"dc.date\": \"2024-01-15\"\n    },\n    \"link\": {\n        \"canonical\": [{\"href\": \"https://example.com/blog/python-tips\"}],\n        \"alternate\": [{\"href\": \"/feed.xml\", \"type\": \"application/rss+xml\"}]\n    },\n    \"jsonld\": [\n        {\n            \"@type\": \"Article\",\n            \"headline\": \"10 Python Tips for Beginners\",\n            \"datePublished\": \"2024-01-15\",\n            \"author\": {\"@type\": \"Person\", \"name\": \"Jane Developer\"}\n        }\n    ]\n}\nCopy\n```\n\nThis metadata is gold for filtering! You can find exactly what you need without crawling a single page.\n### Smart URL-Based Filtering (No Head Extraction)\nWhen `extract_head=False` but you still provide a query, the seeder uses intelligent URL-based scoring:\n```\n# Fast filtering based on URL structure alone\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=False,  # Don't fetch page metadata\n    query=\"python tutorial async\",\n    scoring_method=\"bm25\",\n    score_threshold=0.3\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"example.com\", config)\n\n# URLs are scored based on:\n# 1. Domain parts matching (e.g., 'python' in python.example.com)\n# 2. Path segments (e.g., '/tutorials/python-async/')\n# 3. Query parameters (e.g., '?topic=python')\n# 4. Fuzzy matching using character n-grams\n\n# Example URL scoring:\n# https://example.com/tutorials/python/async-guide.html - High score\n# https://example.com/blog/javascript-tips.html - Low score\nCopy\n```\n\nThis approach is much faster than head extraction while still providing intelligent filtering!\n### Understanding Results\nEach URL in the results has this structure:\n```\n{\n    \"url\": \"https://example.com/blog/python-tips.html\",\n    \"status\": \"valid\",        # \"valid\", \"not_valid\", or \"unknown\"\n    \"head_data\": {            # Only if extract_head=True\n        \"title\": \"Page Title\",\n        \"meta\": {...},\n        \"link\": {...},\n        \"jsonld\": [...]\n    },\n    \"relevance_score\": 0.85   # Only if using BM25 scoring\n}\nCopy\n```\n\nLet's see a real example:\n```\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    live_check=True\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"blog.example.com\", config)\n\n# Analyze the results\nfor url in urls[:5]:\n    print(f\"\\n{'='*60}\")\n    print(f\"URL: {url['url']}\")\n    print(f\"Status: {url['status']}\")\n\n    if url['head_data']:\n        data = url['head_data']\n        print(f\"Title: {data.get('title', 'No title')}\")\n\n        # Check content type\n        meta = data.get('meta', {})\n        content_type = meta.get('og:type', 'unknown')\n        print(f\"Content Type: {content_type}\")\n\n        # Publication date\n        pub_date = None\n        for jsonld in data.get('jsonld', []):\n            if isinstance(jsonld, dict):\n                pub_date = jsonld.get('datePublished')\n                if pub_date:\n                    break\n\n        if pub_date:\n            print(f\"Published: {pub_date}\")\n\n        # Word count (if available)\n        word_count = meta.get('word_count')\n        if word_count:\n            print(f\"Word Count: {word_count}\")\nCopy\n```\n\n## Smart Filtering with BM25 Scoring\nNow for the really cool part - intelligent filtering based on relevance!\n### Introduction to Relevance Scoring\nBM25 is a ranking algorithm that scores how relevant a document is to a search query. With URL seeding, we can score URLs based on their metadata _before_ crawling them.\nThink of it like this: - Traditional way: Read every book in the library to find ones about Python - Smart way: Check the titles and descriptions, score them, read only the most relevant\n### Query-Based Discovery\nHere's how to use BM25 scoring:\n```\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,           # Required for scoring\n    query=\"python async tutorial\",  # What we're looking for\n    scoring_method=\"bm25\",       # Use BM25 algorithm\n    score_threshold=0.3          # Minimum relevance score\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"realpython.com\", config)\n\n# Results are automatically sorted by relevance!\nfor url in urls[:5]:\n    print(f\"Score: {url['relevance_score']:.2f} - {url['url']}\")\n    print(f\"  Title: {url['head_data']['title']}\")\nCopy\n```\n\n### Real Examples\n#### Finding Documentation Pages\n```\n# Find API documentation\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    query=\"API reference documentation endpoints\",\n    scoring_method=\"bm25\",\n    score_threshold=0.5,\n    max_urls=20\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"docs.example.com\", config)\n\n# The highest scoring URLs will be API docs!\nCopy\n```\n\n#### Discovering Product Pages\n```\n# Find specific products\nconfig = SeedingConfig(\n    source=\"sitemap+cc\",  # Use both sources\n    extract_head=True,\n    query=\"wireless headphones noise canceling\",\n    scoring_method=\"bm25\",\n    score_threshold=0.4,\n    pattern=\"*/product/*\"  # Combine with pattern matching\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"shop.example.com\", config)\n\n# Filter further by price (from metadata)\naffordable = [\n    u for u in urls \n    if float(u['head_data'].get('meta', {}).get('product:price', '0')) < 200\n]\nCopy\n```\n\n#### Filtering News Articles\n```\n# Find recent news about AI\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    query=\"artificial intelligence machine learning breakthrough\",\n    scoring_method=\"bm25\",\n    score_threshold=0.35\n)\nasync with AsyncUrlSeeder() as seeder:\n    urls = await seeder.urls(\"technews.com\", config)\n\n# Filter by date\nfrom datetime import datetime, timedelta\n\nrecent = []\ncutoff = datetime.now() - timedelta(days=7)\n\nfor url in urls:\n    # Check JSON-LD for publication date\n    for jsonld in url['head_data'].get('jsonld', []):\n        if 'datePublished' in jsonld:\n            pub_date = datetime.fromisoformat(jsonld['datePublished'].replace('Z', '+00:00'))\n            if pub_date > cutoff:\n                recent.append(url)\n                break\nCopy\n```\n\n#### Complex Query Patterns\n```\n# Multi-concept queries\nqueries = [\n    \"python async await concurrency tutorial\",\n    \"data science pandas numpy visualization\",\n    \"web scraping beautifulsoup selenium automation\",\n    \"machine learning tensorflow keras deep learning\"\n]\n\nall_tutorials = []\n\nfor query in queries:\n    config = SeedingConfig(\n        source=\"sitemap\",\n        extract_head=True,\n        query=query,\n        scoring_method=\"bm25\",\n        score_threshold=0.4,\n        max_urls=10  # Top 10 per topic\n    )\n    async with AsyncUrlSeeder() as seeder:\n        urls = await seeder.urls(\"learning-platform.com\", config)\n    all_tutorials.extend(urls)\n\n# Remove duplicates while preserving order\nseen = set()\nunique_tutorials = []\nfor url in all_tutorials:\n    if url['url'] not in seen:\n        seen.add(url['url'])\n        unique_tutorials.append(url)\n\nprint(f\"Found {len(unique_tutorials)} unique tutorials across all topics\")\nCopy\n```\n\n## Scaling Up: Multiple Domains\nWhen you need to discover URLs across multiple websites, URL seeding really shines.\n### The `many_urls` Method\n```\n# Discover URLs from multiple domains in parallel\ndomains = [\"site1.com\", \"site2.com\", \"site3.com\"]\n\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    query=\"python tutorial\",\n    scoring_method=\"bm25\",\n    score_threshold=0.3\n)\n\n# Returns a dictionary: {domain: [urls]}\nasync with AsyncUrlSeeder() as seeder:\n    results = await seeder.many_urls(domains, config)\n\n# Process results\nfor domain, urls in results.items():\n    print(f\"\\n{domain}: Found {len(urls)} relevant URLs\")\n    if urls:\n        top = urls[0]  # Highest scoring\n        print(f\"  Top result: {top['url']}\")\n        print(f\"  Score: {top['relevance_score']:.2f}\")\nCopy\n```\n\n### Cross-Domain Examples\n#### Competitor Analysis\n```\n# Analyze content strategies across competitors\ncompetitors = [\n    \"competitor1.com\",\n    \"competitor2.com\", \n    \"competitor3.com\"\n]\n\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    pattern=\"*/blog/*\",\n    max_urls=100\n)\nasync with AsyncUrlSeeder() as seeder:\n    results = await seeder.many_urls(competitors, config)\n\n# Analyze content types\nfor domain, urls in results.items():\n    content_types = {}\n\n    for url in urls:\n        # Extract content type from metadata\n        og_type = url['head_data'].get('meta', {}).get('og:type', 'unknown')\n        content_types[og_type] = content_types.get(og_type, 0) + 1\n\n    print(f\"\\n{domain} content distribution:\")\n    for ctype, count in sorted(content_types.items(), key=lambda x: x[1], reverse=True):\n        print(f\"  {ctype}: {count}\")\nCopy\n```\n\n#### Industry Research\n```\n# Research Python tutorials across educational sites\neducational_sites = [\n    \"realpython.com\",\n    \"pythontutorial.net\",\n    \"learnpython.org\",\n    \"python.org\"\n]\n\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    extract_head=True,\n    query=\"beginner python tutorial basics\",\n    scoring_method=\"bm25\",\n    score_threshold=0.3,\n    max_urls=20  # Per site\n)\nasync with AsyncUrlSeeder() as seeder:\n    results = await seeder.many_urls(educational_sites, config)\n\n# Find the best beginner tutorials\nall_tutorials = []\nfor domain, urls in results.items():\n    for url in urls:\n        url['domain'] = domain  # Add domain info\n        all_tutorials.append(url)\n\n# Sort by relevance across all domains\nall_tutorials.sort(key=lambda x: x['relevance_score'], reverse=True)\n\nprint(\"Top 10 Python tutorials for beginners across all sites:\")\nfor i, tutorial in enumerate(all_tutorials[:10], 1):\n    print(f\"{i}. [{tutorial['relevance_score']:.2f}] {tutorial['head_data']['title']}\")\n    print(f\"   {tutorial['url']}\")\n    print(f\"   From: {tutorial['domain']}\")\nCopy\n```\n\n#### Multi-Site Monitoring\n```\n# Monitor news about your company across multiple sources\nnews_sites = [\n    \"techcrunch.com\",\n    \"theverge.com\",\n    \"wired.com\",\n    \"arstechnica.com\"\n]\n\ncompany_name = \"YourCompany\"\n\nconfig = SeedingConfig(\n    source=\"cc\",  # Common Crawl for recent content\n    extract_head=True,\n    query=f\"{company_name} announcement news\",\n    scoring_method=\"bm25\",\n    score_threshold=0.5,  # High threshold for relevance\n    max_urls=10\n)\nasync with AsyncUrlSeeder() as seeder:\n    results = await seeder.many_urls(news_sites, config)\n\n# Collect all mentions\nmentions = []\nfor domain, urls in results.items():\n    mentions.extend(urls)\n\nif mentions:\n    print(f\"Found {len(mentions)} mentions of {company_name}:\")\n    for mention in mentions:\n        print(f\"\\n- {mention['head_data']['title']}\")\n        print(f\"  {mention['url']}\")\n        print(f\"  Score: {mention['relevance_score']:.2f}\")\nelse:\n    print(f\"No recent mentions of {company_name} found\")\nCopy\n```\n\n## Advanced Integration Patterns\nLet's put everything together in a real-world example.\n### Building a Research Assistant\nHere's a complete example that discovers, scores, filters, and crawls intelligently:\n```\nimport asyncio\nfrom datetime import datetime\nfrom crawl4ai import AsyncUrlSeeder, AsyncWebCrawler, SeedingConfig, CrawlerRunConfig\n\nclass ResearchAssistant:\n    def __init__(self):\n        self.seeder = None\n\n    async def __aenter__(self):\n        self.seeder = AsyncUrlSeeder()\n        await self.seeder.__aenter__()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.seeder:\n            await self.seeder.__aexit__(exc_type, exc_val, exc_tb)\n\n    async def research_topic(self, topic, domains, max_articles=20):\n        \"\"\"Research a topic across multiple domains.\"\"\"\n\n        print(f\"ðŸ”¬ Researching '{topic}' across {len(domains)} domains...\")\n\n        # Step 1: Discover relevant URLs\n        config = SeedingConfig(\n            source=\"sitemap+cc\",     # Maximum coverage\n            extract_head=True,       # Get metadata\n            query=topic,             # Research topic\n            scoring_method=\"bm25\",   # Smart scoring\n            score_threshold=0.4,     # Quality threshold\n            max_urls=10,             # Per domain\n            concurrency=20,          # Fast discovery\n            verbose=True\n        )\n\n        # Discover across all domains\n        discoveries = await self.seeder.many_urls(domains, config)\n\n        # Step 2: Collect and rank all articles\n        all_articles = []\n        for domain, urls in discoveries.items():\n            for url in urls:\n                url['domain'] = domain\n                all_articles.append(url)\n\n        # Sort by relevance\n        all_articles.sort(key=lambda x: x['relevance_score'], reverse=True)\n\n        # Take top articles\n        top_articles = all_articles[:max_articles]\n\n        print(f\"\\nðŸ“Š Found {len(all_articles)} relevant articles\")\n        print(f\"ðŸ“Œ Selected top {len(top_articles)} for deep analysis\")\n\n        # Step 3: Show what we're about to crawl\n        print(\"\\nðŸŽ¯ Articles to analyze:\")\n        for i, article in enumerate(top_articles[:5], 1):\n            print(f\"\\n{i}. {article['head_data']['title']}\")\n            print(f\"   Score: {article['relevance_score']:.2f}\")\n            print(f\"   Source: {article['domain']}\")\n            print(f\"   URL: {article['url'][:60]}...\")\n\n        # Step 4: Crawl the selected articles\n        print(f\"\\nðŸš€ Deep crawling {len(top_articles)} articles...\")\n\n        async with AsyncWebCrawler() as crawler:\n            config = CrawlerRunConfig(\n                only_text=True,\n                word_count_threshold=200,  # Substantial content only\n                stream=True\n            )\n\n            # Extract URLs and crawl all articles\n            article_urls = [article['url'] for article in top_articles]\n            results = []\n            crawl_results = await crawler.arun_many(article_urls, config=config)\n            async for result in crawl_results:\n                if result.success:\n                    results.append({\n                        'url': result.url,\n                        'title': result.metadata.get('title', 'No title'),\n                        'content': result.markdown.raw_markdown,\n                        'domain': next(a['domain'] for a in top_articles if a['url'] == result.url),\n                        'score': next(a['relevance_score'] for a in top_articles if a['url'] == result.url)\n                    })\n                    print(f\"âœ“ Crawled: {result.url[:60]}...\")\n\n        # Step 5: Analyze and summarize\n        print(f\"\\nðŸ“ Analysis complete! Crawled {len(results)} articles\")\n\n        return self.create_research_summary(topic, results)\n\n    def create_research_summary(self, topic, articles):\n        \"\"\"Create a research summary from crawled articles.\"\"\"\n\n        summary = {\n            'topic': topic,\n            'timestamp': datetime.now().isoformat(),\n            'total_articles': len(articles),\n            'sources': {}\n        }\n\n        # Group by domain\n        for article in articles:\n            domain = article['domain']\n            if domain not in summary['sources']:\n                summary['sources'][domain] = []\n\n            summary['sources'][domain].append({\n                'title': article['title'],\n                'url': article['url'],\n                'score': article['score'],\n                'excerpt': article['content'][:500] + '...' if len(article['content']) > 500 else article['content']\n            })\n\n        return summary\n\n# Use the research assistant\nasync def main():\n    async with ResearchAssistant() as assistant:\n        # Research Python async programming across multiple sources\n        topic = \"python asyncio best practices performance optimization\"\n        domains = [\n            \"realpython.com\",\n            \"python.org\",\n            \"stackoverflow.com\",\n            \"medium.com\"\n        ]\n\n        summary = await assistant.research_topic(topic, domains, max_articles=15)\n\n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    print(\"RESEARCH SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Topic: {summary['topic']}\")\n    print(f\"Date: {summary['timestamp']}\")\n    print(f\"Total Articles Analyzed: {summary['total_articles']}\")\n\n    print(\"\\nKey Findings by Source:\")\n    for domain, articles in summary['sources'].items():\n        print(f\"\\nðŸ“š {domain} ({len(articles)} articles)\")\n        for article in articles[:2]:  # Top 2 per domain\n            print(f\"\\n  Title: {article['title']}\")\n            print(f\"  Relevance: {article['score']:.2f}\")\n            print(f\"  Preview: {article['excerpt'][:200]}...\")\n\nasyncio.run(main())\nCopy\n```\n\n### Performance Optimization Tips\n  1. **Use caching wisely**\n```\n# First run - populate cache\nconfig = SeedingConfig(source=\"sitemap\", extract_head=True, force=True)\nurls = await seeder.urls(\"example.com\", config)\n\n# Subsequent runs - use cache (much faster)\nconfig = SeedingConfig(source=\"sitemap\", extract_head=True, force=False)\nurls = await seeder.urls(\"example.com\", config)\nCopy\n```\n\n  2. **Optimize concurrency**\n```\n# For many small requests (like HEAD checks)\nconfig = SeedingConfig(concurrency=50, hits_per_sec=20)\n\n# For fewer large requests (like full head extraction)\nconfig = SeedingConfig(concurrency=10, hits_per_sec=5)\nCopy\n```\n\n  3. **Stream large result sets**\n```\n# When crawling many URLs\nasync with AsyncWebCrawler() as crawler:\n    # Assuming urls is a list of URL strings\n    crawl_results = await crawler.arun_many(urls, config=config)\n\n    # Process as they arrive\n    async for result in crawl_results:\n        process_immediately(result)  # Don't wait for all\nCopy\n```\n\n  4. **Memory protection for large domains**\n\n\nThe seeder uses bounded queues to prevent memory issues when processing domains with millions of URLs:\n```\n# Safe for domains with 1M+ URLs\nconfig = SeedingConfig(\n    source=\"cc+sitemap\",\n    concurrency=50,  # Queue size adapts to concurrency\n    max_urls=100000  # Process in batches if needed\n)\n\n# The seeder automatically manages memory by:\n# - Using bounded queues (prevents RAM spikes)\n# - Applying backpressure when queue is full\n# - Processing URLs as they're discovered\nCopy\n```\n\n## Best Practices & Tips\n### Cache Management\nThe seeder automatically caches results to speed up repeated operations:\n  * **Common Crawl cache** : `~/.crawl4ai/seeder_cache/[index]_[domain]_[hash].jsonl`\n  * **Sitemap cache** : `~/.crawl4ai/seeder_cache/sitemap_[domain]_[hash].jsonl`\n  * **HEAD data cache** : `~/.cache/url_seeder/head/[hash].json`\n\n\nCache expires after 7 days by default. Use `force=True` to refresh.\n### Pattern Matching Strategies\n```\n# Be specific when possible\ngood_pattern = \"*/blog/2024/*.html\"  # Specific\nbad_pattern = \"*\"                     # Too broad\n\n# Combine patterns with metadata filtering\nconfig = SeedingConfig(\n    pattern=\"*/articles/*\",\n    extract_head=True\n)\nurls = await seeder.urls(\"news.com\", config)\n\n# Further filter by publish date, author, category, etc.\nrecent = [u for u in urls if is_recent(u['head_data'])]\nCopy\n```\n\n### Rate Limiting Considerations\n```\n# Be respectful of servers\nconfig = SeedingConfig(\n    hits_per_sec=10,      # Max 10 requests per second\n    concurrency=20        # But use 20 workers\n)\n\n# For your own servers\nconfig = SeedingConfig(\n    hits_per_sec=None,    # No limit\n    concurrency=100       # Go fast\n)\nCopy\n```\n\n## Quick Reference\n### Common Patterns\n```\n# Blog post discovery\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    pattern=\"*/blog/*\",\n    extract_head=True,\n    query=\"your topic\",\n    scoring_method=\"bm25\"\n)\n\n# E-commerce product discovery\nconfig = SeedingConfig(\n    source=\"sitemap+cc\",\n    pattern=\"*/product/*\",\n    extract_head=True,\n    live_check=True\n)\n\n# Documentation search\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    pattern=\"*/docs/*\",\n    extract_head=True,\n    query=\"API reference\",\n    scoring_method=\"bm25\",\n    score_threshold=0.5\n)\n\n# News monitoring\nconfig = SeedingConfig(\n    source=\"cc\",\n    extract_head=True,\n    query=\"company name\",\n    scoring_method=\"bm25\",\n    max_urls=50\n)\nCopy\n```\n\n### Troubleshooting Guide\nIssue | Solution  \n---|---  \nNo URLs found | Try `source=\"cc+sitemap\"`, check domain spelling  \nSlow discovery | Reduce `concurrency`, add `hits_per_sec` limit  \nMissing metadata | Ensure `extract_head=True`  \nLow relevance scores | Refine query, lower `score_threshold`  \nRate limit errors | Reduce `hits_per_sec` and `concurrency`  \nMemory issues with large sites | Use `max_urls` to limit results, reduce `concurrency`  \nConnection not closed | Use context manager or call `await seeder.close()`  \n### Performance Benchmarks\nTypical performance on a standard connection:\n  * **Sitemap discovery** : 100-1,000 URLs/second\n  * **Common Crawl discovery** : 50-500 URLs/second \n  * **HEAD checking** : 10-50 URLs/second\n  * **Head extraction** : 5-20 URLs/second\n  * **BM25 scoring** : 10,000+ URLs/second\n\n\n## Conclusion\nURL seeding transforms web crawling from a blind expedition into a surgical strike. By discovering and analyzing URLs before crawling, you can:\n  * Save hours of crawling time\n  * Reduce bandwidth usage by 90%+\n  * Find exactly what you need\n  * Scale across multiple domains effortlessly\n\n\nWhether you're building a research tool, monitoring competitors, or creating a content aggregator, URL seeding gives you the intelligence to crawl smarter, not harder.\n### Smart URL Filtering\nThe seeder automatically filters out nonsense URLs that aren't useful for content crawling:\n```\n# Enabled by default\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    filter_nonsense_urls=True  # Default: True\n)\n\n# URLs that get filtered:\n# - robots.txt, sitemap.xml, ads.txt\n# - API endpoints (/api/, /v1/, .json)\n# - Media files (.jpg, .mp4, .pdf)\n# - Archives (.zip, .tar.gz)\n# - Source code (.js, .css)\n# - Admin/login pages\n# - And many more...\nCopy\n```\n\nTo disable filtering (not recommended):\n```\nconfig = SeedingConfig(\n    source=\"sitemap\",\n    filter_nonsense_urls=False  # Include ALL URLs\n)\nCopy\n```\n\n### Key Features Summary\n  1. **Parallel Sitemap Index Processing** : Automatically detects and processes sitemap indexes in parallel\n  2. **Memory Protection** : Bounded queues prevent RAM issues with large domains (1M+ URLs)\n  3. **Context Manager Support** : Automatic cleanup with `async with` statement\n  4. **URL-Based Scoring** : Smart filtering even without head extraction\n  5. **Smart URL Filtering** : Automatically excludes utility/nonsense URLs\n  6. **Dual Caching** : Separate caches for URL lists and metadata\n\n\nNow go forth and seed intelligently! ðŸŒ±ðŸš€\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/core/url-seeding/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/core/url-seeding/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/core/url-seeding/)\n\n\nESC to close\n#### On this page\n  * [Why URL Seeding?](https://docs.crawl4ai.com/core/url-seeding/#why-url-seeding)\n  * [Deep Crawling: Real-Time Discovery](https://docs.crawl4ai.com/core/url-seeding/#deep-crawling-real-time-discovery)\n  * [URL Seeding: Bulk Discovery](https://docs.crawl4ai.com/core/url-seeding/#url-seeding-bulk-discovery)\n  * [The Trade-offs](https://docs.crawl4ai.com/core/url-seeding/#the-trade-offs)\n  * [When to Use Each](https://docs.crawl4ai.com/core/url-seeding/#when-to-use-each)\n  * [Your First URL Seeding Adventure](https://docs.crawl4ai.com/core/url-seeding/#your-first-url-seeding-adventure)\n  * [Understanding the URL Seeder](https://docs.crawl4ai.com/core/url-seeding/#understanding-the-url-seeder)\n  * [Basic Usage](https://docs.crawl4ai.com/core/url-seeding/#basic-usage)\n  * [1. Sitemaps (Fastest)](https://docs.crawl4ai.com/core/url-seeding/#1-sitemaps-fastest)\n  * [2. Common Crawl (Most Comprehensive)](https://docs.crawl4ai.com/core/url-seeding/#2-common-crawl-most-comprehensive)\n  * [3. Both Sources (Maximum Coverage)](https://docs.crawl4ai.com/core/url-seeding/#3-both-sources-maximum-coverage)\n  * [Configuration Magic: SeedingConfig](https://docs.crawl4ai.com/core/url-seeding/#configuration-magic-seedingconfig)\n  * [Pattern Matching Examples](https://docs.crawl4ai.com/core/url-seeding/#pattern-matching-examples)\n  * [URL Validation: Live Checking](https://docs.crawl4ai.com/core/url-seeding/#url-validation-live-checking)\n  * [The Power of Metadata: Head Extraction](https://docs.crawl4ai.com/core/url-seeding/#the-power-of-metadata-head-extraction)\n  * [What Can We Extract?](https://docs.crawl4ai.com/core/url-seeding/#what-can-we-extract)\n  * [Smart URL-Based Filtering (No Head Extraction)](https://docs.crawl4ai.com/core/url-seeding/#smart-url-based-filtering-no-head-extraction)\n  * [Understanding Results](https://docs.crawl4ai.com/core/url-seeding/#understanding-results)\n  * [Smart Filtering with BM25 Scoring](https://docs.crawl4ai.com/core/url-seeding/#smart-filtering-with-bm25-scoring)\n  * [Introduction to Relevance Scoring](https://docs.crawl4ai.com/core/url-seeding/#introduction-to-relevance-scoring)\n  * [Query-Based Discovery](https://docs.crawl4ai.com/core/url-seeding/#query-based-discovery)\n  * [Real Examples](https://docs.crawl4ai.com/core/url-seeding/#real-examples)\n  * [Finding Documentation Pages](https://docs.crawl4ai.com/core/url-seeding/#finding-documentation-pages)\n  * [Discovering Product Pages](https://docs.crawl4ai.com/core/url-seeding/#discovering-product-pages)\n  * [Filtering News Articles](https://docs.crawl4ai.com/core/url-seeding/#filtering-news-articles)\n  * [Complex Query Patterns](https://docs.crawl4ai.com/core/url-seeding/#complex-query-patterns)\n  * [Scaling Up: Multiple Domains](https://docs.crawl4ai.com/core/url-seeding/#scaling-up-multiple-domains)\n  * [The many_urls Method](https://docs.crawl4ai.com/core/url-seeding/#the-many_urls-method)\n  * [Cross-Domain Examples](https://docs.crawl4ai.com/core/url-seeding/#cross-domain-examples)\n  * [Competitor Analysis](https://docs.crawl4ai.com/core/url-seeding/#competitor-analysis)\n  * [Industry Research](https://docs.crawl4ai.com/core/url-seeding/#industry-research)\n  * [Multi-Site Monitoring](https://docs.crawl4ai.com/core/url-seeding/#multi-site-monitoring)\n  * [Advanced Integration Patterns](https://docs.crawl4ai.com/core/url-seeding/#advanced-integration-patterns)\n  * [Building a Research Assistant](https://docs.crawl4ai.com/core/url-seeding/#building-a-research-assistant)\n  * [Performance Optimization Tips](https://docs.crawl4ai.com/core/url-seeding/#performance-optimization-tips)\n  * [Best Practices & Tips](https://docs.crawl4ai.com/core/url-seeding/#best-practices-tips)\n  * [Cache Management](https://docs.crawl4ai.com/core/url-seeding/#cache-management)\n  * [Pattern Matching Strategies](https://docs.crawl4ai.com/core/url-seeding/#pattern-matching-strategies)\n  * [Rate Limiting Considerations](https://docs.crawl4ai.com/core/url-seeding/#rate-limiting-considerations)\n  * [Quick Reference](https://docs.crawl4ai.com/core/url-seeding/#quick-reference)\n  * [Common Patterns](https://docs.crawl4ai.com/core/url-seeding/#common-patterns)\n  * [Troubleshooting Guide](https://docs.crawl4ai.com/core/url-seeding/#troubleshooting-guide)\n  * [Performance Benchmarks](https://docs.crawl4ai.com/core/url-seeding/#performance-benchmarks)\n  * [Conclusion](https://docs.crawl4ai.com/core/url-seeding/#conclusion)\n  * [Smart URL Filtering](https://docs.crawl4ai.com/core/url-seeding/#smart-url-filtering)\n  * [Key Features Summary](https://docs.crawl4ai.com/core/url-seeding/#key-features-summary)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/extraction/chunking",
    "depth": 1,
    "title": "Chunking - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "ffcbfd8d521d3df51593b167599aa7a4",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/extraction/chunking/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * Chunking\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Chunking Strategies](https://docs.crawl4ai.com/extraction/chunking/#chunking-strategies)\n  * [Why Use Chunking?](https://docs.crawl4ai.com/extraction/chunking/#why-use-chunking)\n  * [Methods of Chunking](https://docs.crawl4ai.com/extraction/chunking/#methods-of-chunking)\n  * [Combining Chunking with Cosine Similarity](https://docs.crawl4ai.com/extraction/chunking/#combining-chunking-with-cosine-similarity)\n\n\n# Chunking Strategies\nChunking strategies are critical for dividing large texts into manageable parts, enabling effective content processing and extraction. These strategies are foundational in cosine similarity-based extraction techniques, which allow users to retrieve only the most relevant chunks of content for a given query. Additionally, they facilitate direct integration into RAG (Retrieval-Augmented Generation) systems for structured and scalable workflows.\n### Why Use Chunking?\n1. **Cosine Similarity and Query Relevance** : Prepares chunks for semantic similarity analysis. 2. **RAG System Integration** : Seamlessly processes and stores chunks for retrieval. 3. **Structured Processing** : Allows for diverse segmentation methods, such as sentence-based, topic-based, or windowed approaches.\n### Methods of Chunking\n#### 1. Regex-Based Chunking\nSplits text based on regular expression patterns, useful for coarse segmentation.\n**Code Example** : \n```\nclass RegexChunking:\n    def __init__(self, patterns=None):\n        self.patterns = patterns or [r'\\n\\n']  # Default pattern for paragraphs\n\n    def chunk(self, text):\n        paragraphs = [text]\n        for pattern in self.patterns:\n            paragraphs = [seg for p in paragraphs for seg in re.split(pattern, p)]\n        return paragraphs\n\n# Example Usage\ntext = \"\"\"This is the first paragraph.\n\nThis is the second paragraph.\"\"\"\nchunker = RegexChunking()\nprint(chunker.chunk(text))\nCopy\n```\n\n#### 2. Sentence-Based Chunking\nDivides text into sentences using NLP tools, ideal for extracting meaningful statements.\n**Code Example** : \n```\nfrom nltk.tokenize import sent_tokenize\n\nclass NlpSentenceChunking:\n    def chunk(self, text):\n        sentences = sent_tokenize(text)\n        return [sentence.strip() for sentence in sentences]\n\n# Example Usage\ntext = \"This is sentence one. This is sentence two.\"\nchunker = NlpSentenceChunking()\nprint(chunker.chunk(text))\nCopy\n```\n\n#### 3. Topic-Based Segmentation\nUses algorithms like TextTiling to create topic-coherent chunks.\n**Code Example** : \n```\nfrom nltk.tokenize import TextTilingTokenizer\n\nclass TopicSegmentationChunking:\n    def __init__(self):\n        self.tokenizer = TextTilingTokenizer()\n\n    def chunk(self, text):\n        return self.tokenizer.tokenize(text)\n\n# Example Usage\ntext = \"\"\"This is an introduction.\nThis is a detailed discussion on the topic.\"\"\"\nchunker = TopicSegmentationChunking()\nprint(chunker.chunk(text))\nCopy\n```\n\n#### 4. Fixed-Length Word Chunking\nSegments text into chunks of a fixed word count.\n**Code Example** : \n```\nclass FixedLengthWordChunking:\n    def __init__(self, chunk_size=100):\n        self.chunk_size = chunk_size\n\n    def chunk(self, text):\n        words = text.split()\n        return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]\n\n# Example Usage\ntext = \"This is a long text with many words to be chunked into fixed sizes.\"\nchunker = FixedLengthWordChunking(chunk_size=5)\nprint(chunker.chunk(text))\nCopy\n```\n\n#### 5. Sliding Window Chunking\nGenerates overlapping chunks for better contextual coherence.\n**Code Example** : \n```\nclass SlidingWindowChunking:\n    def __init__(self, window_size=100, step=50):\n        self.window_size = window_size\n        self.step = step\n\n    def chunk(self, text):\n        words = text.split()\n        chunks = []\n        for i in range(0, len(words) - self.window_size + 1, self.step):\n            chunks.append(' '.join(words[i:i + self.window_size]))\n        return chunks\n\n# Example Usage\ntext = \"This is a long text to demonstrate sliding window chunking.\"\nchunker = SlidingWindowChunking(window_size=5, step=2)\nprint(chunker.chunk(text))\nCopy\n```\n\n### Combining Chunking with Cosine Similarity\nTo enhance the relevance of extracted content, chunking strategies can be paired with cosine similarity techniques. Hereâ€™s an example workflow:\n**Code Example** : \n```\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass CosineSimilarityExtractor:\n    def __init__(self, query):\n        self.query = query\n        self.vectorizer = TfidfVectorizer()\n\n    def find_relevant_chunks(self, chunks):\n        vectors = self.vectorizer.fit_transform([self.query] + chunks)\n        similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n        return [(chunks[i], similarities[i]) for i in range(len(chunks))]\n\n# Example Workflow\ntext = \"\"\"This is a sample document. It has multiple sentences. \nWe are testing chunking and similarity.\"\"\"\n\nchunker = SlidingWindowChunking(window_size=5, step=3)\nchunks = chunker.chunk(text)\nquery = \"testing chunking\"\nextractor = CosineSimilarityExtractor(query)\nrelevant_chunks = extractor.find_relevant_chunks(chunks)\n\nprint(relevant_chunks)\nCopy\n```\n\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/extraction/chunking/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/extraction/chunking/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/extraction/chunking/)\n\n\nESC to close\n#### On this page\n  * [Why Use Chunking?](https://docs.crawl4ai.com/extraction/chunking/#why-use-chunking)\n  * [Methods of Chunking](https://docs.crawl4ai.com/extraction/chunking/#methods-of-chunking)\n  * [1. Regex-Based Chunking](https://docs.crawl4ai.com/extraction/chunking/#1-regex-based-chunking)\n  * [2. Sentence-Based Chunking](https://docs.crawl4ai.com/extraction/chunking/#2-sentence-based-chunking)\n  * [3. Topic-Based Segmentation](https://docs.crawl4ai.com/extraction/chunking/#3-topic-based-segmentation)\n  * [4. Fixed-Length Word Chunking](https://docs.crawl4ai.com/extraction/chunking/#4-fixed-length-word-chunking)\n  * [5. Sliding Window Chunking](https://docs.crawl4ai.com/extraction/chunking/#5-sliding-window-chunking)\n  * [Combining Chunking with Cosine Similarity](https://docs.crawl4ai.com/extraction/chunking/#combining-chunking-with-cosine-similarity)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/extraction/clustring-strategies",
    "depth": 1,
    "title": "Clustering Strategies - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "da863cea2cdae3528aa0fa7db03c0cbd",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * Clustering Strategies\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Cosine Strategy](https://docs.crawl4ai.com/extraction/clustring-strategies/#cosine-strategy)\n  * [How It Works](https://docs.crawl4ai.com/extraction/clustring-strategies/#how-it-works)\n  * [Basic Usage](https://docs.crawl4ai.com/extraction/clustring-strategies/#basic-usage)\n  * [Configuration Options](https://docs.crawl4ai.com/extraction/clustring-strategies/#configuration-options)\n  * [Use Cases](https://docs.crawl4ai.com/extraction/clustring-strategies/#use-cases)\n  * [Advanced Features](https://docs.crawl4ai.com/extraction/clustring-strategies/#advanced-features)\n  * [Best Practices](https://docs.crawl4ai.com/extraction/clustring-strategies/#best-practices)\n  * [Error Handling](https://docs.crawl4ai.com/extraction/clustring-strategies/#error-handling)\n\n\n# Cosine Strategy\nThe Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns.\n## How It Works\nThe Cosine Strategy: 1. Breaks down page content into meaningful chunks 2. Converts text into vector representations 3. Calculates similarity between chunks 4. Clusters similar content together 5. Ranks and filters content based on relevance\n## Basic Usage\n```\nfrom crawl4ai import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n\n    content = result.extracted_content\nCopy\n```\n\n## Configuration Options\n### Core Parameters\n```\nCosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n\n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n\n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n\n    verbose: bool = False             # Enable logging\n)\nCopy\n```\n\n### Parameter Details\n1. **semantic_filter** - Sets the target topic or content type - Use keywords relevant to your desired content - Example: \"technical specifications\", \"user reviews\", \"pricing information\"\n2. **sim_threshold** - Controls how similar content must be to be grouped together - Higher values (e.g., 0.8) mean stricter matching - Lower values (e.g., 0.3) allow more variation \n```\n# Strict matching\nstrategy = CosineStrategy(sim_threshold=0.8)\n\n# Loose matching\nstrategy = CosineStrategy(sim_threshold=0.3)\nCopy\n```\n\n3. **word_count_threshold** - Filters out short content blocks - Helps eliminate noise and irrelevant content \n```\n# Only consider substantial paragraphs\nstrategy = CosineStrategy(word_count_threshold=50)\nCopy\n```\n\n4. **top_k** - Number of top content clusters to return - Higher values return more diverse content \n```\n# Get top 5 most relevant content clusters\nstrategy = CosineStrategy(top_k=5)\nCopy\n```\n\n## Use Cases\n### 1. Article Content Extraction\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)\nCopy\n```\n\n### 2. Product Review Analysis\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)\nCopy\n```\n\n### 3. Technical Documentation\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)\nCopy\n```\n\n## Advanced Features\n### Custom Clustering\n```\nstrategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)\nCopy\n```\n\n### Content Filtering Pipeline\n```\nstrategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n\n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }\nCopy\n```\n\n## Best Practices\n1. **Adjust Thresholds Iteratively** - Start with default values - Adjust based on results - Monitor clustering quality\n2. **Choose Appropriate Word Count Thresholds** - Higher for articles (100+) - Lower for reviews/comments (20+) - Medium for product descriptions (50+)\n3. **Optimize Performance**\n```\nstrategy = CosineStrategy(\n    word_count_threshold=10,  # Filter early\n    top_k=5,                 # Limit results\n    verbose=True             # Monitor performance\n)\nCopy\n```\n\n4. **Handle Different Content Types**\n```\n# For mixed content pages\nstrategy = CosineStrategy(\n    semantic_filter=\"product features\",\n    sim_threshold=0.4,      # More flexible matching\n    max_dist=0.3,          # Larger clusters\n    top_k=3                # Multiple relevant sections\n)\nCopy\n```\n\n## Error Handling\n```\ntry:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n\n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n\nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")\nCopy\n```\n\nThe Cosine Strategy is particularly effective when: - Content structure is inconsistent - You need semantic understanding - You want to find similar content blocks - Structure-based extraction (CSS/XPath) isn't reliable\nIt works well with other strategies and can be used as a pre-processing step for LLM-based extraction.\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n\n\nESC to close\n#### On this page\n  * [How It Works](https://docs.crawl4ai.com/extraction/clustring-strategies/#how-it-works)\n  * [Basic Usage](https://docs.crawl4ai.com/extraction/clustring-strategies/#basic-usage)\n  * [Configuration Options](https://docs.crawl4ai.com/extraction/clustring-strategies/#configuration-options)\n  * [Core Parameters](https://docs.crawl4ai.com/extraction/clustring-strategies/#core-parameters)\n  * [Parameter Details](https://docs.crawl4ai.com/extraction/clustring-strategies/#parameter-details)\n  * [Use Cases](https://docs.crawl4ai.com/extraction/clustring-strategies/#use-cases)\n  * [1. Article Content Extraction](https://docs.crawl4ai.com/extraction/clustring-strategies/#1-article-content-extraction)\n  * [2. Product Review Analysis](https://docs.crawl4ai.com/extraction/clustring-strategies/#2-product-review-analysis)\n  * [3. Technical Documentation](https://docs.crawl4ai.com/extraction/clustring-strategies/#3-technical-documentation)\n  * [Advanced Features](https://docs.crawl4ai.com/extraction/clustring-strategies/#advanced-features)\n  * [Custom Clustering](https://docs.crawl4ai.com/extraction/clustring-strategies/#custom-clustering)\n  * [Content Filtering Pipeline](https://docs.crawl4ai.com/extraction/clustring-strategies/#content-filtering-pipeline)\n  * [Best Practices](https://docs.crawl4ai.com/extraction/clustring-strategies/#best-practices)\n  * [Error Handling](https://docs.crawl4ai.com/extraction/clustring-strategies/#error-handling)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/extraction/llm-strategies",
    "depth": 1,
    "title": "LLM Strategies - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "b6ec7dd25e57b590b961dab6e70b66ce",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/extraction/llm-strategies/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * [LLM-Free Strategies](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n    * LLM Strategies\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Extracting JSON (LLM)](https://docs.crawl4ai.com/extraction/llm-strategies/#extracting-json-llm)\n  * [1. Why Use an LLM?](https://docs.crawl4ai.com/extraction/llm-strategies/#1-why-use-an-llm)\n  * [2. Provider-Agnostic via LiteLLM](https://docs.crawl4ai.com/extraction/llm-strategies/#2-provider-agnostic-via-litellm)\n  * [3. How LLM Extraction Works](https://docs.crawl4ai.com/extraction/llm-strategies/#3-how-llm-extraction-works)\n  * [4. Key Parameters](https://docs.crawl4ai.com/extraction/llm-strategies/#4-key-parameters)\n  * [5. Putting It in CrawlerRunConfig](https://docs.crawl4ai.com/extraction/llm-strategies/#5-putting-it-in-crawlerrunconfig)\n  * [6. Chunking Details](https://docs.crawl4ai.com/extraction/llm-strategies/#6-chunking-details)\n  * [7. Input Format](https://docs.crawl4ai.com/extraction/llm-strategies/#7-input-format)\n  * [8. Token Usage & Show Usage](https://docs.crawl4ai.com/extraction/llm-strategies/#8-token-usage-show-usage)\n  * [9. Example: Building a Knowledge Graph](https://docs.crawl4ai.com/extraction/llm-strategies/#9-example-building-a-knowledge-graph)\n  * [10. Best Practices & Caveats](https://docs.crawl4ai.com/extraction/llm-strategies/#10-best-practices-caveats)\n  * [11. Conclusion](https://docs.crawl4ai.com/extraction/llm-strategies/#11-conclusion)\n\n\n# Extracting JSON (LLM)\nIn some cases, you need to extract **complex or unstructured** information from a webpage that a simple CSS/XPath schema cannot easily parse. Or you want **AI** -driven insights, classification, or summarization. For these scenarios, Crawl4AI provides an **LLM-based extraction strategy** that:\n  1. Works with **any** large language model supported by [LiteLLM](https://github.com/BerriAI/litellm) (Ollama, OpenAI, Claude, and more). \n  2. Automatically splits content into chunks (if desired) to handle token limits, then combines results. \n  3. Lets you define a **schema** (like a Pydantic model) or a simpler â€œblockâ€ extraction approach.\n\n\n**Important** : LLM-based extraction can be slower and costlier than schema-based approaches. If your page data is highly structured, consider using [`JsonCssExtractionStrategy`](https://docs.crawl4ai.com/extraction/no-llm-strategies/) or [`JsonXPathExtractionStrategy`](https://docs.crawl4ai.com/extraction/no-llm-strategies/) first. But if you need AI to interpret or reorganize content, read on!\n* * *\n## 1. Why Use an LLM?\n  * **Complex Reasoning** : If the siteâ€™s data is unstructured, scattered, or full of natural language context. \n  * **Semantic Extraction** : Summaries, knowledge graphs, or relational data that require comprehension. \n  * **Flexible** : You can pass instructions to the model to do more advanced transformations or classification.\n\n\n* * *\n## 2. Provider-Agnostic via LiteLLM\nYou can use LLMConfig, to quickly configure multiple variations of LLMs and experiment with them to find the optimal one for your use case. You can read more about LLMConfig [here](https://docs.crawl4ai.com/api/parameters).\n```\nllm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))\nCopy\n```\n\nCrawl4AI uses a â€œprovider stringâ€ (e.g., `\"openai/gpt-4o\"`, `\"ollama/llama2.0\"`, `\"aws/titan\"`) to identify your LLM. **Any** model that LiteLLM supports is fair game. You just provide:\n  * **`provider`**: The`<provider>/<model_name>` identifier (e.g., `\"openai/gpt-4\"`, `\"ollama/llama2\"`, `\"huggingface/google-flan\"`, etc.). \n  * **`api_token`**: If needed (for OpenAI, HuggingFace, etc.); local models or Ollama might not require it.\n  * **`base_url`**(optional): If your provider has a custom endpoint.\n\n\nThis means you **arenâ€™t locked** into a single LLM vendor. Switch or experiment easily.\n* * *\n## 3. How LLM Extraction Works\n### 3.1 Flow\n1. **Chunking** (optional): The HTML or markdown is split into smaller segments if itâ€™s very long (based on `chunk_token_threshold`, overlap, etc.).  \n2. **Prompt Construction** : For each chunk, the library forms a prompt that includes your **`instruction`**(and possibly schema or examples).  \n3. **LLM Inference** : Each chunk is sent to the model in parallel or sequentially (depending on your concurrency).  \n4. **Combining** : The results from each chunk are merged and parsed into JSON.\n### 3.2 `extraction_type`\n  * **`\"schema\"`**: The model tries to return JSON conforming to your Pydantic-based schema.\n  * **`\"block\"`**: The model returns freeform text, or smaller JSON structures, which the library collects.\n\n\nFor structured data, `\"schema\"` is recommended. You provide `schema=YourPydanticModel.model_json_schema()`.\n* * *\n## 4. Key Parameters\nBelow is an overview of important LLM extraction parameters. All are typically set inside `LLMExtractionStrategy(...)`. You then put that strategy in your `CrawlerRunConfig(..., extraction_strategy=...)`.\n  1. **`llm_config`**(LLMConfig): e.g.,`\"openai/gpt-4\"` , `\"ollama/llama2\"`. 2. **`schema`**(dict): A JSON schema describing the fields you want. Usually generated by`YourModel.model_json_schema()`.  \n3. **`extraction_type`**(str):`\"schema\"` or `\"block\"`.  \n4. **`instruction`**(str): Prompt text telling the LLM what you want extracted. E.g., â€œExtract these fields as a JSON array.â€  \n5. **`chunk_token_threshold`**(int): Maximum tokens per chunk. If your content is huge, you can break it up for the LLM.  \n6. **`overlap_rate`**(float): Overlap ratio between adjacent chunks. E.g.,`0.1` means 10% of each chunk is repeated to preserve context continuity.  \n7. **`apply_chunking`**(bool): Set`True` to chunk automatically. If you want a single pass, set `False`.  \n8. **`input_format`**(str): Determines**which** crawler result is passed to the LLM. Options include: \n  2. `\"markdown\"`: The raw markdown (default). \n  3. `\"fit_markdown\"`: The filtered â€œfitâ€ markdown if you used a content filter. \n  4. `\"html\"`: The cleaned or raw HTML.  \n9. **`extra_args`**(dict): Additional LLM parameters like`temperature` , `max_tokens`, `top_p`, etc.  \n10. **`show_usage()`**: A method you can call to print out usage info (token usage per chunk, total cost if known).\n\n\n**Example** :\n```\nextraction_strategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"openai/gpt-4\", api_token=\"YOUR_OPENAI_KEY\"),\n    schema=MyModel.model_json_schema(),\n    extraction_type=\"schema\",\n    instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\",\n    chunk_token_threshold=1200,\n    overlap_rate=0.1,\n    apply_chunking=True,\n    input_format=\"html\",\n    extra_args={\"temperature\": 0.1, \"max_tokens\": 1000},\n    verbose=True\n)\nCopy\n```\n\n* * *\n## 5. Putting It in `CrawlerRunConfig`\n**Important** : In Crawl4AI, all strategy definitions should go inside the `CrawlerRunConfig`, not directly as a param in `arun()`. Hereâ€™s a full example:\n```\nimport os\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: str\n\nasync def main():\n    # 1. Define the LLM extraction strategy\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=Product.model_json_schema(), # Or use model_json_schema()\n        extraction_type=\"schema\",\n        instruction=\"Extract all product objects with 'name' and 'price' from the content.\",\n        chunk_token_threshold=1000,\n        overlap_rate=0.0,\n        apply_chunking=True,\n        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n        extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n    )\n\n    # 2. Build the crawler config\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strategy,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3. Create a browser config if needed\n    browser_cfg = BrowserConfig(headless=True)\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        # 4. Let's say we want to crawl a single page\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=crawl_config\n        )\n\n        if result.success:\n            # 5. The extracted content is presumably JSON\n            data = json.loads(result.extracted_content)\n            print(\"Extracted items:\", data)\n\n            # 6. Show usage stats\n            llm_strategy.show_usage()  # prints token usage\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n* * *\n## 6. Chunking Details\n### 6.1 `chunk_token_threshold`\nIf your page is large, you might exceed your LLMâ€™s context window. **`chunk_token_threshold`**sets the approximate max tokens per chunk. The library calculates wordâ†’token ratio using`word_token_rate` (often ~0.75 by default). If chunking is enabled (`apply_chunking=True`), the text is split into segments.\n### 6.2 `overlap_rate`\nTo keep context continuous across chunks, we can overlap them. E.g., `overlap_rate=0.1` means each subsequent chunk includes 10% of the previous chunkâ€™s text. This is helpful if your needed info might straddle chunk boundaries.\n### 6.3 Performance & Parallelism\nBy chunking, you can potentially process multiple chunks in parallel (depending on your concurrency settings and the LLM provider). This reduces total time if the site is huge or has many sections.\n* * *\n## 7. Input Format\nBy default, **LLMExtractionStrategy** uses `input_format=\"markdown\"`, meaning the **crawlerâ€™s final markdown** is fed to the LLM. You can change to:\n  * **`html`**: The cleaned HTML or raw HTML (depending on your crawler config) goes into the LLM.\n  * **`fit_markdown`**: If you used, for instance,`PruningContentFilter` , the â€œfitâ€ version of the markdown is used. This can drastically reduce tokens if you trust the filter. \n  * **`markdown`**: Standard markdown output from the crawlerâ€™s`markdown_generator`.\n\n\nThis setting is crucial: if the LLM instructions rely on HTML tags, pick `\"html\"`. If you prefer a text-based approach, pick `\"markdown\"`.\n```\nLLMExtractionStrategy(\n    # ...\n    input_format=\"html\",  # Instead of \"markdown\" or \"fit_markdown\"\n)\nCopy\n```\n\n* * *\n## 8. Token Usage & Show Usage\nTo keep track of tokens and cost, each chunk is processed with an LLM call. We record usage in:\n  * **`usages`**(list): token usage per chunk or call.\n  * **`total_usage`**: sum of all chunk calls.\n  * **`show_usage()`**: prints a usage report (if the provider returns usage data).\n\n\n```\nllm_strategy = LLMExtractionStrategy(...)\n# ...\nllm_strategy.show_usage()\n# e.g. â€œTotal usage: 1241 tokens across 2 chunk callsâ€\nCopy\n```\n\nIf your model provider doesnâ€™t return usage info, these fields might be partial or empty.\n* * *\n## 9. Example: Building a Knowledge Graph\nBelow is a snippet combining **`LLMExtractionStrategy`**with a Pydantic schema for a knowledge graph. Notice how we pass an**`instruction`**telling the model what to parse.\n```\nimport os\nimport json\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\n\nclass Entity(BaseModel):\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    entity1: Entity\n    entity2: Entity\n    description: str\n    relation_type: str\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[Entity]\n    relationships: List[Relationship]\n\nasync def main():\n    # LLM extraction strategy\n    llm_strat = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=KnowledgeGraph.model_json_schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract entities and relationships from the content. Return valid JSON.\",\n        chunk_token_threshold=1400,\n        apply_chunking=True,\n        input_format=\"html\",\n        extra_args={\"temperature\": 0.1, \"max_tokens\": 1500}\n    )\n\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strat,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        # Example page\n        url = \"https://www.nbcnews.com/business\"\n        result = await crawler.arun(url=url, config=crawl_config)\n\n        print(\"--- LLM RAW RESPONSE ---\")\n        print(result.extracted_content)\n        print(\"--- END LLM RAW RESPONSE ---\")\n\n        if result.success:\n            with open(\"kb_result.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.extracted_content)\n            llm_strat.show_usage()\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nCopy\n```\n\n**Key Observations** :\n  * **`extraction_type=\"schema\"`**ensures we get JSON fitting our`KnowledgeGraph`. \n  * **`input_format=\"html\"`**means we feed HTML to the model.\n  * **`instruction`**guides the model to output a structured knowledge graph.\n\n\n* * *\n## 10. Best Practices & Caveats\n1. **Cost & Latency**: LLM calls can be slow or expensive. Consider chunking or smaller coverage if you only need partial data.  \n2. **Model Token Limits** : If your page + instruction exceed the context window, chunking is essential.  \n3. **Instruction Engineering** : Well-crafted instructions can drastically improve output reliability.  \n4. **Schema Strictness** : `\"schema\"` extraction tries to parse the model output as JSON. If the model returns invalid JSON, partial extraction might happen, or you might get an error.  \n5. **Parallel vs. Serial** : The library can process multiple chunks in parallel, but you must watch out for rate limits on certain providers.  \n6. **Check Output** : Sometimes, an LLM might omit fields or produce extraneous text. You may want to post-validate with Pydantic or do additional cleanup.\n* * *\n## 11. Conclusion\n**LLM-based extraction** in Crawl4AI is **provider-agnostic** , letting you choose from hundreds of models via LiteLLM. Itâ€™s perfect for **semantically complex** tasks or generating advanced structures like knowledge graphs. However, itâ€™s **slower** and potentially costlier than schema-based approaches. Keep these tips in mind:\n  * Put your LLM strategy **in`CrawlerRunConfig`**. \n  * Use **`input_format`**to pick which form (markdown, HTML, fit_markdown) the LLM sees.\n  * Tweak **`chunk_token_threshold`**,**`overlap_rate`**, and**`apply_chunking`**to handle large content efficiently.\n  * Monitor token usage with `show_usage()`.\n\n\nIf your siteâ€™s data is consistent or repetitive, consider [`JsonCssExtractionStrategy`](https://docs.crawl4ai.com/extraction/no-llm-strategies/) first for speed and simplicity. But if you need an **AI-driven** approach, `LLMExtractionStrategy` offers a flexible, multi-provider solution for extracting structured JSON from any website.\n**Next Steps** :\n1. **Experiment with Different Providers**  \n- Try switching the `provider` (e.g., `\"ollama/llama2\"`, `\"openai/gpt-4o\"`, etc.) to see differences in speed, accuracy, or cost.  \n- Pass different `extra_args` like `temperature`, `top_p`, and `max_tokens` to fine-tune your results.\n2. **Performance Tuning**  \n- If pages are large, tweak `chunk_token_threshold`, `overlap_rate`, or `apply_chunking` to optimize throughput.  \n- Check the usage logs with `show_usage()` to keep an eye on token consumption and identify potential bottlenecks.\n3. **Validate Outputs**  \n- If using `extraction_type=\"schema\"`, parse the LLMâ€™s JSON with a Pydantic model for a final validation step.  \n- Log or handle any parse errors gracefully, especially if the model occasionally returns malformed JSON.\n4. **Explore Hooks & Automation**  \n- Integrate LLM extraction with [hooks](https://docs.crawl4ai.com/advanced/hooks-auth/) for complex pre/post-processing.  \n- Use a multi-step pipeline: crawl, filter, LLM-extract, then store or index results for further analysis.\n**Last Updated** : 2025-01-01\n* * *\nThatâ€™s it for **Extracting JSON (LLM)** â€”now you can harness AI to parse, classify, or reorganize data on the web. Happy crawling!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/extraction/llm-strategies/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/extraction/llm-strategies/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/extraction/llm-strategies/)\n\n\nESC to close\n#### On this page\n  * [1. Why Use an LLM?](https://docs.crawl4ai.com/extraction/llm-strategies/#1-why-use-an-llm)\n  * [2. Provider-Agnostic via LiteLLM](https://docs.crawl4ai.com/extraction/llm-strategies/#2-provider-agnostic-via-litellm)\n  * [3. How LLM Extraction Works](https://docs.crawl4ai.com/extraction/llm-strategies/#3-how-llm-extraction-works)\n  * [3.1 Flow](https://docs.crawl4ai.com/extraction/llm-strategies/#31-flow)\n  * [3.2 extraction_type](https://docs.crawl4ai.com/extraction/llm-strategies/#32-extraction_type)\n  * [4. Key Parameters](https://docs.crawl4ai.com/extraction/llm-strategies/#4-key-parameters)\n  * [5. Putting It in CrawlerRunConfig](https://docs.crawl4ai.com/extraction/llm-strategies/#5-putting-it-in-crawlerrunconfig)\n  * [6. Chunking Details](https://docs.crawl4ai.com/extraction/llm-strategies/#6-chunking-details)\n  * [6.1 chunk_token_threshold](https://docs.crawl4ai.com/extraction/llm-strategies/#61-chunk_token_threshold)\n  * [6.2 overlap_rate](https://docs.crawl4ai.com/extraction/llm-strategies/#62-overlap_rate)\n  * [6.3 Performance & Parallelism](https://docs.crawl4ai.com/extraction/llm-strategies/#63-performance-parallelism)\n  * [7. Input Format](https://docs.crawl4ai.com/extraction/llm-strategies/#7-input-format)\n  * [8. Token Usage & Show Usage](https://docs.crawl4ai.com/extraction/llm-strategies/#8-token-usage-show-usage)\n  * [9. Example: Building a Knowledge Graph](https://docs.crawl4ai.com/extraction/llm-strategies/#9-example-building-a-knowledge-graph)\n  * [10. Best Practices & Caveats](https://docs.crawl4ai.com/extraction/llm-strategies/#10-best-practices-caveats)\n  * [11. Conclusion](https://docs.crawl4ai.com/extraction/llm-strategies/#11-conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/extraction/no-llm-strategies",
    "depth": 1,
    "title": "LLM-Free Strategies - Crawl4AI Documentation (v0.7.x)",
    "content_hash": "fc563cd520b9623ee500759eed2aaafd",
    "content": "[Crawl4AI Documentation (v0.7.x)](https://docs.crawl4ai.com/)\n  * [ Home ](https://docs.crawl4ai.com/)\n  * [ ðŸ“š Complete SDK Reference ](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/)\n  * [ Quick Start ](https://docs.crawl4ai.com/core/quickstart/)\n  * [ Code Examples ](https://docs.crawl4ai.com/core/examples/)\n  * [ Brand Book ](https://docs.crawl4ai.com/branding/)\n  * [ Search ](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n\n\n[ unclecode/crawl4ai ](https://github.com/unclecode/crawl4ai)\nÃ—\n  * [Home](https://docs.crawl4ai.com/)\n  * [ðŸ“š Complete SDK Reference](https://docs.crawl4ai.com/complete-sdk-reference/)\n  * [Ask AI](https://docs.crawl4ai.com/core/ask-ai/)\n  * [Quick Start](https://docs.crawl4ai.com/core/quickstart/)\n  * [Code Examples](https://docs.crawl4ai.com/core/examples/)\n  * Apps\n    * [Demo Apps](https://docs.crawl4ai.com/apps/)\n    * [C4A-Script Editor](https://docs.crawl4ai.com/apps/c4a-script/)\n    * [LLM Context Builder](https://docs.crawl4ai.com/apps/llmtxt/)\n    * [Marketplace](https://docs.crawl4ai.com/marketplace/)\n    * [Marketplace Admin](https://docs.crawl4ai.com/marketplace/admin/)\n  * Setup & Installation\n    * [Installation](https://docs.crawl4ai.com/core/installation/)\n    * [Self-Hosting Guide](https://docs.crawl4ai.com/core/self-hosting/)\n  * Blog & Changelog\n    * [Blog Home](https://docs.crawl4ai.com/blog/)\n    * [Changelog](https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md)\n  * Core\n    * [Command Line Interface](https://docs.crawl4ai.com/core/cli/)\n    * [Simple Crawling](https://docs.crawl4ai.com/core/simple-crawling/)\n    * [Deep Crawling](https://docs.crawl4ai.com/core/deep-crawling/)\n    * [Adaptive Crawling](https://docs.crawl4ai.com/core/adaptive-crawling/)\n    * [URL Seeding](https://docs.crawl4ai.com/core/url-seeding/)\n    * [C4A-Script](https://docs.crawl4ai.com/core/c4a-script/)\n    * [Crawler Result](https://docs.crawl4ai.com/core/crawler-result/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/core/browser-crawler-config/)\n    * [Markdown Generation](https://docs.crawl4ai.com/core/markdown-generation/)\n    * [Fit Markdown](https://docs.crawl4ai.com/core/fit-markdown/)\n    * [Page Interaction](https://docs.crawl4ai.com/core/page-interaction/)\n    * [Content Selection](https://docs.crawl4ai.com/core/content-selection/)\n    * [Cache Modes](https://docs.crawl4ai.com/core/cache-modes/)\n    * [Local Files & Raw HTML](https://docs.crawl4ai.com/core/local-files/)\n    * [Link & Media](https://docs.crawl4ai.com/core/link-media/)\n  * Advanced\n    * [Overview](https://docs.crawl4ai.com/advanced/advanced-features/)\n    * [Adaptive Strategies](https://docs.crawl4ai.com/advanced/adaptive-strategies/)\n    * [Virtual Scroll](https://docs.crawl4ai.com/advanced/virtual-scroll/)\n    * [File Downloading](https://docs.crawl4ai.com/advanced/file-downloading/)\n    * [Lazy Loading](https://docs.crawl4ai.com/advanced/lazy-loading/)\n    * [Hooks & Auth](https://docs.crawl4ai.com/advanced/hooks-auth/)\n    * [Proxy & Security](https://docs.crawl4ai.com/advanced/proxy-security/)\n    * [Undetected Browser](https://docs.crawl4ai.com/advanced/undetected-browser/)\n    * [Session Management](https://docs.crawl4ai.com/advanced/session-management/)\n    * [Multi-URL Crawling](https://docs.crawl4ai.com/advanced/multi-url-crawling/)\n    * [Crawl Dispatcher](https://docs.crawl4ai.com/advanced/crawl-dispatcher/)\n    * [Identity Based Crawling](https://docs.crawl4ai.com/advanced/identity-based-crawling/)\n    * [SSL Certificate](https://docs.crawl4ai.com/advanced/ssl-certificate/)\n    * [Network & Console Capture](https://docs.crawl4ai.com/advanced/network-console-capture/)\n    * [PDF Parsing](https://docs.crawl4ai.com/advanced/pdf-parsing/)\n  * Extraction\n    * LLM-Free Strategies\n    * [LLM Strategies](https://docs.crawl4ai.com/extraction/llm-strategies/)\n    * [Clustering Strategies](https://docs.crawl4ai.com/extraction/clustring-strategies/)\n    * [Chunking](https://docs.crawl4ai.com/extraction/chunking/)\n  * API Reference\n    * [AsyncWebCrawler](https://docs.crawl4ai.com/api/async-webcrawler/)\n    * [arun()](https://docs.crawl4ai.com/api/arun/)\n    * [arun_many()](https://docs.crawl4ai.com/api/arun_many/)\n    * [Browser, Crawler & LLM Config](https://docs.crawl4ai.com/api/parameters/)\n    * [CrawlResult](https://docs.crawl4ai.com/api/crawl-result/)\n    * [Strategies](https://docs.crawl4ai.com/api/strategies/)\n    * [C4A-Script Reference](https://docs.crawl4ai.com/api/c4a-script-reference/)\n  * [Brand Book](https://docs.crawl4ai.com/branding/)\n\n\n* * *\n  * [Extracting JSON (No LLM)](https://docs.crawl4ai.com/extraction/no-llm-strategies/#extracting-json-no-llm)\n  * [1. Intro to Schema-Based Extraction](https://docs.crawl4ai.com/extraction/no-llm-strategies/#1-intro-to-schema-based-extraction)\n  * [2. Simple Example: Crypto Prices](https://docs.crawl4ai.com/extraction/no-llm-strategies/#2-simple-example-crypto-prices)\n  * [3. Advanced Schema & Nested Structures](https://docs.crawl4ai.com/extraction/no-llm-strategies/#3-advanced-schema-nested-structures)\n  * [4. RegexExtractionStrategy - Fast Pattern-Based Extraction](https://docs.crawl4ai.com/extraction/no-llm-strategies/#4-regexextractionstrategy-fast-pattern-based-extraction)\n  * [5. Why \"No LLM\" Is Often Better](https://docs.crawl4ai.com/extraction/no-llm-strategies/#5-why-no-llm-is-often-better)\n  * [6. Base Element Attributes & Additional Fields](https://docs.crawl4ai.com/extraction/no-llm-strategies/#6-base-element-attributes-additional-fields)\n  * [7. Putting It All Together: Larger Example](https://docs.crawl4ai.com/extraction/no-llm-strategies/#7-putting-it-all-together-larger-example)\n  * [8. Tips & Best Practices](https://docs.crawl4ai.com/extraction/no-llm-strategies/#8-tips-best-practices)\n  * [9. Schema Generation Utility](https://docs.crawl4ai.com/extraction/no-llm-strategies/#9-schema-generation-utility)\n  * [10. Conclusion](https://docs.crawl4ai.com/extraction/no-llm-strategies/#10-conclusion)\n\n\n# Extracting JSON (No LLM)\nOne of Crawl4AI's **most powerful** features is extracting **structured JSON** from websites **without** relying on large language models. Crawl4AI offers several strategies for LLM-free extraction:\n  1. **Schema-based extraction** with CSS or XPath selectors via `JsonCssExtractionStrategy` and `JsonXPathExtractionStrategy`\n  2. **Regular expression extraction** with `RegexExtractionStrategy` for fast pattern matching\n\n\nThese approaches let you extract data instantlyâ€”even from complex or nested HTML structuresâ€”without the cost, latency, or environmental impact of an LLM.\n**Why avoid LLM for basic extractions?**\n  1. **Faster & Cheaper**: No API calls or GPU overhead. \n  2. **Lower Carbon Footprint** : LLM inference can be energy-intensive. Pattern-based extraction is practically carbon-free. \n  3. **Precise & Repeatable**: CSS/XPath selectors and regex patterns do exactly what you specify. LLM outputs can vary or hallucinate. \n  4. **Scales Readily** : For thousands of pages, pattern-based extraction runs quickly and in parallel.\n\n\nBelow, we'll explore how to craft these schemas and use them with **JsonCssExtractionStrategy** (or **JsonXPathExtractionStrategy** if you prefer XPath). We'll also highlight advanced features like **nested fields** and **base element attributes**.\n* * *\n## 1. Intro to Schema-Based Extraction\nA schema defines:\n  1. A **base selector** that identifies each \"container\" element on the page (e.g., a product row, a blog post card). \n  2. **Fields** describing which CSS/XPath selectors to use for each piece of data you want to capture (text, attribute, HTML block, etc.). \n  3. **Nested** or **list** types for repeated or hierarchical structures. \n\n\nFor example, if you have a list of products, each one might have a name, price, reviews, and \"related products.\" This approach is faster and more reliable than an LLM for consistent, structured pages.\n* * *\n## 2. Simple Example: Crypto Prices\nLet's begin with a **simple** schema-based extraction using the `JsonCssExtractionStrategy`. Below is a snippet that extracts cryptocurrency prices from a site (similar to the legacy Coinbase example). Notice we **don't** call any LLM:\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\n\nasync def extract_crypto_prices():\n    # 1. Define a simple extraction schema\n    schema = {\n        \"name\": \"Crypto Prices\",\n        \"baseSelector\": \"div.crypto-row\",    # Repeated elements\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \"h2.coin-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"span.coin-price\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 2. Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # 3. Set up your crawler config (if needed)\n    config = CrawlerRunConfig(\n        # e.g., pass js_code or wait_for if the page is dynamic\n        # wait_for=\"css:.crypto-row:nth-child(20)\"\n        cache_mode = CacheMode.BYPASS,\n        extraction_strategy=extraction_strategy,\n    )\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # 4. Run the crawl and extraction\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # 5. Parse the extracted JSON\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin entries\")\n        print(json.dumps(data[0], indent=2) if data else \"No data found\")\n\nasyncio.run(extract_crypto_prices())\nCopy\n```\n\n**Highlights** :\n  * **`baseSelector`**: Tells us where each \"item\" (crypto row) is.\n  * **`fields`**: Two fields (`coin_name` , `price`) using simple CSS selectors. \n  * Each field defines a **`type`**(e.g.,`text` , `attribute`, `html`, `regex`, etc.).\n\n\nNo LLM is needed, and the performance is **near-instant** for hundreds or thousands of items.\n* * *\n### **XPath Example with`raw://` HTML**\nBelow is a short example demonstrating **XPath** extraction plus the **`raw://`**scheme. We'll pass a**dummy HTML** directly (no network request) and define the extraction strategy in `CrawlerRunConfig`.\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai import JsonXPathExtractionStrategy\n\nasync def extract_crypto_prices_xpath():\n    # 1. Minimal dummy HTML with some repeating rows\n    dummy_html = \"\"\"\n    <html>\n      <body>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Bitcoin</h2>\n          <span class='coin-price'>$28,000</span>\n        </div>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Ethereum</h2>\n          <span class='coin-price'>$1,800</span>\n        </div>\n      </body>\n    </html>\n    \"\"\"\n\n    # 2. Define the JSON schema (XPath version)\n    schema = {\n        \"name\": \"Crypto Prices via XPath\",\n        \"baseSelector\": \"//div[@class='crypto-row']\",\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \".//h2[@class='coin-name']\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \".//span[@class='coin-price']\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 3. Place the strategy in the CrawlerRunConfig\n    config = CrawlerRunConfig(\n        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)\n    )\n\n    # 4. Use raw:// scheme to pass dummy_html directly\n    raw_url = f\"raw://{dummy_html}\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=raw_url,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin rows\")\n        if data:\n            print(\"First item:\", data[0])\n\nasyncio.run(extract_crypto_prices_xpath())\nCopy\n```\n\n**Key Points** :\n  1. **`JsonXPathExtractionStrategy`**is used instead of`JsonCssExtractionStrategy`. \n  2. **`baseSelector`**and each field's`\"selector\"` use **XPath** instead of CSS. \n  3. **`raw://`**lets us pass`dummy_html` with no real network requestâ€”handy for local testing. \n  4. Everything (including the extraction strategy) is in **`CrawlerRunConfig`**.\n\n\nThat's how you keep the config self-contained, illustrate **XPath** usage, and demonstrate the **raw** scheme for direct HTML inputâ€”all while avoiding the old approach of passing `extraction_strategy` directly to `arun()`.\n* * *\n## 3. Advanced Schema & Nested Structures\nReal sites often have **nested** or repeated dataâ€”like categories containing products, which themselves have a list of reviews or features. For that, we can define **nested** or **list** (and even **nested_list**) fields.\n### Sample E-Commerce HTML\nWe have a **sample e-commerce** HTML file on GitHub (example): \n```\nhttps://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\nCopy\n```\n\nThis snippet includes categories, products, features, reviews, and related items. Let's see how to define a schema that fully captures that structure **without LLM**.\n```\nschema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    # (1) We can define optional baseFields if we want to extract attributes \n    # from the category container\n    \"baseFields\": [\n        {\"name\": \"data_cat_id\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"}, \n    ],\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",    # repeated sub-objects\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",  # single sub-object\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\"name\": \"feature\", \"type\": \"text\"} \n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\", \n                            \"selector\": \"span.reviewer\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\", \n                            \"selector\": \"span.rating\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\", \n                            \"selector\": \"p.review-text\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\", \n                            \"selector\": \"span.related-name\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\", \n                            \"selector\": \"span.related-price\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\nCopy\n```\n\nKey Takeaways:\n  * **Nested vs. List** : \n  * **`type: \"nested\"`**means a**single** sub-object (like `details`). \n  * **`type: \"list\"`**means multiple items that are**simple** dictionaries or single text fields. \n  * **`type: \"nested_list\"`**means repeated**complex** objects (like `products` or `reviews`).\n  * **Base Fields** : We can extract **attributes** from the container element via `\"baseFields\"`. For instance, `\"data_cat_id\"` might be `data-cat-id=\"elect123\"`. \n  * **Transforms** : We can also define a `transform` if we want to lower/upper case, strip whitespace, or even run a custom function.\n\n\n### Running the Extraction\n```\nimport json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai import JsonCssExtractionStrategy\n\necommerce_schema = {\n    # ... the advanced schema from above ...\n}\n\nasync def extract_ecommerce_data():\n    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)\n\n    config = CrawlerRunConfig()\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=strategy,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # Parse the JSON output\n        data = json.loads(result.extracted_content)\n        print(json.dumps(data, indent=2) if data else \"No data found.\")\n\nasyncio.run(extract_ecommerce_data())\nCopy\n```\n\nIf all goes well, you get a **structured** JSON array with each \"category,\" containing an array of `products`. Each product includes `details`, `features`, `reviews`, etc. All of that **without** an LLM.\n* * *\n## 4. RegexExtractionStrategy - Fast Pattern-Based Extraction\nCrawl4AI now offers a powerful new zero-LLM extraction strategy: `RegexExtractionStrategy`. This strategy provides lightning-fast extraction of common data types like emails, phone numbers, URLs, dates, and more using pre-compiled regular expressions.\n### Key Features\n  * **Zero LLM Dependency** : Extracts data without any AI model calls\n  * **Blazing Fast** : Uses pre-compiled regex patterns for maximum performance\n  * **Built-in Patterns** : Includes ready-to-use patterns for common data types\n  * **Custom Patterns** : Add your own regex patterns for domain-specific extraction\n  * **LLM-Assisted Pattern Generation** : Optionally use an LLM once to generate optimized patterns, then reuse them without further LLM calls\n\n\n### Simple Example: Extracting Common Entities\nThe easiest way to start is by using the built-in pattern catalog:\n```\nimport json\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy\n)\n\nasync def extract_with_regex():\n    # Create a strategy using built-in patterns for URLs and currencies\n    strategy = RegexExtractionStrategy(\n        pattern = RegexExtractionStrategy.Url | RegexExtractionStrategy.Currency\n    )\n\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data[:5]:  # Show first 5 matches\n                print(f\"{item['label']}: {item['value']}\")\n            print(f\"Total matches: {len(data)}\")\n\nasyncio.run(extract_with_regex())\nCopy\n```\n\n### Available Built-in Patterns\n`RegexExtractionStrategy` provides these common patterns as IntFlag attributes for easy combining:\n```\n# Use individual patterns\nstrategy = RegexExtractionStrategy(pattern=RegexExtractionStrategy.Email)\n\n# Combine multiple patterns\nstrategy = RegexExtractionStrategy(\n    pattern = (\n        RegexExtractionStrategy.Email | \n        RegexExtractionStrategy.PhoneUS | \n        RegexExtractionStrategy.Url\n    )\n)\n\n# Use all available patterns\nstrategy = RegexExtractionStrategy(pattern=RegexExtractionStrategy.All)\nCopy\n```\n\nAvailable patterns include: - `Email` - Email addresses - `PhoneIntl` - International phone numbers - `PhoneUS` - US-format phone numbers - `Url` - HTTP/HTTPS URLs - `IPv4` - IPv4 addresses - `IPv6` - IPv6 addresses - `Uuid` - UUIDs - `Currency` - Currency values (USD, EUR, etc.) - `Percentage` - Percentage values - `Number` - Numeric values - `DateIso` - ISO format dates - `DateUS` - US format dates - `Time24h` - 24-hour format times - `PostalUS` - US postal codes - `PostalUK` - UK postal codes - `HexColor` - HTML hex color codes - `TwitterHandle` - Twitter handles - `Hashtag` - Hashtags - `MacAddr` - MAC addresses - `Iban` - International bank account numbers - `CreditCard` - Credit card numbers\n### Custom Pattern Example\nFor more targeted extraction, you can provide custom patterns:\n```\nimport json\nimport asyncio\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy\n)\n\nasync def extract_prices():\n    # Define a custom pattern for US Dollar prices\n    price_pattern = {\"usd_price\": r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\"}\n\n    # Create strategy with custom pattern\n    strategy = RegexExtractionStrategy(custom=price_pattern)\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com/products\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data:\n                print(f\"Found price: {item['value']}\")\n\nasyncio.run(extract_prices())\nCopy\n```\n\n### LLM-Assisted Pattern Generation\nFor complex or site-specific patterns, you can use an LLM once to generate an optimized pattern, then save and reuse it without further LLM calls:\n```\nimport json\nimport asyncio\nfrom pathlib import Path\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    CrawlerRunConfig,\n    RegexExtractionStrategy,\n    LLMConfig\n)\n\nasync def extract_with_generated_pattern():\n    cache_dir = Path(\"./pattern_cache\")\n    cache_dir.mkdir(exist_ok=True)\n    pattern_file = cache_dir / \"price_pattern.json\"\n\n    # 1. Generate or load pattern\n    if pattern_file.exists():\n        pattern = json.load(pattern_file.open())\n        print(f\"Using cached pattern: {pattern}\")\n    else:\n        print(\"Generating pattern via LLM...\")\n\n        # Configure LLM\n        llm_config = LLMConfig(\n            provider=\"openai/gpt-4o-mini\",\n            api_token=\"env:OPENAI_API_KEY\",\n        )\n\n        # Get sample HTML for context\n        async with AsyncWebCrawler() as crawler:\n            result = await crawler.arun(\"https://example.com/products\")\n            html = result.fit_html\n\n        # Generate pattern (one-time LLM usage)\n        pattern = RegexExtractionStrategy.generate_pattern(\n            label=\"price\",\n            html=html,\n            query=\"Product prices in USD format\",\n            llm_config=llm_config,\n        )\n\n        # Cache pattern for future use\n        json.dump(pattern, pattern_file.open(\"w\"), indent=2)\n\n    # 2. Use pattern for extraction (no LLM calls)\n    strategy = RegexExtractionStrategy(custom=pattern)\n    config = CrawlerRunConfig(extraction_strategy=strategy)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=config\n        )\n\n        if result.success:\n            data = json.loads(result.extracted_content)\n            for item in data[:10]:\n                print(f\"Extracted: {item['value']}\")\n            print(f\"Total matches: {len(data)}\")\n\nasyncio.run(extract_with_generated_pattern())\nCopy\n```\n\nThis pattern allows you to: 1. Use an LLM once to generate a highly optimized regex for your specific site 2. Save the pattern to disk for reuse 3. Extract data using only regex (no further LLM calls) in production\n### Extraction Results Format\nThe `RegexExtractionStrategy` returns results in a consistent format:\n```\n[\n  {\n    \"url\": \"https://example.com\",\n    \"label\": \"email\",\n    \"value\": \"contact@example.com\",\n    \"span\": [145, 163]\n  },\n  {\n    \"url\": \"https://example.com\",\n    \"label\": \"url\",\n    \"value\": \"https://support.example.com\",\n    \"span\": [210, 235]\n  }\n]\nCopy\n```\n\nEach match includes: - `url`: The source URL - `label`: The pattern name that matched (e.g., \"email\", \"phone_us\") - `value`: The extracted text - `span`: The start and end positions in the source content\n* * *\n## 5. Why \"No LLM\" Is Often Better\n  1. **Zero Hallucination** : Pattern-based extraction doesn't guess text. It either finds it or not. \n  2. **Guaranteed Structure** : The same schema or regex yields consistent JSON across many pages, so your downstream pipeline can rely on stable keys. \n  3. **Speed** : LLM-based extraction can be 10â€“1000x slower for large-scale crawling. \n  4. **Scalable** : Adding or updating a field is a matter of adjusting the schema or regex, not re-tuning a model.\n\n\n**When might you consider an LLM?** Possibly if the site is extremely unstructured or you want AI summarization. But always try a schema or regex approach first for repeated or consistent data patterns.\n* * *\n## 6. Base Element Attributes & Additional Fields\nIt's easy to **extract attributes** (like `href`, `src`, or `data-xxx`) from your base or nested elements using:\n```\n{\n  \"name\": \"href\",\n  \"type\": \"attribute\",\n  \"attribute\": \"href\",\n  \"default\": null\n}\nCopy\n```\n\nYou can define them in **`baseFields`**(extracted from the main container element) or in each field's sub-lists. This is especially helpful if you need an item's link or ID stored in the parent`<div>`.\n* * *\n## 7. Putting It All Together: Larger Example\nConsider a blog site. We have a schema that extracts the **URL** from each post card (via `baseFields` with an `\"attribute\": \"href\"`), plus the title, date, summary, and author:\n```\nschema = {\n  \"name\": \"Blog Posts\",\n  \"baseSelector\": \"a.blog-post-card\",\n  \"baseFields\": [\n    {\"name\": \"post_url\", \"type\": \"attribute\", \"attribute\": \"href\"}\n  ],\n  \"fields\": [\n    {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"},\n    {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"}\n  ]\n}\nCopy\n```\n\nThen run with `JsonCssExtractionStrategy(schema)` to get an array of blog post objects, each with `\"post_url\"`, `\"title\"`, `\"date\"`, `\"summary\"`, `\"author\"`.\n* * *\n## 8. Tips & Best Practices\n  1. **Inspect the DOM** in Chrome DevTools or Firefox's Inspector to find stable selectors. \n  2. **Start Simple** : Verify you can extract a single field. Then add complexity like nested objects or lists. \n  3. **Test** your schema on partial HTML or a test page before a big crawl. \n  4. **Combine with JS Execution** if the site loads content dynamically. You can pass `js_code` or `wait_for` in `CrawlerRunConfig`. \n  5. **Look at Logs** when `verbose=True`: if your selectors are off or your schema is malformed, it'll often show warnings. \n  6. **Use baseFields** if you need attributes from the container element (e.g., `href`, `data-id`), especially for the \"parent\" item. \n  7. **Performance** : For large pages, make sure your selectors are as narrow as possible.\n  8. **Consider Using Regex First** : For simple data types like emails, URLs, and dates, `RegexExtractionStrategy` is often the fastest approach.\n\n\n* * *\n## 9. Schema Generation Utility\nWhile manually crafting schemas is powerful and precise, Crawl4AI now offers a convenient utility to **automatically generate** extraction schemas using LLM. This is particularly useful when:\n  1. You're dealing with a new website structure and want a quick starting point\n  2. You need to extract complex nested data structures\n  3. You want to avoid the learning curve of CSS/XPath selector syntax\n\n\n### Using the Schema Generator\nThe schema generator is available as a static method on both `JsonCssExtractionStrategy` and `JsonXPathExtractionStrategy`. You can choose between OpenAI's GPT-4 or the open-source Ollama for schema generation:\n```\nfrom crawl4ai import JsonCssExtractionStrategy, JsonXPathExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Sample HTML with product information\nhtml = \"\"\"\n<div class=\"product-card\">\n    <h2 class=\"title\">Gaming Laptop</h2>\n    <div class=\"price\">$999.99</div>\n    <div class=\"specs\">\n        <ul>\n            <li>16GB RAM</li>\n            <li>1TB SSD</li>\n        </ul>\n    </div>\n</div>\n\"\"\"\n\n# Option 1: Using OpenAI (requires API token)\ncss_schema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"css\", \n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")\n)\n\n# Option 2: Using Ollama (open source, no token needed)\nxpath_schema = JsonXPathExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"xpath\",\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the generated schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(css_schema)\nCopy\n```\n\n### LLM Provider Options\n  1. **OpenAI GPT-4 (`openai/gpt4o`)**\n  2. Default provider\n  3. Requires an API token\n  4. Generally provides more accurate schemas\n  5. Set via environment variable: `OPENAI_API_KEY`\n  6. **Ollama (`ollama/llama3.3`)**\n  7. Open source alternative\n  8. No API token required\n  9. Self-hosted option\n  10. Good for development and testing\n\n\n### Benefits of Schema Generation\n  1. **One-Time Cost** : While schema generation uses LLM, it's a one-time cost. The generated schema can be reused for unlimited extractions without further LLM calls.\n  2. **Smart Pattern Recognition** : The LLM analyzes the HTML structure and identifies common patterns, often producing more robust selectors than manual attempts.\n  3. **Automatic Nesting** : Complex nested structures are automatically detected and properly represented in the schema.\n  4. **Learning Tool** : The generated schemas serve as excellent examples for learning how to write your own schemas.\n\n\n### Best Practices\n  1. **Review Generated Schemas** : While the generator is smart, always review and test the generated schema before using it in production.\n  2. **Provide Representative HTML** : The better your sample HTML represents the overall structure, the more accurate the generated schema will be.\n  3. **Consider Both CSS and XPath** : Try both schema types and choose the one that works best for your specific case.\n  4. **Cache Generated Schemas** : Since generation uses LLM, save successful schemas for reuse.\n  5. **API Token Security** : Never hardcode API tokens. Use environment variables or secure configuration management.\n  6. **Choose Provider Wisely** : \n  7. Use OpenAI for production-quality schemas\n  8. Use Ollama for development, testing, or when you need a self-hosted solution\n\n\n* * *\n## 10. Conclusion\nWith Crawl4AI's LLM-free extraction strategies - `JsonCssExtractionStrategy`, `JsonXPathExtractionStrategy`, and now `RegexExtractionStrategy` - you can build powerful pipelines that:\n  * Scrape any consistent site for structured data. \n  * Support nested objects, repeating lists, or pattern-based extraction. \n  * Scale to thousands of pages quickly and reliably.\n\n\n**Choosing the Right Strategy** :\n  * Use **`RegexExtractionStrategy`**for fast extraction of common data types like emails, phones, URLs, dates, etc.\n  * Use **`JsonCssExtractionStrategy`**or**`JsonXPathExtractionStrategy`**for structured data with clear HTML patterns\n  * If you need both: first extract structured data with JSON strategies, then use regex on specific fields\n\n\n**Remember** : For repeated, structured data, you don't need to pay for or wait on an LLM. Well-crafted schemas and regex patterns get you the data faster, cleaner, and cheaperâ€”**the real power** of Crawl4AI.\n**Last Updated** : 2025-05-02\n* * *\nThat's it for **Extracting JSON (No LLM)**! You've seen how schema-based approaches (either CSS or XPath) and regex patterns can handle everything from simple lists to deeply nested product catalogsâ€”instantly, with minimal overhead. Enjoy building robust scrapers that produce consistent, structured JSON for your data pipelines!\nPage Copy\nPage Copy\n  * [ Copy as Markdown Copy page for LLMs ](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n  * [ View as Markdown Open raw source ](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n  * [ Open in ChatGPT Ask questions about this page ](https://docs.crawl4ai.com/extraction/no-llm-strategies/)\n\n\nESC to close\n#### On this page\n  * [1. Intro to Schema-Based Extraction](https://docs.crawl4ai.com/extraction/no-llm-strategies/#1-intro-to-schema-based-extraction)\n  * [2. Simple Example: Crypto Prices](https://docs.crawl4ai.com/extraction/no-llm-strategies/#2-simple-example-crypto-prices)\n  * [XPath Example with raw:// HTML](https://docs.crawl4ai.com/extraction/no-llm-strategies/#xpath-example-with-raw-html)\n  * [3. Advanced Schema & Nested Structures](https://docs.crawl4ai.com/extraction/no-llm-strategies/#3-advanced-schema-nested-structures)\n  * [Sample E-Commerce HTML](https://docs.crawl4ai.com/extraction/no-llm-strategies/#sample-e-commerce-html)\n  * [Running the Extraction](https://docs.crawl4ai.com/extraction/no-llm-strategies/#running-the-extraction)\n  * [4. RegexExtractionStrategy - Fast Pattern-Based Extraction](https://docs.crawl4ai.com/extraction/no-llm-strategies/#4-regexextractionstrategy-fast-pattern-based-extraction)\n  * [Key Features](https://docs.crawl4ai.com/extraction/no-llm-strategies/#key-features)\n  * [Simple Example: Extracting Common Entities](https://docs.crawl4ai.com/extraction/no-llm-strategies/#simple-example-extracting-common-entities)\n  * [Available Built-in Patterns](https://docs.crawl4ai.com/extraction/no-llm-strategies/#available-built-in-patterns)\n  * [Custom Pattern Example](https://docs.crawl4ai.com/extraction/no-llm-strategies/#custom-pattern-example)\n  * [LLM-Assisted Pattern Generation](https://docs.crawl4ai.com/extraction/no-llm-strategies/#llm-assisted-pattern-generation)\n  * [Extraction Results Format](https://docs.crawl4ai.com/extraction/no-llm-strategies/#extraction-results-format)\n  * [5. Why \"No LLM\" Is Often Better](https://docs.crawl4ai.com/extraction/no-llm-strategies/#5-why-no-llm-is-often-better)\n  * [6. Base Element Attributes & Additional Fields](https://docs.crawl4ai.com/extraction/no-llm-strategies/#6-base-element-attributes-additional-fields)\n  * [7. Putting It All Together: Larger Example](https://docs.crawl4ai.com/extraction/no-llm-strategies/#7-putting-it-all-together-larger-example)\n  * [8. Tips & Best Practices](https://docs.crawl4ai.com/extraction/no-llm-strategies/#8-tips-best-practices)\n  * [9. Schema Generation Utility](https://docs.crawl4ai.com/extraction/no-llm-strategies/#9-schema-generation-utility)\n  * [Using the Schema Generator](https://docs.crawl4ai.com/extraction/no-llm-strategies/#using-the-schema-generator)\n  * [LLM Provider Options](https://docs.crawl4ai.com/extraction/no-llm-strategies/#llm-provider-options)\n  * [Benefits of Schema Generation](https://docs.crawl4ai.com/extraction/no-llm-strategies/#benefits-of-schema-generation)\n  * [Best Practices](https://docs.crawl4ai.com/extraction/no-llm-strategies/#best-practices)\n  * [10. Conclusion](https://docs.crawl4ai.com/extraction/no-llm-strategies/#10-conclusion)\n\n\n* * *\n> Feedback \n##### Search\nxClose\nType to start searching\n[ Ask AI ](https://docs.crawl4ai.com/core/ask-ai/ \"Ask Crawl4AI Assistant\")\n"
  },
  {
    "url": "https://docs.crawl4ai.com/marketplace",
    "depth": 1,
    "title": "Marketplace - Crawl4AI",
    "content_hash": "154bfe36147c11d9d6b18c51eb4cb2fc",
    "content": "![Crawl4AI](https://docs.crawl4ai.com/assets/images/logo.png)\n#  [ Marketplace ]\nTools, Integrations & Resources for Web Crawling\nApps: 2 Articles: 1 Downloads: 0\n> `/`\nAll Captcha solversProxy ServicesLLM IntegrationCloud Browser\nSPONSORED\n##  > Latest Apps\nAll Open Source Paid\n##  > Latest Articles\n##  # Trending\n###  + Submit Your Tool\nShare your integration\nSubmit â†’\n##  > More Apps\nLoad More â†“\n### About Marketplace\nDiscover tools and integrations built by the Crawl4AI community.\n### Become a Sponsor\nReach developers building with Crawl4AI\nLearn More â†’\n[ Crawl4AI Marketplace Â· Updated 1/25/2026 ]\n"
  },
  {
    "url": "https://docs.crawl4ai.com/marketplace/admin",
    "depth": 1,
    "title": "Admin Dashboard - Crawl4AI Marketplace",
    "content_hash": "e9404b49b449db22b55e1345327d6f89",
    "content": "![Crawl4AI](https://docs.crawl4ai.com/assets/images/logo.png)\n# [ Admin Access ]\nâ†’ Login\n![Crawl4AI](https://docs.crawl4ai.com/assets/images/logo.png)\n# [ Admin Dashboard ]\nAdministrator â†— Logout\nâ–“ Dashboard  â—† Apps  â–  Articles  â–¡ Categories  â—† Sponsors \nâ†“ Export Data  â–ª Backup DB \n## Dashboard Overview\nâ—†\n--\nTotal Apps\n-- featured, -- sponsored \nâ– \n--\nArticles\nâ—†\n--\nActive Sponsors\nâ—\n--\nTotal Views\n### Quick Actions\nâ†’ Add New App  â†’ Write Article  â†’ Add Sponsor \n## Apps Management\nAll Categories â†’ Add App \n## Articles Management\nâ†’ Add Article \n## Categories Management\nâ†’ Add Category \n## Sponsors Management\nâ†’ Add Sponsor \n## Add/Edit\nâœ•\nCancel Save\n"
  }
]