{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bitsandbytes accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport json\n\nAPI_KEY = \"\"\n\nresponse = requests.post(\n    url=\"https://openrouter.ai/api/v1/chat/completions\",\n    headers={\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\",\n        \"HTTP-Referer\": \"http://localhost\",   # c√≥ th·ªÉ ƒë·ªÉ ƒë·∫°i\n        \"X-Title\": \"test-app\",                # t√™n app b·∫•t k·ª≥\n    },\n    data=json.dumps({\n        \"model\": \"meta-llama/llama-3.3-70b-instruct:free\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is the meaning of life?\"\n            }\n        ]\n    })\n)\n\nprint(\"Status:\", response.status_code)\n\n# N·∫øu l·ªói th√¨ in ra lu√¥n\nif response.status_code != 200:\n    print(\"‚ùå Error:\", response.text)\n    exit()\n\nresult = response.json()\n\n# In to√†n b·ªô JSON (debug)\nprint(\"\\n=== RAW RESPONSE ===\")\nprint(json.dumps(result, indent=2))\n\n# L·∫•y ri√™ng c√¢u tr·∫£ l·ªùi c·ªßa AI\nif \"choices\" in result:\n    print(\"\\n=== AI TR·∫¢ L·ªúI ===\\n\")\n    print(result[\"choices\"][0][\"message\"][\"content\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T08:05:30.307255Z","iopub.execute_input":"2026-02-02T08:05:30.307730Z","iopub.status.idle":"2026-02-02T08:05:40.747998Z","shell.execute_reply.started":"2026-02-02T08:05:30.307699Z","shell.execute_reply":"2026-02-02T08:05:40.747239Z"}},"outputs":[{"name":"stdout","text":"Status: 200\n\n=== RAW RESPONSE ===\n{\n  \"id\": \"gen-1770019530-FGFX8LWilzsNPIZ1ShnZ\",\n  \"provider\": \"OpenInference\",\n  \"model\": \"meta-llama/llama-3.3-70b-instruct:free\",\n  \"object\": \"chat.completion\",\n  \"created\": 1770019530,\n  \"choices\": [\n    {\n      \"logprobs\": null,\n      \"finish_reason\": \"stop\",\n      \"native_finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"The question of the meaning of life is a complex and subjective one that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer, as the meaning of life can vary greatly from person to person, culture to culture, and even moment to moment.\\n\\nSome possible perspectives on the meaning of life include:\\n\\n1. **Happiness and fulfillment**: For some, the meaning of life is to find happiness and fulfillment through personal relationships, achievements, and experiences.\\n2. **Purpose and direction**: Others believe that life has a specific purpose or direction, such as fulfilling a personal mission, pursuing a passion, or making a positive impact on the world.\\n3. **Spiritual or religious beliefs**: Many people find meaning in life through their spiritual or religious beliefs, which may provide a sense of connection to a higher power, a moral framework, and a sense of purpose.\\n4. **Personal growth and self-actualization**: Some individuals believe that the meaning of life is to continually learn, grow, and self-actualize, becoming the best version of themselves.\\n5. **Contributing to society**: Others find meaning in life by contributing to the greater good, whether through their work, volunteering, or other forms of service.\\n6. **Experiencing and appreciating life**: Some people believe that the meaning of life is simply to experience and appreciate the beauty, wonder, and complexity of life itself.\\n7. **Existentialism**: From an existentialist perspective, life has no inherent meaning, and individuals must create their own meaning and purpose through their choices and actions.\\n8. **Biological and evolutionary perspectives**: Some scientists and philosophers argue that the meaning of life is to survive, reproduce, and pass on genetic material to future generations.\\n9. **Cultural and societal norms**: The meaning of life can also be influenced by cultural and societal norms, such as the importance of family, community, and tradition.\\n10. **Personal values and goals**: Ultimately, the meaning of life may be unique to each individual, reflecting their personal values, goals, and aspirations.\\n\\nIt's worth noting that these perspectives are not mutually exclusive, and many people may find meaning in life through a combination of these factors.\\n\\nWhat do you think is the meaning of life?\",\n        \"refusal\": null,\n        \"reasoning\": null\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 42,\n    \"completion_tokens\": 463,\n    \"total_tokens\": 505,\n    \"cost\": 0,\n    \"is_byok\": false,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"cost_details\": {\n      \"upstream_inference_cost\": 0,\n      \"upstream_inference_prompt_cost\": 0,\n      \"upstream_inference_completions_cost\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0\n    }\n  }\n}\n\n=== AI TR·∫¢ L·ªúI ===\n\nThe question of the meaning of life is a complex and subjective one that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer, as the meaning of life can vary greatly from person to person, culture to culture, and even moment to moment.\n\nSome possible perspectives on the meaning of life include:\n\n1. **Happiness and fulfillment**: For some, the meaning of life is to find happiness and fulfillment through personal relationships, achievements, and experiences.\n2. **Purpose and direction**: Others believe that life has a specific purpose or direction, such as fulfilling a personal mission, pursuing a passion, or making a positive impact on the world.\n3. **Spiritual or religious beliefs**: Many people find meaning in life through their spiritual or religious beliefs, which may provide a sense of connection to a higher power, a moral framework, and a sense of purpose.\n4. **Personal growth and self-actualization**: Some individuals believe that the meaning of life is to continually learn, grow, and self-actualize, becoming the best version of themselves.\n5. **Contributing to society**: Others find meaning in life by contributing to the greater good, whether through their work, volunteering, or other forms of service.\n6. **Experiencing and appreciating life**: Some people believe that the meaning of life is simply to experience and appreciate the beauty, wonder, and complexity of life itself.\n7. **Existentialism**: From an existentialist perspective, life has no inherent meaning, and individuals must create their own meaning and purpose through their choices and actions.\n8. **Biological and evolutionary perspectives**: Some scientists and philosophers argue that the meaning of life is to survive, reproduce, and pass on genetic material to future generations.\n9. **Cultural and societal norms**: The meaning of life can also be influenced by cultural and societal norms, such as the importance of family, community, and tradition.\n10. **Personal values and goals**: Ultimately, the meaning of life may be unique to each individual, reflecting their personal values, goals, and aspirations.\n\nIt's worth noting that these perspectives are not mutually exclusive, and many people may find meaning in life through a combination of these factors.\n\nWhat do you think is the meaning of life?\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T07:35:01.667944Z","iopub.execute_input":"2026-02-02T07:35:01.668554Z","iopub.status.idle":"2026-02-02T07:41:20.355630Z","shell.execute_reply.started":"2026-02-02T07:35:01.668524Z","shell.execute_reply":"2026-02-02T07:41:20.354893Z"}},"outputs":[{"name":"stderr","text":"2026-02-02 07:35:08.401383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770017708.422692     454 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770017708.427926     454 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770017708.441281     454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770017708.441296     454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770017708.441299     454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770017708.441302     454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d445b6f46fa40d89c3d2ce865ac35e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc30bd6c94e464b86f7cac447ae41a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a24c5114b6774209bcf288cb74114f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/3.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0279109eb744e278b5b073c7549401c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a58450664a6407a828c94e2a65c8915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc459ad58e21466aace3543784bb053a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b50f8b54a4444aa19423a164926de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9d13c57e484cf78f5afcac1ee6d55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb3ff89c5b647d2bc61fe172f4d6f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dbffa34836f4923b3adfea331ddf77b"}},"metadata":{}},{"name":"stderr","text":"WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n","output_type":"stream"},{"name":"stdout","text":"thinking content: <think>\nOkay, the user is asking for a short introduction to large language models. Let me start by defining what a large language model is. I should mention that they're AI systems trained on vast amounts of text data.\n\nI need to explain their capabilities, like understanding and generating human-like text. Maybe include examples of tasks they can handle‚Äîtext generation, translation, summarization, answering questions.\n\nAlso, it's important to touch on their training process. They use deep learning techniques, especially transformer architectures. Mention pre-training on diverse data and fine-tuning for specific tasks.\n\nI should highlight their applications in various fields: customer service, content creation, research, etc. But also note the challenges, like computational resources and potential biases.\n\nMake sure the introduction is concise but covers the key points. Avoid too much technical jargon since the user might not be an expert. Keep it clear and straightforward.\n</think>\ncontent: A **large language model (LLM)** is an advanced artificial intelligence system trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, particularly **transformer architectures**, to process and predict sequences of words, enabling them to perform tasks like answering questions, writing stories, translating languages, summarizing text, and more. LLMs are pre-trained on diverse datasets, allowing them to grasp context, grammar, and even nuanced topics, and they can be fine-tuned for specific applications. They power tools like chatbots, virtual assistants, and content creation platforms, revolutionizing how humans interact with technology. However, their development also raises challenges related to computational costs, ethical considerations, and potential biases in training data.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# 1. Configuration\nmodel_name = \"Qwen/Qwen3-14B\"\n\n# Use 4-bit quantization to fit the 14B model into the T4's VRAM\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# 2. Load Tokenizer and Model\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"Loading model (this may take a few minutes)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\", # Automatically balances the model across both T4 GPUs\n    trust_remote_code=True\n)\n\n# 3. Prepare the Input\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\n# apply_chat_template handles the specific formatting Qwen3 needs\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True \n)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# 4. Generate Response\nprint(\"\\n--- Generating Response ---\\n\")\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=2048, # Reduced from 32k for faster testing on T4s\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9\n)\n\n# Remove the input tokens from the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# 5. Parsing Thinking Content\n# Qwen3 uses special tokens to wrap the \"thought\" process\nTHINK_END_TOKEN_ID = 151668 # ID for </think>\n\ntry:\n    # Find the last occurrence of the end-thought token\n    index = len(output_ids) - output_ids[::-1].index(THINK_END_TOKEN_ID)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()\nfinal_content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n\nprint(\"--- THINKING PROCESS ---\")\nprint(thinking_content if thinking_content else \"No thinking content generated.\")\nprint(\"\\n--- FINAL ANSWER ---\")\nprint(final_content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport subprocess\nimport sys\nimport shutil\n\n# ==========================================\n# 1. C√ÄI ƒê·∫∂T M√îI TR∆Ø·ªúNG\n# ==========================================\ntry:\n    import llama_cpp\n    print(\"‚úÖ Th∆∞ vi·ªán ƒë√£ c√≥ s·∫µn. B·ªè qua b∆∞·ªõc c√†i ƒë·∫∑t.\")\nexcept ImportError:\n    print(\"‚è≥ ƒêang c√†i ƒë·∫∑t Llama.cpp v√† c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt...\")\n    install_cmd = [\n        sys.executable, \"-m\", \"pip\", \"install\", \n        \"llama-cpp-python\",\n        \"--extra-index-url\", \"https://abetlen.github.io/llama-cpp-python/whl/cu121\",\n        \"--force-reinstall\",\n        \"--no-cache-dir\"\n    ]\n    subprocess.check_call(install_cmd)\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"])\n\nfrom huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama\n\n# ==========================================\n# 2. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (D√πng /tmp ƒë·ªÉ tr√°nh gi·ªõi h·∫°n 20GB)\n# ==========================================\nTEMP_DIR = \"/tmp/qwen_models\"\nCACHE_DIR = \"/tmp/hf_cache\"\n\nos.makedirs(TEMP_DIR, exist_ok=True)\nos.makedirs(CACHE_DIR, exist_ok=True)\n\nREPO_ID = \"bartowski/Qwen2.5-32B-Instruct-GGUF\"\nFILENAME = \"Qwen2.5-32B-Instruct-Q4_K_M.gguf\"\n\ndef run_solution():\n    print(f\"\\nüöÄ ƒêang ki·ªÉm tra/t·∫£i model t·∫°i ({TEMP_DIR})...\")\n    \n    try:\n        # T·∫£i file model\n        model_path = hf_hub_download(\n            repo_id=REPO_ID,\n            filename=FILENAME,\n            local_dir=TEMP_DIR,\n            cache_dir=CACHE_DIR,\n            local_dir_use_symlinks=False\n        )\n        print(f\"‚úÖ Model s·∫µn s√†ng t·∫°i: {model_path}\")\n\n        # Load Model v√†o GPU\n        print(\"‚ö° ƒêang n·∫°p model v√†o GPU (vram)...\")\n        llm = Llama(\n            model_path=model_path,\n            n_gpu_layers=-1,      # ƒê·∫©y to√†n b·ªô model l√™n GPU\n            n_ctx=4096,           # ƒê·ªô d√†i ng·ªØ c·∫£nh (c√≥ th·ªÉ tƒÉng n·∫øu GPU m·∫°nh)\n            verbose=False,\n            main_gpu=0            \n        )\n\n        # C√¢u h·ªèi test\n        prompt = \"why the sky is blue \"\n        \n        # CH·ªàNH S·ª¨A QUAN TR·ªåNG: System prompt √©p tr·∫£ l·ªùi ti·∫øng Vi·ªát\n        formatted_prompt = (\n            \"<|im_start|>system\\n\"\n            \"You are an AI helpful assistant\"\n            \"<|im_end|>\\n\"\n            f\"<|im_start|>user\\n{prompt}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n\n        print(f\"\\nüìù C√¢u h·ªèi: {prompt}\")\n        print(\"‚è≥ ƒêang x·ª≠ l√Ω...\\n\")\n        \n        output = llm(\n            formatted_prompt,\n            max_tokens=1024*4,\n            temperature=0.7,\n            top_p=0.9,\n            stop=[\"<|im_end|>\", \"<|im_start|>\"],\n            echo=False\n        )\n\n        response = output['choices'][0]['text'].strip()\n        \n        print(\"=\"*40)\n        print(\"ü§ñ K·∫æT QU·∫¢ TI·∫æNG VI·ªÜT:\")\n        print(\"=\"*40)\n        print(response)\n        print(\"=\"*40)\n\n    except Exception as e:\n        print(f\"\\n‚ùå L·ªñI: {e}\")\n        if \"No space\" in str(e):\n            print(\"‚ö†Ô∏è H·∫øt b·ªô nh·ªõ /tmp. ƒêang d·ªçn d·∫πp...\")\n            shutil.rmtree(TEMP_DIR, ignore_errors=True)\n\nif __name__ == \"__main__\":\n    run_solution()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}