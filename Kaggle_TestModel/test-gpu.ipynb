{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U  accelerate bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit\"\n\n# Load model chia ƒë·ªÅu l√™n 2 GPU T4 (m·ªói c√°i 16GB VRAM)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\" \n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprompt = \"Vi·∫øt code Python ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì h√¨nh sin b·∫±ng matplotlib.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=2048)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U pynvml # ƒê·ªÉ ƒëo VRAM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================\n# B∆Ø·ªöC 1: C√ÄI ƒê·∫∂T V√Ä S·ª¨A L·ªñI XUNG ƒê·ªòT PHI√äN B·∫¢N\n# ======================================================================\nimport os\nimport subprocess\nimport sys\nimport torch\nimport gc\n\ndef install_dependencies():\n    # Ki·ªÉm tra xem th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ch∆∞a\n    try:\n        import awq\n        import transformers\n        # Ki·ªÉm tra phi√™n b·∫£n transformers ƒë·ªÉ tr√°nh l·ªói PytorchGELUTanh\n        from transformers.activations import PytorchGELUTanh\n    except (ImportError, NameError, AttributeError):\n        print(\"‚è≥ ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)...\")\n        # √âp phi√™n b·∫£n transformers c≈© h∆°n ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi AutoAWQ\n        packages = [\n            \"transformers<4.45.0\", \n            \"autoawq\", \n            \"optimum\", \n            \"accelerate\"\n        ]\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages)\n        print(\"‚úÖ C√†i ƒë·∫∑t ho√†n t·∫•t. Vui l√≤ng 'Restart Session' n·∫øu ƒë√¢y l√† l·∫ßn ƒë·∫ßu ch·∫°y!\")\n\ninstall_dependencies()\n\n# ======================================================================\n# B∆Ø·ªöC 2: D·ªåN D·∫∏P B·ªò NH·ªö (FIX L·ªñI TR√ÄN VRAM KHI CH·∫†Y L·∫†I)\n# ======================================================================\ndef clear_gpu_memory():\n    print(\"üßπ ƒêang d·ªçn d·∫πp b·ªô nh·ªõ GPU ƒë·ªÉ tr√°nh l·ªói n·∫°p ch·ªìng...\")\n    global model, tokenizer\n    \n    # X√≥a c√°c bi·∫øn global n·∫øu t·ªìn t·∫°i\n    if 'model' in globals():\n        del model\n    if 'tokenizer' in globals():\n        del tokenizer\n        \n    # Gi·∫£i ph√≥ng b·ªô nh·ªõ ƒë·ªám\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize() # ƒê·ª£i GPU d·ªçn d·∫πp xong ho√†n to√†n\n    print(\"‚ú® B·ªô nh·ªõ s·∫°ch s·∫Ω. B·∫Øt ƒë·∫ßu n·∫°p model m·ªõi.\")\n\nclear_gpu_memory()\n\n# ======================================================================\n# B∆Ø·ªöC 3: C·∫§U H√åNH V√Ä LOAD MODEL QWEN 2.5 32B AWQ\n# ======================================================================\nimport warnings\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n\n# ·∫®n c√°c c·∫£nh b√°o r√°c\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nmodel_id = \"Qwen/Qwen2.5-32B-Instruct-AWQ\"\n\nprint(f\"\\nüöÄ ƒêang n·∫°p model: {model_id}\")\nprint(\"üì¶ C·∫•u h√¨nh: 2x T4 GPU (32GB VRAM t·ªïng c·ªông) | 4-bit AWQ\")\n\ntry:\n    # 1. T·∫£i Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # 2. T·∫£i Model v·ªõi thi·∫øt l·∫≠p t·ªëi ∆∞u cho Kaggle\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        device_map=\"auto\",           # T·ª± ƒë·ªông chia layer sang 2 GPU T4\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,      # Gi·∫£m t·∫£i RAM h·ªá th·ªëng khi n·∫°p\n        torch_dtype=torch.float16    # S·ª≠ d·ª•ng FP16 ƒë·ªÉ ti·∫øt ki·ªám VRAM\n    )\n    \n    # 3. Kh·ªüi t·∫°o Streamer ƒë·ªÉ in ch·ªØ tr·ª±c ti·∫øp\n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n    print(\"‚úÖ Model ƒë√£ s·∫µn s√†ng tr√™n GPU!\")\n\n    # ======================================================================\n    # B∆Ø·ªöC 4: T·∫†O VƒÇN B·∫¢N V·ªöI CH·∫æ ƒê·ªò STREAMING\n    # ======================================================================\n    prompt = \"Why is the sky blue?\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    \n    # Chu·∫©n b·ªã input theo format c·ªßa Qwen\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n\n    print(f\"\\nü§ñ Qwen ƒëang tr·∫£ l·ªùi (Streaming):\\n{'-'*30}\")\n    \n    # Ch·∫°y generate v·ªõi streamer (k·∫øt qu·∫£ s·∫Ω in ra m√†n h√¨nh ngay l·∫≠p t·ª©c)\n    _ = model.generate(\n        **model_inputs, \n        streamer=streamer,           # K√≠ch ho·∫°t streaming\n        max_new_tokens=1024,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.9,\n        repetition_penalty=1.1\n    )\n    \n    print(f\"{'-'*30}\\n‚úÖ ƒê√£ ho√†n th√†nh ph·∫£n h·ªìi.\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå L·ªñI: {e}\")\n    if \"PytorchGELUTanh\" in str(e):\n        print(\"üí° G·ª¢I √ù: B·∫°n c·∫ßn nh·∫•n 'Restart Session' ·ªü menu b√™n ph·∫£i r·ªìi ch·∫°y l·∫°i √¥ n√†y.\")\n    elif \"CUDA out of memory\" in str(e):\n        print(\"üí° G·ª¢I √ù: VRAM ƒë√£ ƒë·∫ßy. H√£y ch·ªçn 'Restart Session' ƒë·ªÉ gi·∫£i ph√≥ng ho√†n to√†n.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport gc\nimport pynvml\nimport time\n\n# Kh·ªüi t·∫°o th∆∞ vi·ªán ƒëo VRAM\npynvml.nvmlInit()\n\ndef print_gpu_utilization():\n    device_count = pynvml.nvmlDeviceGetCount()\n    print(f\"--- GPU Memory Status ({device_count} GPUs) ---\")\n    for i in range(device_count):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        print(f\"GPU {i}: Used {info.used // 1024**2} MB / Total {info.total // 1024**2} MB\")\n    print(\"---------------------------------------\")\n\ndef run_model_test(model_id, use_4bit=True):\n    print(f\"\\n{'='*20} TESTING: {model_id} {'='*20}\")\n    print(f\"Mode: {'4-bit Quantization' if use_4bit else 'Full/Half Precision'}\")\n    \n    # 1. C·∫•u h√¨nh Quantization (N·∫øu d√πng 4-bit ƒë·ªÉ test model to)\n    bnb_config = None\n    if use_4bit:\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n    try:\n        # 2. Load Tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        \n        # 3. Load Model v·ªõi device_map=\"auto\" (T·ª± ƒë·ªông chia qua 2 T4)\n        print(\"Loading model... (This may take time)\")\n        start_time = time.time()\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\", # T·ª± ƒë·ªông r·∫£i model l√™n GPU 0 v√† GPU 1\n            trust_remote_code=True,\n            torch_dtype=torch.float16\n        )\n        print(f\"Model loaded in {time.time() - start_time:.2f} seconds.\")\n        \n        # 4. Ki·ªÉm tra VRAM sau khi load\n        print_gpu_utilization()\n        print(f\"Model Footprint: {model.get_memory_footprint() // 1024**2} MB\")\n\n        # 5. Test Inference (Ch·∫°y th·ª≠)\n        input_text = \"Explain quantum physics in one sentence.\"\n        inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n        \n        print(\"\\nGenerating response...\")\n        outputs = model.generate(**inputs, max_new_tokens=2048)\n        print(f\"Response: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå ERROR loading {model_id}: {e}\")\n        # N·∫øu l·ªói Out Of Memory\n        if \"CUDA out of memory\" in str(e):\n            print(\">>> MODEL IS TOO BIG FOR 2x T4 (32GB Total) <<<\")\n\n    finally:\n        # 6. D·ªçn d·∫πp b·ªô nh·ªõ (Quan tr·ªçng ƒë·ªÉ test model ti·∫øp theo)\n        print(\"\\nCleaning up memory...\")\n        try:\n            del model\n            del tokenizer\n        except:\n            pass\n        gc.collect()\n        torch.cuda.empty_cache()\n        print_gpu_utilization()\n        print(f\"{'='*50}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. M·ª©c Trung b√¨nh (Th·ª≠ th√°ch nh·∫π)\n# Qwen 2.5 14B (Alibaba - Model r·∫•t m·∫°nh)\nrun_model_test(\"Qwen/Qwen2.5-14B-Instruct\", use_4bit=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Thay th·∫ø b·∫±ng token b·∫°n v·ª´a copy ·ªü B∆∞·ªõc 2\nhf_token = \"\" \n\nlogin(token=hf_token)\n\n# --- Sau ƒë√≥ m·ªõi ch·∫°y l·ªánh test ---\nrun_model_test(\"google/gemma-2-27b-it\", use_4bit=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_model_test(\"meta-llama/Llama-3.1-8B-Instruct\", use_4bit=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}