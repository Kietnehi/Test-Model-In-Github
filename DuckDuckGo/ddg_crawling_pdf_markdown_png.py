import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import asyncio
import re
import base64
import os
from ddgs import DDGS
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


# ================== H√ÄM T·∫†O T√äN FILE AN TO√ÄN ==================
def clean_filename(url):
    name = re.sub(r'[\\/*?:"<>|]', "", url.split("//")[-1])
    return name[:50]


# ================== T·∫†O 3 FOLDER C·ªê ƒê·ªäNH ==================
MD_DIR = "output_md"
PDF_DIR = "output_pdf"
IMG_DIR = "output_img"

os.makedirs(MD_DIR, exist_ok=True)
os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)


# ================== PH·∫¶N SEARCH DUCKDUCKGO ==================
def get_links(choice, query, max_results):
    links = []

    with DDGS() as ddgs:

        # ================= TEXT =================
        if choice == "1":
            results = list(ddgs.text(query, max_results=max_results))

            print("\n===== K·∫æT QU·∫¢ TEXT =====")
            for i, r in enumerate(results, 1):
                print(f"\n--- K·∫øt qu·∫£ {i} ---")
                print("Ti√™u ƒë·ªÅ:", r.get("title"))
                print("Link:", r.get("href"))
                print("M√¥ t·∫£:", r.get("body"))

                if r.get("href"):
                    links.append(r["href"])


        # ================= NEWS =================
        elif choice == "2":
            results = list(ddgs.news(query, max_results=max_results))

            print("\n===== K·∫æT QU·∫¢ NEWS =====")
            for i, r in enumerate(results, 1):
                print(f"\n--- Tin {i} ---")
                print("Ng√†y:", r.get("date"))
                print("Ti√™u ƒë·ªÅ:", r.get("title"))
                print("Ngu·ªìn:", r.get("source"))
                print("Link:", r.get("url"))
                print("T√≥m t·∫Øt:", r.get("body"))

                if r.get("url"):
                    links.append(r["url"])


        # ================= BOOKS =================
        elif choice == "3":
            results = list(ddgs.books(query, max_results=max_results))

            print("\n===== K·∫æT QU·∫¢ BOOKS =====")
            for i, r in enumerate(results, 1):
                print(f"\n--- S√°ch {i} ---")
                print("T√™n:", r.get("title"))
                print("T√°c gi·∫£:", r.get("author"))
                print("NXB:", r.get("publisher"))
                print("Link:", r.get("url"))

                if r.get("url"):
                    links.append(r["url"])

    return links



# ================== PH·∫¶N CRAWL B·∫∞NG crawl4ai ==================
async def crawl_links(urls):
    if not urls:
        print("‚ùå Kh√¥ng c√≥ link n√†o ƒë·ªÉ crawl.")
        return

    run_cfg = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=True,
        pdf=True,
        screenshot=True
    )

    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun_many(urls, config=run_cfg):
            if result.success:
                base_name = clean_filename(result.url)
                print(f"üíæ ƒêang x·ª≠ l√Ω: {base_name}...")

                # L∆∞u Markdown
                md_path = os.path.join(MD_DIR, f"{base_name}.md")
                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(f"# URL: {result.url}\n\n")
                    f.write(result.markdown.raw_markdown)

                # L∆∞u PDF
                if result.pdf:
                    pdf_path = os.path.join(PDF_DIR, f"{base_name}.pdf")    
                    with open(pdf_path, "wb") as f:
                        f.write(result.pdf)

                # L∆∞u Screenshot (PNG)
                if result.screenshot:
                    img_path = os.path.join(IMG_DIR, f"{base_name}.png")
                    img_data = base64.b64decode(result.screenshot)
                    with open(img_path, "wb") as f:
                        f.write(img_data)

                print(f"‚úÖ ƒê√£ l∆∞u xong cho: {result.url}")

            else:
                print(f"‚ùå L·ªói t·∫°i {result.url}: {result.error_message}")


# ================== MAIN ==================
def main():
    print("=== SEARCH + CRAWL DUCKDUCKGO + CRAWL4AI ===")
    print("Ch·ªçn lo·∫°i t√¨m ki·∫øm:")
    print("1 - Text (Web)")
    print("2 - News (Tin t·ª©c)")
    print("3 - Books (S√°ch)")

    choice = input("Nh·∫≠p l·ª±a ch·ªçn (1-3): ").strip()
    query = input("Nh·∫≠p n·ªôi dung c·∫ßn t√¨m: ").strip()

    try:
        max_results = int(input("S·ªë k·∫øt qu·∫£ mu·ªën l·∫•y (m·∫∑c ƒë·ªãnh 5): ") or 5)
    except ValueError:
        max_results = 5

    print("\nüîé ƒêang t√¨m ki·∫øm tr√™n DuckDuckGo...")
    links = get_links(choice, query, max_results)

    if not links:
        print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c link n√†o t·ª´ DuckDuckGo.")
        return

    print("\nüìå Danh s√°ch link s·∫Ω crawl:")
    for i, link in enumerate(links, 1):
        print(f"{i}. {link}")

    print("\nüï∑Ô∏è B·∫Øt ƒë·∫ßu crawl b·∫±ng crawl4ai...\n")
    asyncio.run(crawl_links(links))


if __name__ == "__main__":
    main()
